File,LineNumber,src
shims/src/common/java/org/apache/hadoop/fs/ProxyFileSystem.java,48,  private Path swizzleParamPath(Path p) {
shims/src/common/java/org/apache/hadoop/fs/ProxyFileSystem.java,60,"  private FileStatus swizzleFileStatus(FileStatus orig, boolean isParam) {"
shims/src/common/java/org/apache/hadoop/fs/ProxyLocalFileSystem.java,64,"    fs = new ProxyFileSystem(localFs, URI.create(proxyUriString));"
hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/transfer/impl/HCatInputFormatReader.java,66,"        job, re.getDbName(), re.getTableName()).setFilter(re.getFilterString());"
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatInputFormat.java,42,"   * @deprecated as of release 0.5, and will be removed in a future release"
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatInputFormat.java,44,  @Deprecated
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatInputFormat.java,45,"  public static void setInput(Job job, InputJobInfo inputJobInfo) throws IOException {"
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatInputFormat.java,46,"    setInput(job.getConfiguration(), inputJobInfo);"
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatInputFormat.java,50,"   * @deprecated as of release 0.5, and will be removed in a future release"
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatInputFormat.java,52,  @Deprecated
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatInputFormat.java,53,"  public static void setInput(Configuration conf, InputJobInfo inputJobInfo) throws IOException {"
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatInputFormat.java,54,"    setInput(conf, inputJobInfo.getDatabaseName(), inputJobInfo.getTableName())"
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatInputFormat.java,55,      .setFilter(inputJobInfo.getFilter())
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatInputFormat.java,56,      .setProperties(inputJobInfo.getProperties());
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatInputFormat.java,60,"   * See {@link #setInput(org.apache.hadoop.conf.Configuration, String, String)}"
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatInputFormat.java,62,"  public static HCatInputFormat setInput(Job job, String dbName, String tableName) throws IOException {"
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatInputFormat.java,63,"    return setInput(job.getConfiguration(), dbName, tableName);"
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatInputFormat.java,74,"  public static HCatInputFormat setInput(Configuration conf, String dbName, String tableName)"
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatInputFormat.java,82,"    hCatInputFormat.inputJobInfo = InputJobInfo.create(dbName, tableName, null, null);"
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatInputFormat.java,94,   * Set a filter on the input table.
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatInputFormat.java,95,"   * @param filter the filter specification, which may be null"
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatInputFormat.java,96,   * @return this
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatInputFormat.java,97,   * @throws IOException on all errors
hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/HCatMapReduceTest.java,344,"    HCatInputFormat.setInput(job, dbName, tableName).setFilter(filter);"
hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hive/hcatalog/pig/HCatLoader.java,119,"      HCatInputFormat.setInput(job, dbName, tableName).setFilter(getPartitionFilterString());"
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hcatalog/utils/HBaseReadWrite.java,44,import org.apache.hcatalog.mapreduce.InputJobInfo;
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hcatalog/utils/HBaseReadWrite.java,168,"    HCatInputFormat.setInput(job, InputJobInfo.create(dbName, tableName,"
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hcatalog/utils/HBaseReadWrite.java,169,      null));
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/GroupByAge.java,41,import org.apache.hive.hcatalog.mapreduce.InputJobInfo;
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/GroupByAge.java,108,"    HCatInputFormat.setInput(job, InputJobInfo.create(dbName,"
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/GroupByAge.java,109,"      inputTableName, null));"
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/ReadJson.java,40,import org.apache.hive.hcatalog.mapreduce.InputJobInfo;
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/ReadJson.java,93,"    HCatInputFormat.setInput(job, InputJobInfo.create("
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/ReadJson.java,94,"      dbName, tableName, null));"
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/ReadRC.java,40,import org.apache.hive.hcatalog.mapreduce.InputJobInfo;
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/ReadRC.java,94,"    HCatInputFormat.setInput(job, InputJobInfo.create("
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/ReadRC.java,95,"      dbName, tableName, null));"
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/ReadText.java,40,import org.apache.hive.hcatalog.mapreduce.InputJobInfo;
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/ReadText.java,105,"    HCatInputFormat.setInput(job, InputJobInfo.create("
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/ReadText.java,106,"      dbName, tableName, null));"
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/ReadWrite.java,39,import org.apache.hive.hcatalog.mapreduce.InputJobInfo;
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/ReadWrite.java,89,"    HCatInputFormat.setInput(job, InputJobInfo.create(dbName,"
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/ReadWrite.java,90,"      inputTableName, null));"
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/SimpleRead.java,40,import org.apache.hive.hcatalog.mapreduce.InputJobInfo;
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/SimpleRead.java,90,"    HCatInputFormat.setInput(job, InputJobInfo.create("
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/SimpleRead.java,91,"      dbName, tableName, null));"
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/StoreComplex.java,39,import org.apache.hive.hcatalog.mapreduce.InputJobInfo;
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/StoreComplex.java,106,"    HCatInputFormat.setInput(job, InputJobInfo.create("
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/StoreComplex.java,107,"      dbName, tableName, null));"
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/StoreDemo.java,38,import org.apache.hive.hcatalog.mapreduce.InputJobInfo;
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/StoreDemo.java,117,"    HCatInputFormat.setInput(job, InputJobInfo.create("
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/StoreDemo.java,118,"      dbName, tableName, null));"
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/StoreNumbers.java,42,import org.apache.hive.hcatalog.mapreduce.InputJobInfo;
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/StoreNumbers.java,181,"    HCatInputFormat.setInput(job, InputJobInfo.create("
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/StoreNumbers.java,182,"      dbName, tableName, null));"
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/SumNumbers.java,44,import org.apache.hive.hcatalog.mapreduce.InputJobInfo;
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/SumNumbers.java,165,"    HCatInputFormat.setInput(job, InputJobInfo.create("
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/SumNumbers.java,166,"      dbName, tableName, null));"
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/TypeDataCheck.java,40,import org.apache.hive.hcatalog.mapreduce.InputJobInfo;
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/TypeDataCheck.java,153,"      HCatInputFormat.setInput(job, InputJobInfo.create("
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/TypeDataCheck.java,154,"        dbName, tableName, null));"
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/WriteJson.java,38,import org.apache.hive.hcatalog.mapreduce.InputJobInfo;
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/WriteJson.java,94,"    HCatInputFormat.setInput(job, InputJobInfo.create(dbName,"
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/WriteJson.java,95,"      inputTableName, null));"
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/WriteRC.java,38,import org.apache.hive.hcatalog.mapreduce.InputJobInfo;
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/WriteRC.java,96,"    HCatInputFormat.setInput(job, InputJobInfo.create(dbName,"
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/WriteRC.java,97,"      inputTableName, null));"
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/WriteText.java,38,import org.apache.hive.hcatalog.mapreduce.InputJobInfo;
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/WriteText.java,106,"    HCatInputFormat.setInput(job, InputJobInfo.create(dbName,"
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/WriteText.java,107,"      inputTableName, null));"
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/WriteTextPartitioned.java,42,import org.apache.hive.hcatalog.mapreduce.InputJobInfo;
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/WriteTextPartitioned.java,97,"    HCatInputFormat.setInput(job, InputJobInfo.create(dbName,"
hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/WriteTextPartitioned.java,98,"      inputTableName, filter));"
hcatalog/storage-handlers/hbase/src/test/org/apache/hive/hcatalog/hbase/TestHBaseInputFormat.java,65,import org.apache.hive.hcatalog.mapreduce.InputJobInfo;
hcatalog/storage-handlers/hbase/src/test/org/apache/hive/hcatalog/hbase/TestHBaseInputFormat.java,163,"    InputJobInfo inputJobInfo = InputJobInfo.create(databaseName, tableName,"
hcatalog/storage-handlers/hbase/src/test/org/apache/hive/hcatalog/hbase/TestHBaseInputFormat.java,164,                null);
hcatalog/storage-handlers/hbase/src/test/org/apache/hive/hcatalog/hbase/TestHBaseInputFormat.java,165,"    HCatInputFormat.setInput(job, inputJobInfo);"
hcatalog/storage-handlers/hbase/src/test/org/apache/hive/hcatalog/hbase/TestHBaseInputFormat.java,228,    InputJobInfo inputJobInfo = InputJobInfo.create(
hcatalog/storage-handlers/hbase/src/test/org/apache/hive/hcatalog/hbase/TestHBaseInputFormat.java,229,"      MetaStoreUtils.DEFAULT_DATABASE_NAME, tableName, null);"
hcatalog/storage-handlers/hbase/src/test/org/apache/hive/hcatalog/hbase/TestHBaseInputFormat.java,231,"    HCatInputFormat.setInput(job, inputJobInfo);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,3508,      new ArrayList<Class<?>>();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,3534,"      groupByOutputRowResolver.putExpression(value, new ColumnInfo("
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,3535,"          field, udaf.returnType, """", false));"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,3720,          String name = getColumnInternalName(numExprs);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,3721,"          String field = Utilities.ReduceField.KEY.toString() + ""."" + colName"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,3722,"              + "":"" + i"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,3723,"              + ""."" + name;"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,3724,"          ColumnInfo colInfo = new ColumnInfo(field, expr.getTypeInfo(), null, false);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,3725,"          reduceSinkOutputRowResolver.putExpression(parameter, colInfo);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,3726,"          colExprMap.put(field, expr);"
hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/schema/HCatSchema.java,33,
hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/transfer/HCatReader.java,35,
hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/transfer/HCatWriter.java,36,
hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hive/hcatalog/pig/HCatLoader.java,52,
hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hive/hcatalog/pig/HCatStorer.java,51,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/CompleteDelegator.java,71,"        failed(""Job not yet complete. jobId="" + id + "" Status from JT="" + jobStatus, null);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/HDFSStorage.java,105,"      //don't print stack trace since clients poll for 'exitValue', 'completed',"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/HDFSStorage.java,106,      //files which are not there until job completes
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/HDFSStorage.java,107,"      LOG.info(""Couldn't find "" + p + "": "" + e.getMessage());"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/HiveJobIDParser.java,35,"    return parseJobID(TempletonControllerJob.STDERR_FNAME, jobidPattern);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JarJobIDParser.java,35,"    return parseJobID(TempletonControllerJob.STDERR_FNAME, jobidPattern);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/PigJobIDParser.java,35,"    return parseJobID(TempletonControllerJob.STDERR_FNAME, jobidPattern);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,21,import java.io.BufferedReader;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,23,import java.io.InputStream;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,24,import java.io.InputStreamReader;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,25,import java.io.OutputStream;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,26,import java.io.PrintWriter;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,27,import java.net.URISyntaxException;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,29,import java.util.ArrayList;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,30,import java.util.Arrays;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,31,import java.util.Iterator;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,32,import java.util.LinkedList;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,33,import java.util.List;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,34,import java.util.Map;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,35,import java.util.concurrent.ExecutorService;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,36,import java.util.concurrent.Executors;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,37,import java.util.concurrent.TimeUnit;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,53,import org.apache.hadoop.mapreduce.Mapper;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,54,import org.apache.hadoop.mapreduce.Mapper.Context;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,59,import org.apache.hadoop.util.Shell;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,61,import org.apache.hive.hcatalog.templeton.BadParam;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,62,import org.apache.hive.hcatalog.templeton.LauncherDelegator;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,86,public class TempletonControllerJob extends Configured implements Tool {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,87,"  public static final String COPY_NAME = ""templeton.copy"";"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,88,"  public static final String STATUSDIR_NAME = ""templeton.statusdir"";"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,89,"  public static final String ENABLE_LOG = ""templeton.enablelog"";"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,90,"  public static final String JOB_TYPE = ""templeton.jobtype"";"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,91,"  public static final String JAR_ARGS_NAME = ""templeton.args"";"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,92,"  public static final String OVERRIDE_CLASSPATH = ""templeton.override-classpath"";"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,93,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,94,"  public static final String STDOUT_FNAME = ""stdout"";"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,95,"  public static final String STDERR_FNAME = ""stderr"";"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,96,"  public static final String EXIT_FNAME = ""exit"";"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,97,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,98,  public static final int WATCHER_TIMEOUT_SECS = 10;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,99,  public static final int KEEP_ALIVE_MSEC = 60 * 1000;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,100,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,101,  public static final String TOKEN_FILE_ARG_PLACEHOLDER
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,102,"    = ""__WEBHCAT_TOKEN_FILE_LOCATION__"";"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,103,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,104,  private static TrivialExecService execService = TrivialExecService.getInstance();
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,105,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,107,  private final boolean secureMetastoreAccess;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,110,"   * @param secureMetastoreAccess - if true, a delegation token will be created"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,111,   *                              and added to the job
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,113,  public TempletonControllerJob(boolean secureMetastoreAccess) {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,114,    super();
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,115,    this.secureMetastoreAccess = secureMetastoreAccess;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,116,  }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,117,  public static class LaunchMapper
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,118,"    extends Mapper<NullWritable, NullWritable, Text, Text> {"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,119,"    protected Process startJob(Context context, String user,"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,120,                   String overrideClasspath)
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,121,"      throws IOException, InterruptedException {"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,122,      Configuration conf = context.getConfiguration();
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,123,"      copyLocal(COPY_NAME, conf);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,124,      String[] jarArgs
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,125,        = TempletonUtils.decodeArray(conf.get(JAR_ARGS_NAME));
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,126,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,127,      ArrayList<String> removeEnv = new ArrayList<String>();
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,128,"      removeEnv.add(""HADOOP_ROOT_LOGGER"");"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,129,"      removeEnv.add(""hadoop-command"");"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,130,"      removeEnv.add(""CLASS"");"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,131,"      removeEnv.add(""mapredcommand"");"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,132,"      Map<String, String> env = TempletonUtils.hadoopUserEnv(user,"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,133,        overrideClasspath);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,134,      List<String> jarArgsList = new LinkedList<String>(Arrays.asList(jarArgs));
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,135,"      String tokenFile = System.getenv(""HADOOP_TOKEN_FILE_LOCATION"");"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,136,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,137,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,138,      if (tokenFile != null) {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,139,"        //Token is available, so replace the placeholder"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,140,"        tokenFile = tokenFile.replaceAll(""\"""", """");"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,141,"        String tokenArg = ""mapreduce.job.credentials.binary="" + tokenFile;"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,142,        if (Shell.WINDOWS) {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,143,          try {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,144,            tokenArg = TempletonUtils.quoteForWindows(tokenArg);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,145,          } catch (BadParam e) {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,146,"            throw new IOException(""cannot pass "" + tokenFile + "" to mapreduce.job.credentials.binary"", e);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,147,          }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,148,        }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,149,        for(int i=0; i<jarArgsList.size(); i++){
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,150,          String newArg =
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,151,"            jarArgsList.get(i).replace(TOKEN_FILE_ARG_PLACEHOLDER, tokenArg);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,152,"          jarArgsList.set(i, newArg);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,153,        }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,154,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,155,      }else{
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,156,"        //No token, so remove the placeholder arg"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,157,        Iterator<String> it = jarArgsList.iterator();
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,158,        while(it.hasNext()){
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,159,          String arg = it.next();
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,160,          if(arg.contains(TOKEN_FILE_ARG_PLACEHOLDER)){
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,161,            it.remove();
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,162,          }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,163,        }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,164,      }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,165,"      return execService.run(jarArgsList, removeEnv, env);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,166,    }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,167,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,168,"    private void copyLocal(String var, Configuration conf)"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,169,      throws IOException {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,170,      String[] filenames = TempletonUtils.decodeArray(conf.get(var));
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,171,      if (filenames != null) {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,172,        for (String filename : filenames) {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,173,          Path src = new Path(filename);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,174,          Path dst = new Path(src.getName());
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,175,          FileSystem fs = src.getFileSystem(conf);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,176,"          System.err.println(""templeton: copy "" + src + "" => "" + dst);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,177,"          fs.copyToLocalFile(src, dst);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,179,      }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,180,    }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,181,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,182,    @Override
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,183,    public void run(Context context)
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,184,"      throws IOException, InterruptedException {"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,185,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,186,      Configuration conf = context.getConfiguration();
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,187,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,188,"      Process proc = startJob(context,"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,189,"        conf.get(""user.name""),"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,190,        conf.get(OVERRIDE_CLASSPATH));
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,191,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,192,      String statusdir = conf.get(STATUSDIR_NAME);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,193,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,194,      if (statusdir != null) {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,195,        try {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,196,"          statusdir = TempletonUtils.addUserHomeDirectoryIfApplicable(statusdir,"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,197,"            conf.get(""user.name""));"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,198,        } catch (URISyntaxException e) {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,199,"          throw new IOException(""Invalid status dir URI"", e);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,202,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,203,      Boolean enablelog = Boolean.parseBoolean(conf.get(ENABLE_LOG));
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,204,      LauncherDelegator.JobType jobType = LauncherDelegator.JobType.valueOf(conf.get(JOB_TYPE));
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,205,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,206,      ExecutorService pool = Executors.newCachedThreadPool();
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,207,"      executeWatcher(pool, conf, context.getJobID(),"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,208,"        proc.getInputStream(), statusdir, STDOUT_FNAME);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,209,"      executeWatcher(pool, conf, context.getJobID(),"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,210,"        proc.getErrorStream(), statusdir, STDERR_FNAME);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,211,"      KeepAlive keepAlive = startCounterKeepAlive(pool, context);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,212,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,213,      proc.waitFor();
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,214,      keepAlive.sendReport = false;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,215,      pool.shutdown();
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,216,"      if (!pool.awaitTermination(WATCHER_TIMEOUT_SECS, TimeUnit.SECONDS)) {"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,217,        pool.shutdownNow();
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,218,      }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,219,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,220,"      writeExitValue(conf, proc.exitValue(), statusdir);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,221,"      JobState state = new JobState(context.getJobID().toString(), conf);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,222,      state.setExitValue(proc.exitValue());
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,223,"      state.setCompleteStatus(""done"");"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,224,      state.close();
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,225,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,226,      if (enablelog && TempletonUtils.isset(statusdir)) {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,227,"        System.err.println(""templeton: collecting logs for "" + context.getJobID().toString()"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,228,"          + "" to "" + statusdir + ""/logs"");"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,229,"        LogRetriever logRetriever = new LogRetriever(statusdir, jobType, conf);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,230,        logRetriever.run();
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,231,      }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,232,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,233,      if (proc.exitValue() != 0) {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,234,"        System.err.println(""templeton: job failed with exit code """
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,235,          + proc.exitValue());
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,236,      }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,237,      else {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,238,"        System.err.println(""templeton: job completed with exit code 0"");"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,239,      }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,240,    }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,241,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,242,"    private void executeWatcher(ExecutorService pool, Configuration conf,"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,243,"                  JobID jobid, InputStream in, String statusdir,"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,244,                  String name)
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,245,      throws IOException {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,246,"      Watcher w = new Watcher(conf, jobid, in, statusdir, name);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,247,      pool.execute(w);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,248,    }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,249,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,250,"    private KeepAlive startCounterKeepAlive(ExecutorService pool, Context context)"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,251,      throws IOException {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,252,      KeepAlive k = new KeepAlive(context);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,253,      pool.execute(k);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,254,      return k;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,255,    }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,256,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,257,"    private void writeExitValue(Configuration conf, int exitValue, String statusdir)"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,258,      throws IOException {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,259,      if (TempletonUtils.isset(statusdir)) {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,260,"        Path p = new Path(statusdir, EXIT_FNAME);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,261,        FileSystem fs = p.getFileSystem(conf);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,262,        OutputStream out = fs.create(p);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,263,"        System.err.println(""templeton: Writing exit value """
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,264,"          + exitValue + "" to "" + p);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,265,        PrintWriter writer = new PrintWriter(out);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,266,        writer.println(exitValue);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,267,        writer.close();
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,268,      }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,269,    }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,271,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,272,  private static class Watcher implements Runnable {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,273,    private final InputStream in;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,274,    private OutputStream out;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,275,    private final JobID jobid;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,276,    private final Configuration conf;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,277,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,278,"    public Watcher(Configuration conf, JobID jobid, InputStream in,"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,279,"             String statusdir, String name)"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,280,      throws IOException {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,281,      this.conf = conf;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,282,      this.jobid = jobid;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,283,      this.in = in;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,284,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,285,      if (name.equals(STDERR_FNAME))
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,286,        out = System.err;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,287,      else
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,288,        out = System.out;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,289,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,290,      if (TempletonUtils.isset(statusdir)) {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,291,"        Path p = new Path(statusdir, name);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,292,        FileSystem fs = p.getFileSystem(conf);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,293,        out = fs.create(p);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,294,"        System.err.println(""templeton: Writing status to "" + p);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,295,      }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,296,    }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,297,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,298,    @Override
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,299,    public void run() {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,300,      try {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,301,        InputStreamReader isr = new InputStreamReader(in);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,302,        BufferedReader reader = new BufferedReader(isr);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,303,        PrintWriter writer = new PrintWriter(out);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,304,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,305,        String line;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,306,        while ((line = reader.readLine()) != null) {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,307,          writer.println(line);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,308,          JobState state = null;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,310,            String percent = TempletonUtils.extractPercentComplete(line);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,311,            String childid = TempletonUtils.extractChildJobId(line);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,312,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,313,            if (percent != null || childid != null) {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,314,"              state = new JobState(jobid.toString(), conf);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,315,              state.setPercentComplete(percent);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,316,              state.setChildId(childid);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,317,            }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,318,          } catch (IOException e) {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,319,"            System.err.println(""templeton: state error: "" + e);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,320,          } finally {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,321,            if (state != null) {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,322,              try {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,323,                state.close();
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,324,              } catch (IOException e) {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,325,              }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,326,            }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,329,        writer.flush();
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,330,      } catch (IOException e) {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,331,"        System.err.println(""templeton: execute error: "" + e);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,336,  public static class KeepAlive implements Runnable {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,337,    private Context context;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,338,    public boolean sendReport;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,339,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,340,    public KeepAlive(Context context)
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,341,    {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,342,      this.sendReport = true;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,343,      this.context = context;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,344,    }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,346,    @Override
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,347,    public void run() {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,348,      try {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,349,        while (sendReport) {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,350,          // Periodically report progress on the Context object
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,351,          // to prevent TaskTracker from killing the Templeton
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,352,          // Controller task
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,353,          context.progress();
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,354,"          System.err.println(""KeepAlive Heart beat"");"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,355,          Thread.sleep(KEEP_ALIVE_MSEC);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,356,        }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,357,      } catch (InterruptedException e) {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,358,        // Ok to be interrupted
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,359,      }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,360,    }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,368,    }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,369,    else {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,379,  public int run(String[] args)
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,380,"    throws IOException, InterruptedException, ClassNotFoundException, TException {"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,382,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,387,    job.setJarByClass(TempletonControllerJob.class);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,388,"    job.setJobName(""TempletonControllerJob"");"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,407,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,412,"      LOG.debug(""Added metastore delegation token for jobId="" + submittedJobId.toString() + "" "" +"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java,413,"              ""user="" + user);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonUtils.java,88,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonUtils.java,89,  public static final Pattern JAR_COMPLETE
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonUtils.java,90,"    = Pattern.compile("" map \\d+%\\s+reduce \\d+%$"");"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonUtils.java,104,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TrivialExecService.java,25,import org.apache.commons.logging.Log;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TrivialExecService.java,26,import org.apache.commons.logging.LogFactory;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TrivialExecService.java,32,public class TrivialExecService {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TrivialExecService.java,33,  private static volatile TrivialExecService theSingleton;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TrivialExecService.java,35,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TrivialExecService.java,44,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TrivialExecService.java,46,"             Map<String, String> environmentVariables)"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TrivialExecService.java,48,"    logDebugCmd(cmd, environmentVariables);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TrivialExecService.java,50,    for (String key : removeEnv)
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TrivialExecService.java,55,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TrivialExecService.java,56,"  private void logDebugCmd(List<String> cmd,"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TrivialExecService.java,57,"    Map<String, String> environmentVariables) {"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TrivialExecService.java,58,    if(!LOG.isDebugEnabled()){
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TrivialExecService.java,59,      return;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TrivialExecService.java,60,    }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TrivialExecService.java,61,"    LOG.debug(""starting "" + cmd);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TrivialExecService.java,62,"    LOG.debug(""With environment variables: "" );"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TrivialExecService.java,63,"    for(Map.Entry<String, String> keyVal : environmentVariables.entrySet()){"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TrivialExecService.java,64,"      LOG.debug(keyVal.getKey() + ""="" + keyVal.getValue());"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TrivialExecService.java,65,    }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TrivialExecService.java,66,"    LOG.debug(""With environment variables already set: "" );"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TrivialExecService.java,67,"    Map<String, String> env = System.getenv();"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TrivialExecService.java,68,    for (String envName : env.keySet()) {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TrivialExecService.java,69,"      LOG.debug(envName + ""="" + env.get(envName));"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TrivialExecService.java,70,    }
hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/tool/TestTrivialExecService.java,41,"          new HashMap<String, String>());"
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,34,    private JobSubmissionProtocol cnx;
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,36,    /**
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,37,     * Create a connection to the Job Tracker.
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,38,     */
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,39,"    public WebHCatJTShim20S(Configuration conf, UserGroupInformation ugi)"
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,40,            throws IOException {
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,41,      cnx = (JobSubmissionProtocol)
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,42,"              RPC.getProxy(JobSubmissionProtocol.class,"
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,43,"                      JobSubmissionProtocol.versionID,"
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,44,"                      getAddress(conf),"
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,45,"                      ugi,"
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,46,"                      conf,"
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,47,"                      NetUtils.getSocketFactory(conf,"
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,48,                              JobSubmissionProtocol.class));
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,49,    }
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,51,    /**
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,52,     * Grab a handle to a job that is already known to the JobTracker.
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,53,     *
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,54,"     * @return Profile of the job, or null if not found."
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,55,     */
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,56,    public JobProfile getJobProfile(org.apache.hadoop.mapred.JobID jobid)
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,57,            throws IOException {
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,58,      return cnx.getJobProfile(jobid);
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,59,    }
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,61,    /**
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,62,     * Grab a handle to a job that is already known to the JobTracker.
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,63,     *
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,64,"     * @return Status of the job, or null if not found."
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,65,     */
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,66,    public org.apache.hadoop.mapred.JobStatus getJobStatus(org.apache.hadoop.mapred.JobID jobid)
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,67,            throws IOException {
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,68,      return cnx.getJobStatus(jobid);
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,69,    }
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,72,    /**
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,73,     * Kill a job.
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,74,     */
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,75,    public void killJob(org.apache.hadoop.mapred.JobID jobid)
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,76,            throws IOException {
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,77,      cnx.killJob(jobid);
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,78,    }
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,80,    /**
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,81,     * Get all the jobs submitted.
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,82,     */
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,83,    public org.apache.hadoop.mapred.JobStatus[] getAllJobs()
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,84,            throws IOException {
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,85,      return cnx.getAllJobs();
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,86,    }
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,88,    /**
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,89,     * Close the connection to the Job Tracker.
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,90,     */
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,91,    public void close() {
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,92,      RPC.stopProxy(cnx);
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,93,    }
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,94,    private InetSocketAddress getAddress(Configuration conf) {
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,95,"      String jobTrackerStr = conf.get(""mapred.job.tracker"", ""localhost:8012"");"
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,96,      return NetUtils.createSocketAddr(jobTrackerStr);
shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java,97,    }
service/src/java/org/apache/hive/service/cli/operation/ExecuteStatementOperation.java,43,    this.confOverlay = confOverlay;
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,83,  private void runInternal() throws HiveSQLException {
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,94,"      driver = new Driver(getParentSession().getHiveConf(), getParentSession().getUserName());"
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,100,"      String subStatement = new VariableSubstitution().substitute(getParentSession().getHiveConf(), statement);"
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,146,      runInternal();
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,154,            runInternal();
jdbc/src/java/org/apache/hive/jdbc/HiveBaseResultSet.java,618,"    throw new SQLException(""Method not supported"");"
jdbc/src/java/org/apache/hive/jdbc/HiveCallableStatement.java,53,  public HiveCallableStatement() {
jdbc/src/java/org/apache/hive/jdbc/HiveCallableStatement.java,54,    // TODO Auto-generated constructor stub
jdbc/src/java/org/apache/hive/jdbc/HiveCallableStatement.java,1390,    return new HiveQueryResultSet.Builder().build();
jdbc/src/java/org/apache/hive/jdbc/HiveCallableStatement.java,2172,    // TODO Auto-generated method stub
jdbc/src/java/org/apache/hive/jdbc/HiveCallableStatement.java,2173,"    throw new SQLException(""Method not supported"");"
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,418,"    return new HiveStatement(client, sessHandle);"
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,517,"    return new HiveDatabaseMetaData(client, sessHandle);"
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,646,"    return new HivePreparedStatement(client, sessHandle, sql);"
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,657,"    return new HivePreparedStatement(client, sessHandle, sql);"
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,693,"    return new HivePreparedStatement(client, sessHandle, sql);"
jdbc/src/java/org/apache/hive/jdbc/HiveDatabaseMetaData.java,70,"  public HiveDatabaseMetaData(TCLIService.Iface client, TSessionHandle sessHandle) {"
jdbc/src/java/org/apache/hive/jdbc/HiveDatabaseMetaData.java,131,    return new HiveQueryResultSet.Builder()
jdbc/src/java/org/apache/hive/jdbc/HiveDatabaseMetaData.java,215,    return new HiveQueryResultSet.Builder()
jdbc/src/java/org/apache/hive/jdbc/HiveDatabaseMetaData.java,244,"    throw new SQLException(""Method not supported"");"
jdbc/src/java/org/apache/hive/jdbc/HiveDatabaseMetaData.java,330,    return new HiveQueryResultSet.Builder()
jdbc/src/java/org/apache/hive/jdbc/HiveDatabaseMetaData.java,343,    return new HiveQueryResultSet.Builder()
jdbc/src/java/org/apache/hive/jdbc/HiveDatabaseMetaData.java,485,    return new HiveQueryResultSet.Builder().setClient(client).setEmptyResultSet(true).
jdbc/src/java/org/apache/hive/jdbc/HiveDatabaseMetaData.java,496,    return new HiveQueryResultSet.Builder().setClient(client).setEmptyResultSet(true).
jdbc/src/java/org/apache/hive/jdbc/HiveDatabaseMetaData.java,517,    return new HiveQueryResultSet.Builder().setClient(client).setEmptyResultSet(true).
jdbc/src/java/org/apache/hive/jdbc/HiveDatabaseMetaData.java,571,    return new HiveQueryResultSet.Builder()
jdbc/src/java/org/apache/hive/jdbc/HiveDatabaseMetaData.java,615,    return new HiveQueryResultSet.Builder()
jdbc/src/java/org/apache/hive/jdbc/HiveDatabaseMetaData.java,648,    return new HiveQueryResultSet.Builder()
jdbc/src/java/org/apache/hive/jdbc/HiveDatabaseMetaData.java,704,    return new HiveQueryResultSet.Builder()
jdbc/src/java/org/apache/hive/jdbc/HiveDatabaseMetaData.java,1126,"    HiveDatabaseMetaData meta = new HiveDatabaseMetaData(null, null);"
jdbc/src/java/org/apache/hive/jdbc/HivePreparedStatement.java,58,"  public HivePreparedStatement(TCLIService.Iface client, TSessionHandle sessHandle,"
jdbc/src/java/org/apache/hive/jdbc/HivePreparedStatement.java,59,      String sql) {
jdbc/src/java/org/apache/hive/jdbc/HivePreparedStatement.java,60,"    super(client, sessHandle);"
jdbc/src/java/org/apache/hive/jdbc/HiveQueryResultSet.java,58,  private HiveStatement hiveStatement;
jdbc/src/java/org/apache/hive/jdbc/HiveQueryResultSet.java,74,    private HiveStatement hiveStatement = null;
jdbc/src/java/org/apache/hive/jdbc/HiveQueryResultSet.java,104,    public Builder setHiveStatement(HiveStatement hiveStatement) {
jdbc/src/java/org/apache/hive/jdbc/HiveQueryResultSet.java,105,      this.hiveStatement = hiveStatement;
jdbc/src/java/org/apache/hive/jdbc/HiveQueryResultSet.java,106,      return this;
jdbc/src/java/org/apache/hive/jdbc/HiveQueryResultSet.java,107,    }
jdbc/src/java/org/apache/hive/jdbc/HiveQueryResultSet.java,108,
jdbc/src/java/org/apache/hive/jdbc/HiveQueryResultSet.java,156,    this.hiveStatement = builder.hiveStatement;
jdbc/src/java/org/apache/hive/jdbc/HiveQueryResultSet.java,255,    if (hiveStatement != null) {
jdbc/src/java/org/apache/hive/jdbc/HiveQueryResultSet.java,256,      hiveStatement.closeClientOperation();
jdbc/src/java/org/apache/hive/jdbc/HiveQueryResultSet.java,261,    hiveStatement = null;
jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java,79,"  public HiveStatement(TCLIService.Iface client, TSessionHandle sessHandle) {"
jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java,247,    resultSet =  new HiveQueryResultSet.Builder().setClient(client).setSessionHandle(sessHandle)
jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java,248,        .setStmtHandle(stmtHandle).setHiveStatement(this).setMaxRows(maxRows).setFetchSize(fetchSize)
jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java,354,"    throw new SQLException(""Method not supported"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPMod.java,64,    if ((a == null) || (b == null)) {
ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPMod.java,76,    if ((a == null) || (b == null)) {
ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPMod.java,88,    if ((a == null) || (b == null)) {
ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPMod.java,100,    if ((a == null) || (b == null)) {
ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPMod.java,112,    if ((a == null) || (b == null)) {
ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPMod.java,124,    if ((a == null) || (b == null)) {
ql/src/java/org/apache/hadoop/hive/ql/udf/UDFPosMod.java,50,    if ((a == null) || (b == null)) {
ql/src/java/org/apache/hadoop/hive/ql/udf/UDFPosMod.java,62,    if ((a == null) || (b == null)) {
ql/src/java/org/apache/hadoop/hive/ql/udf/UDFPosMod.java,74,    if ((a == null) || (b == null)) {
ql/src/java/org/apache/hadoop/hive/ql/udf/UDFPosMod.java,86,    if ((a == null) || (b == null)) {
ql/src/java/org/apache/hadoop/hive/ql/udf/UDFPosMod.java,98,    if ((a == null) || (b == null)) {
ql/src/java/org/apache/hadoop/hive/ql/udf/UDFPosMod.java,110,    if ((a == null) || (b == null)) {
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,152,  /**
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,153,   * The conf variables that depends on current user
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,154,   */
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,155,  public static final HiveConf.ConfVars[] userVars = {
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,156,"    HiveConf.ConfVars.SCRATCHDIR,"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,157,"    HiveConf.ConfVars.LOCALSCRATCHDIR,"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,158,"    HiveConf.ConfVars.DOWNLOADED_RESOURCES_DIR,"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,159,    HiveConf.ConfVars.HIVEHISTORYFILELOC
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,160,  };
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,161,
service/src/java/org/apache/hive/service/cli/CLIService.java,80,    // Initialize and test a connection to the metastore
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,21,import java.io.File;
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,25,import org.apache.hadoop.hive.conf.HiveConf;
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,49,    setUserPath(username);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,141,  // Append the user name to temp/scratch directory path for each impersonated user
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,142,  private void setUserPath(String userName) {
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,143,    for (HiveConf.ConfVars var: HiveConf.userVars) {
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,144,      String userVar = getHiveConf().getVar(var);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,145,      if (userVar != null) {
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,146,        // If there's a path separator at end then remove it
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,147,        if (userVar.endsWith(File.separator)) {
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,148,"          userVar = userVar.substring(0, userVar.length()-2);"
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,149,        }
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,150,"        getHiveConf().setVar(var, userVar + ""-"" + userName);"
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,151,      }
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,152,    }
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,153,  }
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,154,
service/src/test/org/apache/hive/service/cli/TestEmbeddedThriftBinaryCLIService.java,33,
service/src/test/org/apache/hive/service/cli/TestEmbeddedThriftBinaryCLIService.java,34,  private static ThriftCLIService service;
ql/src/test/org/apache/hadoop/hive/ql/udf/TestUDFRound.java,11,import org.apache.hadoop.hive.ql.testutil.BaseScalarUdfTest;
ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFConcat.java,11,import org.apache.hadoop.hive.ql.testutil.BaseScalarUdfTest;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/DeleteDelegator.java,50,      String childid = state.getChildId();
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/DeleteDelegator.java,51,      if (childid != null)
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/DeleteDelegator.java,52,        tracker.killJob(StatusDelegator.StringToJobID(childid));
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/ListDelegator.java,29,import org.apache.hive.hcatalog.templeton.tool.JobState;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/ListDelegator.java,53,          JobState state = null;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/ListDelegator.java,54,          try {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/ListDelegator.java,55,            String id = job.getJobID().toString();
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/ListDelegator.java,56,"            state = new JobState(id, Main.getAppConfigInstance());"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/ListDelegator.java,57,            if (showall || user.equals(state.getUser()))
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/ListDelegator.java,58,              ids.add(id);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/ListDelegator.java,59,          } finally {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/ListDelegator.java,60,            if (state != null) {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/ListDelegator.java,61,              state.close();
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/ListDelegator.java,62,            }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/ListDelegator.java,63,          }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/QueueStatusBean.java,60,    parentId = state.getId();
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/QueueStatusBean.java,61,    if (id.equals(parentId))
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/QueueStatusBean.java,62,      parentId = null;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/QueueStatusBean.java,65,    user = state.getUser();
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/StatusDelegator.java,34, * Fetch the status of a given job id in the queue.
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/StatusDelegator.java,66,"  public static QueueStatusBean makeStatus(WebHCatJTShim tracker,"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/StatusDelegator.java,68,"                       String childid,"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/StatusDelegator.java,71,    JobID bestid = jobid;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/StatusDelegator.java,72,    if (childid != null)
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/StatusDelegator.java,73,      bestid = StatusDelegator.StringToJobID(childid);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/StatusDelegator.java,74,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/StatusDelegator.java,75,    JobStatus status = tracker.getJobStatus(bestid);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/StatusDelegator.java,76,    JobProfile profile = tracker.getJobProfile(bestid);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/StatusDelegator.java,77,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/StatusDelegator.java,78,    if (status == null || profile == null) {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/StatusDelegator.java,79,"      if (bestid != jobid) { // Corrupt childid, retry."
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/StatusDelegator.java,80,"        LOG.error(""Corrupt child id "" + childid + "" for "" + jobid);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/StatusDelegator.java,81,        bestid = jobid;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/StatusDelegator.java,82,        status = tracker.getJobStatus(bestid);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/StatusDelegator.java,83,        profile = tracker.getJobProfile(bestid);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/StatusDelegator.java,84,      }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/StatusDelegator.java,85,    }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/StatusDelegator.java,88,"      throw new BadParam(""Could not find job "" + bestid);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/StatusDelegator.java,93,"  public static QueueStatusBean makeStatus(WebHCatJTShim tracker,"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/StatusDelegator.java,94,"                       JobID jobid,"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/StatusDelegator.java,95,                       JobState state)
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/StatusDelegator.java,96,"    throws BadParam, IOException {"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/StatusDelegator.java,97,"    return makeStatus(tracker, jobid, state.getChildId(), state);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/StatusDelegator.java,98,  }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/StatusDelegator.java,99,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobState.java,128,  /**
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobState.java,129,   * The child id of TempletonControllerJob
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobState.java,130,   */
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobState.java,131,  public String getChildId()
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobState.java,132,    throws IOException {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobState.java,133,"    return getField(""childid"");"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobState.java,134,  }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobState.java,135,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobState.java,136,  public void setChildId(String childid)
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobState.java,137,    throws IOException {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobState.java,138,"    setField(""childid"", childid);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobState.java,139,  }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobState.java,140,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobState.java,173,  /**
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobState.java,174,   * Save a comma-separated list of jobids that are children
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobState.java,175,   * of this job.
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobState.java,176,   * @param jobids
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobState.java,177,   * @throws IOException
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobState.java,178,   */
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobState.java,179,  public void setChildren(String jobids) throws IOException {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobState.java,180,"    setField(""children"", jobids);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobState.java,181,  }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobState.java,182,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobState.java,183,  /**
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobState.java,184,   * Set the list of child jobs of this job
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobState.java,185,   * @param children
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobState.java,186,   */
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobState.java,187,  public void setChildren(List<JobState> children) throws IOException {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobState.java,188,"    String val = """";"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobState.java,189,    for (JobState jobstate : children) {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobState.java,190,"      if (!val.equals("""")) {"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobState.java,191,"        val += "","";"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobState.java,192,      }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobState.java,193,      val += jobstate.getId();
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobState.java,194,    }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobState.java,195,"    setField(""children"", val);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobState.java,196,  }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobState.java,197,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/LaunchMapper.java,264,              state.setPercentComplete(percent);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/LaunchMapper.java,265,              state.setChildId(childid);
ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java,259,    tss.set(startSs);
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,152,          SessionState.start(ss);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,111,    SessionState.start(sessionState);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,315,      operationManager.closeOperation(opHandle);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/HiveDelegator.java,131,"      args.add(""-archives"");"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/JarDelegator.java,46,"               boolean usehcatalog, String completedUrl,"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/JarDelegator.java,53,"      statusdir, usehcatalog, completedUrl, enablelog, jobType);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/JarDelegator.java,61,"                  String statusdir, boolean usehcatalog, String completedUrl,"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/JarDelegator.java,75,      if(usehcatalog){
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/PigDelegator.java,50,"               boolean usehcatalog, String completedUrl, boolean enablelog)"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/PigDelegator.java,56,"      otherFiles, statusdir, usehcatalog, completedUrl, enablelog);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/PigDelegator.java,67,   * @param usehcatalog whether the command uses hcatalog/needs to connect
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/PigDelegator.java,77,"                  String statusdir, boolean usehcatalog,"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/PigDelegator.java,92,"      if (appConf.pigArchive() != null && !appConf.pigArchive().equals(""""))"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/PigDelegator.java,93,      {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/PigDelegator.java,94,"        args.add(""-archives"");"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/PigDelegator.java,95,        args.add(appConf.pigArchive());
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/PigDelegator.java,97,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/PigDelegator.java,107,      //check if the REST command specified explicitly to use hcatalog
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/PigDelegator.java,108,      // or if it says that implicitly using the pig -useHCatalog arg
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/PigDelegator.java,109,      if(usehcatalog || hasPigArgUseHcat(pigArgs)){
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/Server.java,638,"   * @param  usehcatalog if {@code true}, means the Jar uses HCat and thus needs to access"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/Server.java,654,"                  @FormParam(""usehcatalog"") boolean usehcatalog,"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/Server.java,680,"      statusdir, callback, usehcatalog, getCompletedUrl(), enablelog, JobType.JAR);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/Server.java,685,"   * Params correspond to the REST api params.  If '-useHCatalog' is in the {@code pigArgs, usehcatalog},"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/Server.java,687,"   * @param  usehcatalog if {@code true}, means the Pig script uses HCat and thus needs to access"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/Server.java,702,"               @FormParam(""usehcatalog"") boolean usehcatalog,"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/Server.java,728,"      statusdir, callback, usehcatalog, getCompletedUrl(), enablelog);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/LaunchMapper.java,68,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/LaunchMapper.java,69,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/LaunchMapper.java,82,"    Map<String, String> env = TempletonUtils.hadoopUserEnv(user,"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/LaunchMapper.java,83,            overrideClasspath);
hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/HCatCli.java,79,    SessionState.start(ss);
hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/HCatCli.java,80,
hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/HCatCli.java,129,"      printUsage(options, ss.err);"
hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/HCatCli.java,132,    // -e
hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/HCatCli.java,133,    String execString = (String) cmdLine.getOptionValue('e');
hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/HCatCli.java,134,    // -f
hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/HCatCli.java,135,    String fileName = (String) cmdLine.getOptionValue('f');
hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/HCatCli.java,160,    // -D
hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/HCatCli.java,161,"    setConfProperties(conf, cmdLine.getOptionProperties(""D""));"
ql/src/java/org/apache/hadoop/hive/ql/security/HadoopDefaultAuthenticator.java,31,  private String userName;
ql/src/java/org/apache/hadoop/hive/ql/security/HadoopDefaultAuthenticator.java,32,  private List<String> groupNames;
ql/src/java/org/apache/hadoop/hive/ql/security/HadoopDefaultAuthenticator.java,34,  private Configuration conf;
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java,96,  private static String getQuorumServers(HiveConf conf) {
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java,97,    String hosts = conf.getVar(HiveConf.ConfVars.HIVE_ZOOKEEPER_QUORUM);
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java,99,"    return hosts + "":"" + port;"
serde/src/test/org/apache/hadoop/hive/serde2/columnar/TestLazyBinaryColumnarSerDe.java,242,}
service/src/java/org/apache/hive/service/cli/session/SessionManager.java,21,import java.util.HashMap;
service/src/java/org/apache/hive/service/cli/session/SessionManager.java,46,"  private final Map<SessionHandle, HiveSession> handleToSession = new HashMap<SessionHandle, HiveSession>();"
service/src/java/org/apache/hive/service/cli/session/SessionManager.java,47,  private OperationManager operationManager = new OperationManager();
service/src/java/org/apache/hive/service/cli/session/SessionManager.java,48,  private static final Object sessionMapLock = new Object();
service/src/java/org/apache/hive/service/cli/session/SessionManager.java,58,    operationManager = new OperationManager();
service/src/java/org/apache/hive/service/cli/session/SessionManager.java,116,    synchronized(sessionMapLock) {
service/src/java/org/apache/hive/service/cli/session/SessionManager.java,117,"      handleToSession.put(session.getSessionHandle(), session);"
service/src/java/org/apache/hive/service/cli/session/SessionManager.java,118,    }
service/src/java/org/apache/hive/service/cli/session/SessionManager.java,128,    HiveSession session;
service/src/java/org/apache/hive/service/cli/session/SessionManager.java,129,    synchronized(sessionMapLock) {
service/src/java/org/apache/hive/service/cli/session/SessionManager.java,130,      session = handleToSession.remove(sessionHandle);
service/src/java/org/apache/hive/service/cli/session/SessionManager.java,131,    }
service/src/java/org/apache/hive/service/cli/session/SessionManager.java,139,    HiveSession session;
service/src/java/org/apache/hive/service/cli/session/SessionManager.java,140,    synchronized(sessionMapLock) {
service/src/java/org/apache/hive/service/cli/session/SessionManager.java,141,      session = handleToSession.get(sessionHandle);
service/src/java/org/apache/hive/service/cli/session/SessionManager.java,142,    }
shims/0.23/src/main/java/org/apache/hadoop/mapred/WebHCatJTShim23.java,34,"  public WebHCatJTShim23(Configuration conf, final UserGroupInformation ugi)"
shims/0.23/src/main/java/org/apache/hadoop/mapred/WebHCatJTShim23.java,36,
shims/0.23/src/main/java/org/apache/hadoop/mapred/WebHCatJTShim23.java,37,    jc = new JobClient(conf);
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapredLocalTask.java,233,          conf.getBoolVar(HiveConf.ConfVars.HIVE_SERVER2_ENABLE_DOAS) == true
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapredLocalTask.java,234,          ){
ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeGenericFuncDesc.java,136,    sb.append(genericUDF.getClass().toString());
ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeGenericFuncDesc.java,144,"    sb.append(""("");"
ql/src/test/org/apache/hadoop/hive/ql/io/TestRCFile.java,138,"      patialS.set(5, new BytesRefWritable(""NULL"".getBytes(""UTF-8"")));"
ql/src/test/org/apache/hadoop/hive/ql/io/TestRCFile.java,140,"      patialS.set(7, new BytesRefWritable(""NULL"".getBytes(""UTF-8"")));"
serde/src/java/org/apache/hadoop/hive/serde2/columnar/BytesRefArrayWritable.java,142,      if (other.contains(bytesRefWritables[i])) {
serde/src/java/org/apache/hadoop/hive/serde2/columnar/BytesRefArrayWritable.java,143,        continue;
serde/src/java/org/apache/hadoop/hive/serde2/columnar/BytesRefArrayWritable.java,144,      } else {
serde/src/java/org/apache/hadoop/hive/serde2/columnar/BytesRefArrayWritable.java,145,        return 1;
serde/src/java/org/apache/hadoop/hive/serde2/columnar/BytesRefArrayWritable.java,151,  /**
serde/src/java/org/apache/hadoop/hive/serde2/columnar/BytesRefArrayWritable.java,152,   * Returns <tt>true</tt> if this instance contains one or more the specified
serde/src/java/org/apache/hadoop/hive/serde2/columnar/BytesRefArrayWritable.java,153,   * BytesRefWritable.
serde/src/java/org/apache/hadoop/hive/serde2/columnar/BytesRefArrayWritable.java,154,   *
serde/src/java/org/apache/hadoop/hive/serde2/columnar/BytesRefArrayWritable.java,155,   * @param bytesRefWritable
serde/src/java/org/apache/hadoop/hive/serde2/columnar/BytesRefArrayWritable.java,156,   *          BytesRefWritable element to be tested
serde/src/java/org/apache/hadoop/hive/serde2/columnar/BytesRefArrayWritable.java,157,   * @return <tt>true</tt> if contains the specified element
serde/src/java/org/apache/hadoop/hive/serde2/columnar/BytesRefArrayWritable.java,158,   * @throws IllegalArgumentException
serde/src/java/org/apache/hadoop/hive/serde2/columnar/BytesRefArrayWritable.java,159,   *           if the specified element is null
serde/src/java/org/apache/hadoop/hive/serde2/columnar/BytesRefArrayWritable.java,160,   */
serde/src/java/org/apache/hadoop/hive/serde2/columnar/BytesRefArrayWritable.java,161,  public boolean contains(BytesRefWritable bytesRefWritable) {
serde/src/java/org/apache/hadoop/hive/serde2/columnar/BytesRefArrayWritable.java,162,    if (bytesRefWritable == null) {
serde/src/java/org/apache/hadoop/hive/serde2/columnar/BytesRefArrayWritable.java,163,"      throw new IllegalArgumentException(""Argument can not be null."");"
serde/src/java/org/apache/hadoop/hive/serde2/columnar/BytesRefArrayWritable.java,164,    }
serde/src/java/org/apache/hadoop/hive/serde2/columnar/BytesRefArrayWritable.java,165,    for (int i = 0; i < valid; i++) {
serde/src/java/org/apache/hadoop/hive/serde2/columnar/BytesRefArrayWritable.java,166,      if (bytesRefWritables[i].equals(bytesRefWritable)) {
serde/src/java/org/apache/hadoop/hive/serde2/columnar/BytesRefArrayWritable.java,167,        return true;
serde/src/java/org/apache/hadoop/hive/serde2/columnar/BytesRefArrayWritable.java,168,      }
serde/src/java/org/apache/hadoop/hive/serde2/columnar/BytesRefArrayWritable.java,169,    }
serde/src/java/org/apache/hadoop/hive/serde2/columnar/BytesRefArrayWritable.java,170,    return false;
serde/src/java/org/apache/hadoop/hive/serde2/columnar/BytesRefArrayWritable.java,171,  }
serde/src/java/org/apache/hadoop/hive/serde2/columnar/BytesRefArrayWritable.java,172,
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,904,  public CommandProcessorResponse run(String command) throws CommandNeedRetryException {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,905,    CommandProcessorResponse cpr = runInternal(command);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,960,  private CommandProcessorResponse runInternal(String command) throws CommandNeedRetryException {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,993,    synchronized (compileMonitor) {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,994,      ret = compile(command);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,995,    }
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,996,    if (ret != 0) {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,997,      releaseLocks(ctx.getHiveLocks());
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,998,"      return new CommandProcessorResponse(ret, errorMessage, SQLState);"
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,79,
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,80,  public void prepare() throws HiveSQLException {
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,81,  }
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,82,
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,83,  private void runInternal(HiveConf sqlOperationConf) throws HiveSQLException {
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,85,    String statement_trimmed = statement.trim();
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,86,"    String[] tokens = statement_trimmed.split(""\\s"");"
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,87,    String cmd_1 = statement_trimmed.substring(tokens[0].length()).trim();
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,88,
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,89,    int ret = 0;
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,90,"    String errorMessage = """";"
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,91,    String SQLState = null;
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,101,
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,102,      response = driver.run(subStatement);
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,104,"        throw new HiveSQLException(""Error while processing statement: """
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,115,"          throw new HiveSQLException(""Error running query: Schema and FieldSchema "" +"
service/src/test/org/apache/hive/service/cli/CLIServiceTest.java,158,    OperationHandle ophandle;
service/src/test/org/apache/hive/service/cli/CLIServiceTest.java,174,"    String wrongQueryString = ""SELECT NAME FROM TEST_EXEC"";"
service/src/test/org/apache/hive/service/cli/CLIServiceTest.java,175,"    ophandle = client.executeStatementAsync(sessionHandle, wrongQueryString, confOverlay);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCorrelation.java,22,import org.apache.commons.logging.Log;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCorrelation.java,23,import org.apache.commons.logging.LogFactory;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCorrelation.java,77,  static final Log LOG = LogFactory.getLog(GenericUDAFCorrelation.class.getName());
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCorrelation.java,78,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCorrelation.java,292,        double xavgOld = myagg.xavg;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCorrelation.java,293,        double yavgOld = myagg.yavg;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCorrelation.java,295,        myagg.xavg += (vx - xavgOld) / myagg.count;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCorrelation.java,296,        myagg.yavg += (vy - yavgOld) / myagg.count;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCorrelation.java,298,            myagg.covar += (vx - xavgOld) * (vy - myagg.yavg);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCorrelation.java,299,            myagg.xvar += (vx - xavgOld) * (vx - myagg.xavg);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCorrelation.java,300,            myagg.yvar += (vy - yavgOld) * (vy - myagg.yavg);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCorrelation.java,355,          myagg.xvar += xvarB + (xavgA - xavgB) * (xavgA - xavgB) * myagg.count;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCorrelation.java,356,          myagg.yvar += yvarB + (yavgA - yavgB) * (yavgA - yavgB) * myagg.count;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java,455,    if (start < 0) {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/JarDelegator.java,97,"      //the token file location comes after mainClass, as a -Dprop=val"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/JarDelegator.java,98,"      args.add(""-D"" + TempletonControllerJob.TOKEN_FILE_ARG_PLACEHOLDER);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/JarDelegator.java,102,        TempletonUtils.quoteForWindows(d);
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,293,          if (Utilities.ReduceField.KEY.name().equals(names[0])) {
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,317,            // will be VALUE._COLx
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,694,      // update non-distinct value aggregations: 'VALUE._colx'
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcWithMiniHS2.java,55,      miniHS2.start();
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcWithMiniHS2.java,63,      miniHS2.stop();
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,145,    configureConnection();
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,279,  private void configureConnection() throws SQLException {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/GenMRSkewJoinProcessor.java,341,    cndTsk
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/GenMRSkewJoinProcessor.java,342,        .setResolverCtx(new ConditionalResolverSkewJoin.ConditionalResolverSkewJoinCtx(
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/GenMRSkewJoinProcessor.java,343,        bigKeysDirToTaskMap));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/GenMRSkewJoinProcessor.java,344,    List<Task<? extends Serializable>> oldChildTasks = currTask.getChildTasks();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/GenMRSkewJoinProcessor.java,348,    if (oldChildTasks != null) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/GenMRSkewJoinProcessor.java,349,      for (Task<? extends Serializable> tsk : cndTsk.getListTasks()) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/GenMRSkewJoinProcessor.java,350,        for (Task<? extends Serializable> oldChild : oldChildTasks) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/GenMRSkewJoinProcessor.java,351,          tsk.addDependentTask(oldChild);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/GenMRSkewJoinProcessor.java,352,        }
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/GenMRSkewJoinProcessor.java,353,      }
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/GenMRSkewJoinProcessor.java,354,    }
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverSkewJoin.java,53,"    HashMap<String, Task<? extends Serializable>> dirToTaskMap;"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverSkewJoin.java,62,"        HashMap<String, Task<? extends Serializable>> dirToTaskMap) {"
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,748,      // Force string-based handling in some cases to be compatible with JDO pushdown.
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,749,      boolean forceStringEq = !isStringCol && node.canJdoUseStringsWithIntegral();
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,750,
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,768,      if (!isStringCol && !forceStringEq) {
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,777,      params.add(forceStringEq ? node.value.toString() : node.value);
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2202,"    tree.generateJDOFilterFragment(table, params, queryBuilder);"
metastore/src/java/org/apache/hadoop/hive/metastore/parser/ExpressionTree.java,28,import org.apache.hadoop.hive.metastore.ObjectStore;
metastore/src/java/org/apache/hadoop/hive/metastore/parser/ExpressionTree.java,234,"    public void generateJDOFilter(Table table, Map<String, Object> params,"
metastore/src/java/org/apache/hadoop/hive/metastore/parser/ExpressionTree.java,235,        FilterBuilder filterBuffer) throws MetaException {
metastore/src/java/org/apache/hadoop/hive/metastore/parser/ExpressionTree.java,239,"        lhs.generateJDOFilter(table, params, filterBuffer);"
metastore/src/java/org/apache/hadoop/hive/metastore/parser/ExpressionTree.java,248,"          rhs.generateJDOFilter(table, params, filterBuffer);"
metastore/src/java/org/apache/hadoop/hive/metastore/parser/ExpressionTree.java,272,"    public void generateJDOFilter(Table table, Map<String, Object> params,"
metastore/src/java/org/apache/hadoop/hive/metastore/parser/ExpressionTree.java,275,"        generateJDOFilterOverPartitions(table, params, filterBuilder);"
metastore/src/java/org/apache/hadoop/hive/metastore/parser/ExpressionTree.java,345,"    private void generateJDOFilterOverPartitions(Table table, Map<String, Object> params,"
metastore/src/java/org/apache/hadoop/hive/metastore/parser/ExpressionTree.java,346,        FilterBuilder filterBuilder) throws MetaException {
metastore/src/java/org/apache/hadoop/hive/metastore/parser/ExpressionTree.java,351,"      String valueAsString = getJdoFilterPushdownParam(table, partitionColumnIndex, filterBuilder);"
metastore/src/java/org/apache/hadoop/hive/metastore/parser/ExpressionTree.java,438,    private String getJdoFilterPushdownParam(
metastore/src/java/org/apache/hadoop/hive/metastore/parser/ExpressionTree.java,439,"        Table table, int partColIndex, FilterBuilder filterBuilder) throws MetaException {"
metastore/src/java/org/apache/hadoop/hive/metastore/parser/ExpressionTree.java,440,      boolean isIntegralSupported = canJdoUseStringsWithIntegral();
metastore/src/java/org/apache/hadoop/hive/metastore/parser/ExpressionTree.java,570,"  public void generateJDOFilterFragment(Table table, Map<String, Object> params,"
metastore/src/java/org/apache/hadoop/hive/metastore/parser/ExpressionTree.java,571,      FilterBuilder filterBuilder) throws MetaException {
metastore/src/java/org/apache/hadoop/hive/metastore/parser/ExpressionTree.java,577,"    root.generateJDOFilter(table, params, filterBuilder);"
hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/HCatBaseTest.java,44,"  protected static final String TEST_DATA_DIR = System.getProperty(""user.dir"") +"
hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/HCatBaseTest.java,45,"      ""/build/test/data/"" + HCatBaseTest.class.getCanonicalName();"
hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestHCatPartitionPublish.java,86,"    System.setProperty(""hadoop.log.dir"", new File(fs.getWorkingDirectory()"
hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestHCatPartitionPublish.java,87,"        .toString(), ""/logs"").getAbsolutePath());"
hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestMultiOutputFormat.java,70,  private static Configuration mrConf = null;
hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestMultiOutputFormat.java,85,"    mrCluster = new MiniMRCluster(1, fs.getUri().toString(), 1, null, null,"
hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestMultiOutputFormat.java,86,      new JobConf(conf));
hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestMultiOutputFormat.java,87,    mrConf = mrCluster.createJobConf();
hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/HCatBaseTest.java,43,  protected static final String TEST_DATA_DIR =
hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/HCatBaseTest.java,44,"      ""/tmp/build/test/data/"" + HCatBaseTest.class.getCanonicalName();"
hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatPartitionPublish.java,79,  @BeforeClass
hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatPartitionPublish.java,80,  public static void setup() throws Exception {
hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatPartitionPublish.java,81,"    String testDir = System.getProperty(""test.tmp.dir"", ""./"");"
hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestMultiOutputFormat.java,67,  private static Configuration mrConf = null;
hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestMultiOutputFormat.java,82,"    mrCluster = new MiniMRCluster(1, fs.getUri().toString(), 1, null, null,"
hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestMultiOutputFormat.java,83,      new JobConf(conf));
hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestMultiOutputFormat.java,84,    mrConf = mrCluster.createJobConf();
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hcatalog/pig/TestHCatLoader.java,63,"  private static final String TEST_DATA_DIR = System.getProperty(""java.io.tmpdir"") + File.separator"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hcatalog/pig/TestHCatLoader.java,64,"      + TestHCatLoader.class.getCanonicalName() + ""-"" + System.currentTimeMillis();"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hcatalog/pig/TestHCatLoader.java,412,"      "" stored as textfile location 'file://"" +"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hcatalog/pig/TestHCatLoader.java,413,"      inputDataDir.getAbsolutePath() + ""'"").getResponseCode());"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hcatalog/pig/TestHCatLoaderStorer.java,72,"      dataDir.getAbsolutePath() + ""' into table "" + readTblName).getResponseCode());"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hcatalog/pig/TestHCatLoaderStorer.java,107,"    smallTinyIntBoundsCheckHelper(writeDataFile.getAbsolutePath(), ExecJob.JOB_STATUS.COMPLETED);"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hcatalog/pig/TestHCatStorerMulti.java,44,"  private static final String TEST_DATA_DIR = System.getProperty(""user.dir"") +"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hcatalog/pig/TestHCatStorerMulti.java,45,"    ""/build/test/data/"" + TestHCatStorerMulti.class.getCanonicalName();"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hcatalog/pig/TestHCatStorerWrapper.java,51,"    File tmpExternalDir = new File(SystemUtils.getJavaIoTmpDir(), UUID.randomUUID().toString());"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hcatalog/pig/TestHCatStorerWrapper.java,74,"        + ""('c="" + part_val + ""','"" + tmpExternalDir.getAbsolutePath() + ""');"");"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hcatalog/pig/TestHCatStorerWrapper.java,78,"    Assert.assertTrue(new File(tmpExternalDir.getAbsoluteFile() + ""/"" + ""part-m-00000"").exists());"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatLoader.java,60,"  private static final String TEST_DATA_DIR = System.getProperty(""java.io.tmpdir"") + File.separator"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatLoader.java,61,"      + TestHCatLoader.class.getCanonicalName() + ""-"" + System.currentTimeMillis();"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatLoader.java,423,"      "" stored as textfile location 'file://"" +"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatStorerMulti.java,41,  private static final String TEST_DATA_DIR =
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatStorerMulti.java,42,"    ""/tmp/build/test/data/"" + TestHCatStorerMulti.class.getCanonicalName();"
hcatalog/storage-handlers/hbase/src/test/org/apache/hcatalog/hbase/SkeletonHBaseTest.java,168,      try {
hcatalog/storage-handlers/hbase/src/test/org/apache/hcatalog/hbase/SkeletonHBaseTest.java,169,"        testDir = new File(TEST_DIR + ""/test_"" + handle + ""_"" + Math.abs(new Random().nextLong()) + ""/"").getCanonicalPath();"
hcatalog/storage-handlers/hbase/src/test/org/apache/hcatalog/hbase/SkeletonHBaseTest.java,170,"        System.out.println(""Cluster work directory: "" + testDir);"
hcatalog/storage-handlers/hbase/src/test/org/apache/hcatalog/hbase/SkeletonHBaseTest.java,171,      } catch (IOException e) {
hcatalog/storage-handlers/hbase/src/test/org/apache/hcatalog/hbase/SkeletonHBaseTest.java,172,"        throw new IllegalStateException(""Failed to generate testDir"", e);"
hcatalog/storage-handlers/hbase/src/test/org/apache/hcatalog/hbase/SkeletonHBaseTest.java,173,      }
hcatalog/storage-handlers/hbase/src/test/org/apache/hcatalog/hbase/TestHCatHBaseInputFormat.java,67,import org.apache.hcatalog.common.HCatUtil;
hcatalog/storage-handlers/hbase/src/test/org/apache/hcatalog/hbase/TestHCatHBaseInputFormat.java,191,"    String db_dir = getTestDir() + ""/hbasedb"";"
hcatalog/storage-handlers/hbase/src/test/org/apache/hive/hcatalog/hbase/SkeletonHBaseTest.java,175,      try {
hcatalog/storage-handlers/hbase/src/test/org/apache/hive/hcatalog/hbase/SkeletonHBaseTest.java,176,"        testDir = new File(TEST_DIR + ""/test_"" + handle + ""_"" + Math.abs(new Random().nextLong()) + ""/"").getCanonicalPath();"
hcatalog/storage-handlers/hbase/src/test/org/apache/hive/hcatalog/hbase/SkeletonHBaseTest.java,177,"        System.out.println(""Cluster work directory: "" + testDir);"
hcatalog/storage-handlers/hbase/src/test/org/apache/hive/hcatalog/hbase/SkeletonHBaseTest.java,178,      } catch (IOException e) {
hcatalog/storage-handlers/hbase/src/test/org/apache/hive/hcatalog/hbase/SkeletonHBaseTest.java,179,"        throw new IllegalStateException(""Failed to generate testDir"", e);"
hcatalog/storage-handlers/hbase/src/test/org/apache/hive/hcatalog/hbase/SkeletonHBaseTest.java,180,      }
hcatalog/webhcat/java-client/src/test/java/org/apache/hcatalog/api/TestHCatClient.java,131,"    String expectedDir = warehouseDir.replaceAll(""\\\\"", ""/"").replaceFirst(""pfile:///"", ""pfile:/"");"
hcatalog/webhcat/java-client/src/test/java/org/apache/hive/hcatalog/api/TestHCatClient.java,103,
hcatalog/webhcat/java-client/src/test/java/org/apache/hive/hcatalog/api/TestHCatClient.java,124,"    String expectedDir = warehouseDir.replaceAll(""\\\\"", ""/"").replaceFirst(""pfile:///"", ""pfile:/"");"
itests/hcatalog-unit/src/test/java/org/apache/hcatalog/mapreduce/TestSequenceFileReadWrite.java,46,import org.apache.hcatalog.common.HCatUtil;
itests/hcatalog-unit/src/test/java/org/apache/hcatalog/mapreduce/TestSequenceFileReadWrite.java,75,"    warehouseDir = new File(dataDir, ""warehouse"").getAbsolutePath();"
itests/hcatalog-unit/src/test/java/org/apache/hcatalog/mapreduce/TestSequenceFileReadWrite.java,76,"    inputFileName = new File(dataDir, ""input.data"").getAbsolutePath();"
itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/hbase/TestPigHBaseStorageHandler.java,141,"    String db_dir = getTestDir() + ""/hbasedb"";"
itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/hbase/TestPigHBaseStorageHandler.java,196,"    String db_dir = getTestDir() + ""/hbasedb"";"
itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/hbase/TestPigHBaseStorageHandler.java,258,"    String db_dir = getTestDir() + ""/hbasedb"";"
itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/mapreduce/TestSequenceFileReadWrite.java,72,"    warehouseDir = new File(dataDir, ""warehouse"").getAbsolutePath();"
itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/mapreduce/TestSequenceFileReadWrite.java,73,"    inputFileName = new File(dataDir, ""input.data"").getAbsolutePath();"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java,53,    throws SemanticException {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java,62,"          + parameters[0].getTypeName() + "" is passed."");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java,69,    case TIMESTAMP:
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java,84,"          + parameters[0].getTypeName() + "" is passed."");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java,148,              .warn(getClass().getSimpleName()
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java,237,              .warn(getClass().getSimpleName()
ql/src/java/org/apache/hadoop/hive/ql/exec/CopyTask.java,29,import org.apache.hadoop.hive.ql.Context;
ql/src/java/org/apache/hadoop/hive/ql/exec/CopyTask.java,54,      Path fromPath = new Path(work.getFromPath());
ql/src/java/org/apache/hadoop/hive/ql/exec/CopyTask.java,55,      toPath = new Path(work.getToPath());
ql/src/java/org/apache/hadoop/hive/ql/exec/CopyTask.java,64,
ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java,255,            dirs = fs.globStatus(new Path(tbd.getSourceDir()));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRFileSink1.java,352,"        new LoadFileDesc(fsInputDesc.getFinalDirName(), finalName, true, null, null), false);"
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,1015,"        LoadTableDesc ltd = new LoadTableDesc(queryTmpdir, queryTmpdir, tblDesc,"
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,1630,"      LoadTableDesc ltd = new LoadTableDesc(queryTmpdir, queryTmpdir, tblDesc,"
ql/src/java/org/apache/hadoop/hive/ql/parse/ExportSemanticAnalyzer.java,94,"          path.toString(), toURI.toString(), false), conf);"
ql/src/java/org/apache/hadoop/hive/ql/parse/ExportSemanticAnalyzer.java,111,"            new CopyWork(fromURI.toString(), toPartPath.toString(), false),"
ql/src/java/org/apache/hadoop/hive/ql/parse/ExportSemanticAnalyzer.java,120,"          fromURI.toString(), toDataPath.toString(), false), conf);"
ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java,284,    String tmpURI = ctx.getExternalTmpFileURI(fromURI);
ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java,285,"    Task<?> copyTask = TaskFactory.get(new CopyWork(dataPath.toString(),"
ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java,286,"        tmpURI, false), conf);"
ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java,287,"    LoadTableDesc loadTableWork = new LoadTableDesc(tmpURI.toString(),"
ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java,329,      String tmpURI = ctx.getExternalTmpFileURI(fromURI);
ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java,330,"      Task<?> copyTask = TaskFactory.get(new CopyWork(srcLocation,"
ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java,331,"          tmpURI, false), conf);"
ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java,334,"      LoadTableDesc loadTableWork = new LoadTableDesc(tmpURI,"
ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java,30,import org.apache.commons.httpclient.URIException;
ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java,31,import org.apache.commons.httpclient.util.URIUtil;
ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java,87,"        path = URIUtil.decode( new Path(System.getProperty(""user.dir""), path).toUri().toString() );"
ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java,128,"      FileStatus[] srcs = matchFilesOrDir(FileSystem.get(fromURI, conf),"
ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java,129,"          new Path(fromURI.getScheme(), fromURI.getAuthority(), fromURI"
ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java,130,          .getPath()));
ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java,131,
ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java,236,      try {
ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java,237,"        rTask = TaskFactory.get(new CopyWork(URIUtil.decode(fromURI.toString()), copyURIStr),"
ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java,238,            conf);
ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java,239,      } catch (URIException e) {
ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java,240,"        throw new SemanticException(ErrorMsg.INVALID_PATH.getMsg(fromTree, e"
ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java,241,"            .getMessage()), e);"
ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java,242,      }
ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java,272,    try {
ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java,273,"      loadTableWork = new LoadTableDesc(URIUtil.decode(fromURI.toString()),"
ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java,275,    } catch (URIException e1) {
ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java,276,"      throw new SemanticException(ErrorMsg.INVALID_PATH.getMsg(fromTree, e1"
ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java,277,"          .getMessage()), e1);"
ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java,278,    }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5357,"        ltd = new LoadTableDesc(queryTmpdir, ctx.getExternalTmpFileURI(dest_path.toUri()),"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5441,"      ltd = new LoadTableDesc(queryTmpdir, ctx.getExternalTmpFileURI(dest_path.toUri()),"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5563,"      loadFileWork.add(new LoadFileDesc(tblDesc, queryTmpdir, destStr, isDfsDir, cols,"
ql/src/java/org/apache/hadoop/hive/ql/plan/CopyWork.java,30,  private String fromPath;
ql/src/java/org/apache/hadoop/hive/ql/plan/CopyWork.java,31,  private String toPath;
ql/src/java/org/apache/hadoop/hive/ql/plan/CopyWork.java,37,"  public CopyWork(final String fromPath, final String toPath) {"
ql/src/java/org/apache/hadoop/hive/ql/plan/CopyWork.java,41,"  public CopyWork(final String fromPath, final String toPath, boolean errorOnSrcEmpty) {"
ql/src/java/org/apache/hadoop/hive/ql/plan/CopyWork.java,48,  public String getFromPath() {
ql/src/java/org/apache/hadoop/hive/ql/plan/CopyWork.java,49,    return fromPath;
ql/src/java/org/apache/hadoop/hive/ql/plan/CopyWork.java,51,
ql/src/java/org/apache/hadoop/hive/ql/plan/CopyWork.java,52,  public void setFromPath(final String fromPath) {
ql/src/java/org/apache/hadoop/hive/ql/plan/CopyWork.java,53,    this.fromPath = fromPath;
ql/src/java/org/apache/hadoop/hive/ql/plan/CopyWork.java,56,"  @Explain(displayName = ""destination"")"
ql/src/java/org/apache/hadoop/hive/ql/plan/CopyWork.java,57,  public String getToPath() {
ql/src/java/org/apache/hadoop/hive/ql/plan/CopyWork.java,58,    return toPath;
ql/src/java/org/apache/hadoop/hive/ql/plan/CopyWork.java,61,  public void setToPath(final String toPath) {
ql/src/java/org/apache/hadoop/hive/ql/plan/CopyWork.java,62,    this.toPath = toPath;
ql/src/java/org/apache/hadoop/hive/ql/plan/LoadDesc.java,29,  private String sourceDir;
ql/src/java/org/apache/hadoop/hive/ql/plan/LoadDesc.java,34,  public LoadDesc(final String sourceDir) {
ql/src/java/org/apache/hadoop/hive/ql/plan/LoadDesc.java,35,
ql/src/java/org/apache/hadoop/hive/ql/plan/LoadDesc.java,36,    this.sourceDir = sourceDir;
ql/src/java/org/apache/hadoop/hive/ql/plan/LoadDesc.java,41,    return sourceDir;
ql/src/java/org/apache/hadoop/hive/ql/plan/LoadDesc.java,43,
ql/src/java/org/apache/hadoop/hive/ql/plan/LoadDesc.java,44,  public void setSourceDir(final String source) {
ql/src/java/org/apache/hadoop/hive/ql/plan/LoadDesc.java,45,    sourceDir = source;
ql/src/java/org/apache/hadoop/hive/ql/plan/LoadFileDesc.java,39,"  public LoadFileDesc(final CreateTableDesc createTableDesc, final String sourceDir,"
ql/src/java/org/apache/hadoop/hive/ql/plan/LoadFileDesc.java,42,"    this(sourceDir, targetDir, isDfsDir, columns, columnTypes);"
ql/src/java/org/apache/hadoop/hive/ql/plan/LoadFileDesc.java,51,"  public LoadFileDesc(final String sourceDir, final String targetDir,"
ql/src/java/org/apache/hadoop/hive/ql/plan/LoadFileDesc.java,54,    super(sourceDir);
ql/src/java/org/apache/hadoop/hive/ql/plan/LoadTableDesc.java,48,"  public LoadTableDesc(final String sourceDir, final String tmpDir,"
ql/src/java/org/apache/hadoop/hive/ql/plan/LoadTableDesc.java,51,    super(sourceDir);
ql/src/java/org/apache/hadoop/hive/ql/plan/LoadTableDesc.java,52,"    init(sourceDir, tmpDir, table, partitionSpec, replace);"
ql/src/java/org/apache/hadoop/hive/ql/plan/LoadTableDesc.java,55,"  public LoadTableDesc(final String sourceDir, final String tmpDir,"
ql/src/java/org/apache/hadoop/hive/ql/plan/LoadTableDesc.java,58,"    this(sourceDir, tmpDir, table, partitionSpec, true);"
ql/src/java/org/apache/hadoop/hive/ql/plan/LoadTableDesc.java,61,"  public LoadTableDesc(final String sourceDir, final String tmpDir,"
ql/src/java/org/apache/hadoop/hive/ql/plan/LoadTableDesc.java,64,    super(sourceDir);
ql/src/java/org/apache/hadoop/hive/ql/plan/LoadTableDesc.java,67,"      init(sourceDir, tmpDir, table, dpCtx.getPartSpec(), true);"
ql/src/java/org/apache/hadoop/hive/ql/plan/LoadTableDesc.java,69,"      init(sourceDir, tmpDir, table, new LinkedHashMap<String, String>(), true);"
ql/src/java/org/apache/hadoop/hive/ql/plan/LoadTableDesc.java,73,"  private void init(final String sourceDir, final String tmpDir,"
ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java,981,      String origStreamDesc;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java,982,"      streamDesc = ""$INTNAME"";"
ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java,983,      origStreamDesc = streamDesc;
ql/src/java/org/apache/hadoop/hive/ql/parse/QBJoinTree.java,165,  public String getJoinStreamDesc() {
ql/src/java/org/apache/hadoop/hive/ql/parse/QBJoinTree.java,166,"    return ""$INTNAME"";"
ql/src/java/org/apache/hadoop/hive/ql/parse/QBJoinTree.java,167,  }
ql/src/java/org/apache/hadoop/hive/ql/parse/QBJoinTree.java,168,
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java,459,"      // can throw KeeperException.NoNodeException, which might mean something is wrong"
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java,463,      HiveLockObject obj = zLock.getHiveLockObject();
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java,464,"      String name  = getLastObjectName(parent, obj);"
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java,465,
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java,466,      try {
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java,467,"        List<String> children = zkpClient.getChildren(name, false);"
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java,468,        if (children == null || children.isEmpty()) {
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java,469,"          zkpClient.delete(name, -1);"
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java,470,        }
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java,471,      } catch (KeeperException.NoNodeException e) {
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java,472,"        LOG.debug(""Node "" + name + "" previously deleted when attempting to delete."");"
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java,473,      }
ql/src/java/org/apache/hadoop/hive/ql/udf/UDAFPercentile.java,62,      return o1.getKey().compareTo(o2.getKey());
shims/common/src/main/java/org/apache/hadoop/hive/shims/HadoopShims.java,41,import org.apache.hadoop.io.Text;
shims/common/src/main/java/org/apache/hadoop/hive/shims/HadoopShims.java,43,import org.apache.hadoop.mapred.InputFormat;
shims/common/src/main/java/org/apache/hadoop/hive/shims/HadoopShims.java,50,import org.apache.hadoop.mapred.RunningJob;
shims/common/src/main/java/org/apache/hadoop/hive/shims/HadoopShims.java,51,import org.apache.hadoop.mapred.TaskCompletionEvent;
hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/JsonSerDe.java,174,    int fpos;
hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/JsonSerDe.java,175,    try {
hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/JsonSerDe.java,176,      fpos = s.getPosition(fieldName);
hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/JsonSerDe.java,177,    } catch (NullPointerException npe) {
hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/JsonSerDe.java,179,"      LOG.debug(""NPE finding position for field [{}] in schema [{}]"", fieldName, s);"
hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/JsonSerDe.java,183,        throw npe;
hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/JsonSerDe.java,184,      }
hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/JsonSerDe.java,185,      if (fpos == -1) {
hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/JsonSerDe.java,186,"        return; // unknown field, we return."
hcatalog/core/src/main/java/org/apache/hcatalog/cli/SemanticAnalysis/CreateTableHook.java,219,          table.setDataLocation(new Path(desc.getLocation()).toUri());
hcatalog/core/src/main/java/org/apache/hcatalog/security/HdfsAuthorizationProvider.java,211,"      authorize(part.getPartitionPath(), readRequiredPriv, writeRequiredPriv);"
hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/CreateTableHook.java,205,"        //Authorization checks are performed by the storageHandler.getAuthorizationProvider(), if"
hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/CreateTableHook.java,216,          table.setDataLocation(new Path(desc.getLocation()).toUri());
hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestHCatMultiOutputFormat.java,380,"      work = new FetchWork(new Path(tbl.getDataLocation()), Utilities.getTableDesc(tbl));"
hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestHCatMultiOutputFormat.java,386,        partLocs.add(part.getPartitionPath());
hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatMultiOutputFormat.java,384,        partLocs.add(part.getPartitionPath());
hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatMultiOutputFormat.java,390,"      work = new FetchWork(new Path(tbl.getDataLocation()), Utilities.getTableDesc(tbl));"
itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyOutputTableLocationSchemeIsFileHook.java,28,        String scheme = output.getTable().getDataLocation().getScheme();
itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsNotSubdirectoryOfTableHook.java,47,        partition.getPartitionPath().toString().startsWith(table.getPath().toString()));
itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsSubdirectoryOfTableHook.java,46,        partition.getPartitionPath().toString().startsWith(table.getPath().toString()));
ql/src/java/org/apache/hadoop/hive/ql/exec/ArchiveUtils.java,111,      URI tableDir = tbl.getDataLocation();
ql/src/java/org/apache/hadoop/hive/ql/exec/ArchiveUtils.java,115,"      return new Path(tableDir.toString(), prefixSubdir);"
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,886,                FileSystem fs = p.getPartitionPath().getFileSystem(db.getConf());
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,887,                FileStatus fss = fs.getFileStatus(p.getPartitionPath());
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1215,    URI tableDir = tbl.getDataLocation();
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1220,"    String standardLocation = (new Path(tableDir.toString(), subdir)).toString();"
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1269,        originalDir = p.getPartitionPath();
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1421,        URI originalPartitionUri = ArchiveUtils.addSlash(p.getPartitionPath().toUri());
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1422,        URI test = p.getPartitionPath().toUri();
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,3207,          tbl.setDataLocation(locUri);
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,3664,      tbl.setDataLocation(new Path(crtTbl.getLocation()).toUri());
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,3802,        tbl.setDataLocation(new Path(crtTbl.getLocation()).toUri());
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,3956,          locations.add(partition.getPartitionPath());
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,3963,        locations.add(partition.getPartitionPath());
ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java,254,"            fs = FileSystem.get(table.getDataLocation(), conf);"
ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java,463,      FileSystem fileSys = partn.getPartitionPath().getFileSystem(conf);
ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java,465,"          partn.getPartitionPath(), 1, fileSys);"
ql/src/java/org/apache/hadoop/hive/ql/hooks/Entity.java,26,import org.apache.hadoop.hive.ql.metadata.Partition;
ql/src/java/org/apache/hadoop/hive/ql/hooks/Entity.java,239,      return t.getDataLocation();
ql/src/java/org/apache/hadoop/hive/ql/hooks/Entity.java,243,      return p.getDataLocation();
ql/src/java/org/apache/hadoop/hive/ql/index/IndexMetadataChangeTask.java,26,import org.apache.hadoop.hive.ql.Context;
ql/src/java/org/apache/hadoop/hive/ql/index/IndexMetadataChangeTask.java,70,        Path url = new Path(part.getPartitionPath().toString());
ql/src/java/org/apache/hadoop/hive/ql/index/IndexMetadataChangeTask.java,71,        FileSystem fs = url.getFileSystem(conf);
ql/src/java/org/apache/hadoop/hive/ql/index/IndexMetadataChangeTask.java,72,        FileStatus fstat = fs.getFileStatus(url);
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,1198,        oldPartPath = oldPart.getPartitionPath();
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,1204,"        Path partPath = new Path(tbl.getDataLocation().getPath(),"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,1230,"        FileSystem fs = FileSystem.get(tbl.getDataLocation(), getConf());"
ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMetaStoreChecker.java,257,      Path partPath = partition.getPartitionPath();
ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java,22,import java.net.URI;
ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java,23,import java.net.URISyntaxException;
ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java,78,  private URI uri;
ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java,231,    Path[] ret = new Path[]{getPartitionPath()};
ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java,235,  public Path getPartitionPath() {
ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java,239,
ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java,240,      /**
ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java,241,       * Table location string need to be constructed as URI first to decode
ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java,242,       * the http encoded characters in the location path (because location is
ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java,243,       * stored as URI in org.apache.hadoop.hive.ql.metadata.Table before saved
ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java,244,       * to metastore database). This is not necessary for partition location.
ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java,245,       */
ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java,246,      try {
ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java,247,        return new Path(new URI(table.getTTable().getSd().getLocation()));
ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java,248,      } catch (URISyntaxException e) {
ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java,249,"        throw new RuntimeException(""Invalid table path "" +"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java,250,"          table.getTTable().getSd().getLocation(), e);"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java,251,      }
ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java,252,    }
ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java,253,  }
ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java,254,
ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java,255,  final public URI getDataLocation() {
ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java,256,    if (uri == null) {
ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java,257,      uri = getPartitionPath().toUri();
ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java,259,    return uri;
ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java,395,"      FileSystem fs = FileSystem.get(getPartitionPath().toUri(), Hive.get()"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java,396,          .getConf());
ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java,397,      String pathPattern = getPartitionPath().toString();
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java,23,import java.net.URI;
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java,24,import java.net.URISyntaxException;
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java,40,import org.apache.hadoop.hive.conf.HiveConf;
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java,87,  private URI uri;
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java,248,    try {
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java,249,      return new Path(new URI(location));
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java,250,    } catch (URISyntaxException e) {
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java,251,"      throw new RuntimeException(""Invalid table path "" + location, e);"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java,252,    }
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java,259,  final public URI getDataLocation() {
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java,260,    if (uri == null) {
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java,261,      Path path = getPath();
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java,262,      if (path != null) {
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java,263,        uri = path.toUri();
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java,264,      }
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java,266,    return uri;
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java,507,  public void setDataLocation(URI uri) {
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java,508,    this.uri = uri;
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java,509,    tTable.getSd().setLocation(uri.toString());
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java,513,    this.uri = null;
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java,644,    Path tableDest =  new Path(getDataLocation().getPath());
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java,657,"      fs = FileSystem.get(getDataLocation(), Hive.get().getConf());"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java,658,"      Hive.copyFiles(Hive.get().getConf(), srcf, new Path(getDataLocation().getPath()), fs);"
ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/JsonMetaDataFormatter.java,22,import java.io.OutputStream;
ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/JsonMetaDataFormatter.java,42,import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/JsonMetaDataFormatter.java,43,import org.apache.hadoop.hive.shims.ShimLoader;
ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/TextMetaDataFormatter.java,45,import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/TextMetaDataFormatter.java,46,import org.apache.hadoop.hive.shims.ShimLoader;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/AbstractBucketJoinProc.java,21,import java.net.URI;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/AbstractBucketJoinProc.java,80,"      URI location, ParseContext pGraphContext) throws SemanticException {"
ql/src/java/org/apache/hadoop/hive/ql/optimizer/AbstractBucketJoinProc.java,83,"      FileSystem fs = FileSystem.get(location, pGraphContext.getConf());"
ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRTableScan1.java,188,            part.getPartitionPath().toString() + e.getMessage()));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRTableScan1.java,190,      inputPaths.add(part.getPartitionPath());
ql/src/java/org/apache/hadoop/hive/ql/optimizer/IndexUtils.java,168,      FileSystem partFs = part.getPartitionPath().getFileSystem(hive.getConf());
ql/src/java/org/apache/hadoop/hive/ql/optimizer/IndexUtils.java,169,      FileStatus partFss = partFs.getFileStatus(part.getPartitionPath());
ql/src/java/org/apache/hadoop/hive/ql/optimizer/SamplePruner.java,322,"      FileSystem fs = FileSystem.get(part.getPartitionPath().toUri(), Hive.get()"
ql/src/java/org/apache/hadoop/hive/ql/optimizer/SamplePruner.java,323,          .getConf());
ql/src/java/org/apache/hadoop/hive/ql/optimizer/SamplePruner.java,324,"      String pathPattern = part.getPartitionPath().toString() + ""/*"";"
ql/src/java/org/apache/hadoop/hive/ql/optimizer/SimpleFetchOptimizer.java,245,        listP.add(partition.getPartitionPath());
ql/src/java/org/apache/hadoop/hive/ql/optimizer/SimpleFetchOptimizer.java,299,        Path path = partition.getPartitionPath();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/SizeBasedBigTableSelectorForAutoSMJ.java,81,    Path path = partition.getPartitionPath();
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,761,          Path partPath = part.getPartitionPath();
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,1405,          Path partPath = part.getPartitionPath();
ql/src/java/org/apache/hadoop/hive/ql/parse/ExportSemanticAnalyzer.java,108,        URI fromURI = partition.getDataLocation();
ql/src/java/org/apache/hadoop/hive/ql/parse/ExportSemanticAnalyzer.java,111,"            new CopyWork(new Path(fromURI), toPartPath, false),"
ql/src/java/org/apache/hadoop/hive/ql/parse/ExportSemanticAnalyzer.java,117,      URI fromURI = ts.tableHandle.getDataLocation();
ql/src/java/org/apache/hadoop/hive/ql/parse/ExportSemanticAnalyzer.java,120,"          new Path(fromURI), toDataPath, false), conf);"
ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java,402,              .equals(new URI(tableDesc.getLocation()))) {
ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java,209,    URI toURI = (ts.partHandle != null) ? ts.partHandle.getDataLocation()
ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java,210,        : ts.tableHandle.getDataLocation();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5421,      Path partPath = dest_part.getPartitionPath();
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/StorageBasedAuthorizationProvider.java,177,"      authorize(part.getPartitionPath(), readRequiredPriv, writeRequiredPriv);"
ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java,308,      Path path = part.getPartitionPath();
ql/src/test/org/apache/hadoop/hive/ql/metadata/TestHiveMetaStoreChecker.java,168,    fs.deleteOnExit(fakeTable);
ql/src/test/org/apache/hadoop/hive/ql/metadata/TestHiveMetaStoreChecker.java,224,    Path partToRemovePath = new Path(partToRemove.getDataLocation().toString());
ql/src/test/org/apache/hadoop/hive/ql/metadata/TestHiveMetaStoreChecker.java,301,    fs.deleteOnExit(fakeTable);
ql/src/test/org/apache/hadoop/hive/ql/metadata/TestHiveMetaStoreChecker.java,302,
shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java,106,"    return new TaskAttemptContextImpl(conf, new TaskAttemptID()) {"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java,324,"          c = Class.forName(className, true,"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java,325,            JavaUtils.getClassLoader());
itests/hive-unit/src/test/java/org/apache/hive/beeline/TestBeeLineWithArgs.java,35,import org.apache.hive.beeline.BeeLine;
itests/hive-unit/src/test/java/org/apache/hive/beeline/TestBeeLineWithArgs.java,42,import static org.junit.Assert.*;
itests/hive-unit/src/test/java/org/apache/hive/beeline/TestBeeLineWithArgs.java,43,
ql/src/java/org/apache/hadoop/hive/ql/processors/SetProcessor.java,184,  private CommandProcessorResponse getVariable(String varname){
ql/src/java/org/apache/hadoop/hive/ql/processors/SetProcessor.java,188,      return new CommandProcessorResponse(0);
ql/src/java/org/apache/hadoop/hive/ql/processors/SetProcessor.java,195,        return new CommandProcessorResponse(0);
ql/src/java/org/apache/hadoop/hive/ql/processors/SetProcessor.java,204,        return new CommandProcessorResponse(0);
ql/src/java/org/apache/hadoop/hive/ql/processors/SetProcessor.java,213,        return new CommandProcessorResponse(0);
ql/src/java/org/apache/hadoop/hive/ql/processors/SetProcessor.java,222,        return new CommandProcessorResponse(0);
ql/src/java/org/apache/hadoop/hive/ql/processors/SetProcessor.java,229,"      return new CommandProcessorResponse(0, null, null, getSchema());"
ql/src/java/org/apache/hadoop/hive/ql/processors/SetProcessor.java,235,    Schema sch = getSchema();
ql/src/java/org/apache/hadoop/hive/ql/processors/SetProcessor.java,240,"      return new CommandProcessorResponse(0, null, null, sch);"
ql/src/java/org/apache/hadoop/hive/ql/processors/SetProcessor.java,245,"      return new CommandProcessorResponse(0, null, null, sch);"
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,98,import org.apache.hadoop.hive.common.ObjectPair;
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,136,import org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner;
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,137,import org.apache.hadoop.hive.ql.parse.PrunedPartitionList;
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,141,import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,187,import org.apache.commons.codec.binary.Base64;
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,188,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,444,"      // If sizes of atleast n-1 tables in a n-way join is known, and their sum is smaller than"
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,454,        boolean bigTableFound = false;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,455,        long largestBigTableCandidateSize = -1;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,456,        long sumTableSizes = 0;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,457,        for (String alias : aliasToWork.keySet()) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,459,          boolean bigTableCandidate = bigTableCandidates.contains(tablePosition);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,460,          Long size = aliasToSize.get(alias);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,461,          // The size is not available at compile time if the input is a sub-query.
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,462,"          // If the size of atleast n-1 inputs for a n-way join are available at compile time,"
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,463,"          // and the sum of them is less than the specified threshold, then convert the join"
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,464,          // into a map-join without the conditional task.
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,465,          if ((size == null) || (size > mapJoinSize)) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,466,            sumTableSizes += largestBigTableCandidateSize;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,467,            if (bigTableFound || (sumTableSizes > mapJoinSize) || !bigTableCandidate) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,468,              convertJoinMapJoin = false;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,469,              break;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,470,            }
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,471,            bigTableFound = true;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,473,            largestBigTableCandidateSize = mapJoinSize + 1;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,474,          } else {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,475,            if (bigTableCandidate && size > largestBigTableCandidateSize) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,476,              bigTablePosition = tablePosition;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,477,              sumTableSizes += largestBigTableCandidateSize;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,478,              largestBigTableCandidateSize = size;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,479,            } else {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,480,              sumTableSizes += size;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,481,            }
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,482,            if (sumTableSizes > mapJoinSize) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,483,              convertJoinMapJoin = false;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,484,              break;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,485,            }
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,490,      String bigTableAlias = null;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,494,      if (convertJoinMapJoin) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,524,        bigTableAlias = newTaskAlias.getSecond();
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,22,import java.util.Collections;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,74,"      return aliasToKnownSize == null ? new HashMap<String, Long>() : aliasToKnownSize;"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,111,    ConditionalResolverCommonJoinCtx ctx = (ConditionalResolverCommonJoinCtx) objCtx;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,115,"    HashMap<String, ArrayList<String>> pathToAliases = ctx.getPathToAliases();"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,116,"    HashMap<String, Long> aliasToKnownSize = ctx.getAliasToKnownSize();"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,117,"    String bigTableAlias = this.resolveMapJoinTask(pathToAliases, ctx"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,118,"        .getAliasToTask(), aliasToKnownSize, ctx.getHdfsTmpDir(), ctx"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,119,"        .getLocalTmpDir(), conf);"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,138,  static class AliasFileSizePair implements Comparable<AliasFileSizePair> {
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,139,    String alias;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,140,    long size;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,141,"    AliasFileSizePair(String alias, long size) {"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,142,      super();
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,143,      this.alias = alias;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,144,      this.size = size;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,145,    }
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,146,    @Override
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,147,    public int compareTo(AliasFileSizePair o) {
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,148,      if (o == null) {
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,149,        return 1;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,150,      }
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,151,      return (size < o.size) ? -1 : ((size > o.size) ? 1 : 0);
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,155,  private String resolveMapJoinTask(
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,156,"      HashMap<String, ArrayList<String>> pathToAliases,"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,157,"      HashMap<String, Task<? extends Serializable>> aliasToTask,"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,158,"      HashMap<String, Long> aliasToKnownSize, Path hdfsTmpDir,"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,159,"      Path localTmpDir, HiveConf conf) {"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,162,    long smallTablesFileSizeSum = 0;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,164,"    Map<String, AliasFileSizePair> aliasToFileSizeMap = new HashMap<String, AliasFileSizePair>();"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,165,"    for (Map.Entry<String, Long> entry : aliasToKnownSize.entrySet()) {"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,166,      String alias = entry.getKey();
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,167,"      AliasFileSizePair pair = new AliasFileSizePair(alias, entry.getValue());"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,168,"      aliasToFileSizeMap.put(alias, pair);"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,171,    try {
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,172,"      // need to compute the input size at runtime, and select the biggest as"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,173,      // the big table.
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,174,"      for (Map.Entry<String, ArrayList<String>> oneEntry : pathToAliases"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,175,          .entrySet()) {
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,176,        String p = oneEntry.getKey();
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,177,        // this path is intermediate data
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,178,        if (p.startsWith(hdfsTmpDir.toString()) || p.startsWith(localTmpDir.toString())) {
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,179,          ArrayList<String> aliasArray = oneEntry.getValue();
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,180,          if (aliasArray.size() <= 0) {
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,181,            continue;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,182,          }
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,183,          Path path = new Path(p);
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,184,          FileSystem fs = path.getFileSystem(conf);
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,185,          long fileSize = fs.getContentSummary(path).getLength();
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,186,          for (String alias : aliasArray) {
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,187,            AliasFileSizePair pair = aliasToFileSizeMap.get(alias);
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,188,            if (pair == null) {
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,189,"              pair = new AliasFileSizePair(alias, 0);"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,190,"              aliasToFileSizeMap.put(alias, pair);"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,191,            }
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,192,            pair.size += fileSize;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,193,          }
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,196,"      // generate file size to alias mapping; but not set file size as key,"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,197,      // because different file may have the same file size.
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,198,
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,199,      List<AliasFileSizePair> aliasFileSizeList = new ArrayList<AliasFileSizePair>(
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,200,          aliasToFileSizeMap.values());
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,201,
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,202,      Collections.sort(aliasFileSizeList);
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,203,"      // iterating through this list from the end to beginning, trying to find"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,204,      // the big table for mapjoin
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,205,      int idx = aliasFileSizeList.size() - 1;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,206,      boolean bigAliasFound = false;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,207,      while (idx >= 0) {
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,208,        AliasFileSizePair pair = aliasFileSizeList.get(idx);
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,209,        String alias = pair.alias;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,210,        long size = pair.size;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,211,        idx--;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,212,        if (!bigAliasFound && aliasToTask.get(alias) != null) {
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,213,          // got the big table
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,214,          bigAliasFound = true;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,215,          bigTableFileAlias = alias;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,216,          continue;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,218,        smallTablesFileSizeSum += size;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,220,
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,221,      // compare with threshold
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,222,"      long threshold = HiveConf.getLongVar(conf, HiveConf.ConfVars.HIVESMALLTABLESFILESIZE);"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,223,      if (smallTablesFileSizeSum <= threshold) {
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,224,        return bigTableFileAlias;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,225,      } else {
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,226,        return null;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,227,      }
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,228,    } catch (Exception e) {
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,229,      e.printStackTrace();
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,230,      return null;
ql/src/test/org/apache/hadoop/hive/ql/plan/TestConditionalResolverCommonJoin.java,21,import junit.framework.TestCase;
ql/src/test/org/apache/hadoop/hive/ql/plan/TestConditionalResolverCommonJoin.java,23,import org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin.AliasFileSizePair;
ql/src/test/org/apache/hadoop/hive/ql/plan/TestConditionalResolverCommonJoin.java,25,public class TestConditionalResolverCommonJoin extends TestCase {
ql/src/test/org/apache/hadoop/hive/ql/plan/TestConditionalResolverCommonJoin.java,27,    public void testAliasFileSizePairCompareTo() {
ql/src/test/org/apache/hadoop/hive/ql/plan/TestConditionalResolverCommonJoin.java,28,"        AliasFileSizePair big = new AliasFileSizePair(""big"", 389560034778L);"
ql/src/test/org/apache/hadoop/hive/ql/plan/TestConditionalResolverCommonJoin.java,29,"        AliasFileSizePair small = new AliasFileSizePair(""small"", 1647L);"
ql/src/test/org/apache/hadoop/hive/ql/plan/TestConditionalResolverCommonJoin.java,31,"        assertEquals(0, big.compareTo(big));"
ql/src/test/org/apache/hadoop/hive/ql/plan/TestConditionalResolverCommonJoin.java,32,"        assertEquals(1, big.compareTo(small));"
ql/src/test/org/apache/hadoop/hive/ql/plan/TestConditionalResolverCommonJoin.java,33,"        assertEquals(-1, small.compareTo(big));"
ql/src/test/org/apache/hadoop/hive/ql/plan/TestConditionalResolverCommonJoin.java,34,    }
ql/src/test/org/apache/hadoop/hive/ql/plan/TestConditionalResolverCommonJoin.java,35,}
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,149,"    HiveConf.ConfVars.HADOOPJT,"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,234,"    HADOOPFS(""fs.default.name"", null),"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,236,"    HADOOPMAPFILENAME(""map.input.file"", null),"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,237,"    HADOOPMAPREDINPUTDIR(""mapred.input.dir"", null),"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,238,"    HADOOPMAPREDINPUTDIRRECURSIVE(""mapred.input.dir.recursive"", false),"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,239,"    HADOOPJT(""mapred.job.tracker"", null),"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,240,"    MAPREDMAXSPLITSIZE(""mapred.max.split.size"", 256000000L),"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,241,"    MAPREDMINSPLITSIZE(""mapred.min.split.size"", 1L),"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,242,"    MAPREDMINSPLITSIZEPERNODE(""mapred.min.split.size.per.rack"", 1L),"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,243,"    MAPREDMINSPLITSIZEPERRACK(""mapred.min.split.size.per.node"", 1L),"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,247,"    HADOOPNUMREDUCERS(""mapred.reduce.tasks"", -1),"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,248,"    HADOOPJOBNAME(""mapred.job.name"", null),"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,249,"    HADOOPSPECULATIVEEXECREDUCERS(""mapred.reduce.tasks.speculative.execution"", true),"
shims/common-secure/src/main/java/org/apache/hadoop/hive/shims/HadoopShimsSecure.java,327,"      long minSize = job.getLong(""mapred.min.split.size"", 0);"
shims/common-secure/src/main/java/org/apache/hadoop/hive/shims/HadoopShimsSecure.java,330,"      if (job.getLong(""mapred.min.split.size.per.node"", 0) == 0) {"
shims/common-secure/src/main/java/org/apache/hadoop/hive/shims/HadoopShimsSecure.java,334,"      if (job.getLong(""mapred.min.split.size.per.rack"", 0) == 0) {"
shims/common-secure/src/main/java/org/apache/hadoop/hive/shims/HadoopShimsSecure.java,338,"      if (job.getLong(""mapred.max.split.size"", 0) == 0) {"
shims/common-secure/src/main/java/org/apache/hadoop/hive/shims/HadoopShimsSecure.java,429,"    conf.setBoolean(""mapred.committer.job.setup.cleanup.needed"", false);"
shims/common-secure/src/main/java/org/apache/hadoop/hive/shims/HadoopShimsSecure.java,433,"    conf.setBoolean(""mapreduce.job.committer.task.cleanup.needed"", false);"
ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java,88,  public static long getCountForMapJoinDumpFilePrefix() {
ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java,106,"    TableDesc tableDesc = getDefaultTableDesc(Integer.toString(Utilities.ctrlaCode), cols,"
ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java,109,      return tableDesc;
ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java,114,        tableDesc.getProperties().setProperty(
ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java,116,        tableDesc.getProperties().setProperty(
ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java,120,        tableDesc.getProperties().setProperty(
ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java,124,        tableDesc.getProperties().setProperty(
ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java,128,        tableDesc.getProperties().setProperty(
ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java,132,        tableDesc.getProperties().setProperty(
ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java,136,        tableDesc.getProperties().setProperty(
ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java,140,          tableDesc.setOutputFileFormatClass(Class.forName(localDirectoryDesc.getOutputFormat()));
ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java,143,"        tableDesc.getProperties().setProperty(serdeConstants.SERIALIZATION_NULL_FORMAT,"
ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java,153,    return tableDesc;
ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java,248,"    // compatiblity reasons,"
ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java,468,   * Adds uniontype for distinctColIndices.
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,2194,    readRowIndex();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,2788,  private void readRowIndex() throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,2789,    long offset = stripes.get(currentStripe).getOffset();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,2835,    readRowIndex();
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,247,    nextSz = joinEmitInterval;
ql/src/java/org/apache/hadoop/hive/ql/exec/JoinOperator.java,74,      // get alias
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatUtil.java,83,  private final static int DEFAULT_HIVE_CACHE_EXPIRY_TIME_SECONDS = 2 * 60;
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatUtil.java,554,    // Singleton behaviour: create the cache instance if required. The cache needs to be created lazily and
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatUtil.java,555,    // using the expiry time available in hiveConf.
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatUtil.java,560,"          hiveClientCache = new HiveClientCache(hiveConf.getInt(HCatConstants.HCAT_HIVE_CLIENT_EXPIRY_TIME,"
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatUtil.java,561,            DEFAULT_HIVE_CACHE_EXPIRY_TIME_SECONDS));
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,21,import com.google.common.cache.Cache;
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,22,import com.google.common.cache.CacheBuilder;
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,23,import com.google.common.cache.RemovalListener;
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,24,import com.google.common.cache.RemovalNotification;
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,36,import javax.security.auth.login.LoginException;
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,37,import java.io.IOException;
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,38,import java.util.concurrent.Callable;
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,39,import java.util.concurrent.ConcurrentMap;
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,40,import java.util.concurrent.ExecutionException;
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,41,import java.util.concurrent.TimeUnit;
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,42,import java.util.concurrent.atomic.AtomicInteger;
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,2198,"      LOG.debug((replace ? ""Replacing src:"" : ""Renaming src:"") + srcf.toString()"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,2300,"      // point of no return -- delete oldPath only if it is not same as destf,"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,2301,"      // otherwise, the oldPath/destf will be cleaned later just before move"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,2302,      if (oldPath != null && (!destf.getFileSystem(conf).equals(oldPath.getFileSystem(conf))
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,2303,          || !destf.equals(oldPath))) {
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,97,"      HiveConf.ConfVars.METASTOREATTEMPTS,"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,98,"      HiveConf.ConfVars.METASTOREINTERVAL,"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,276,    // Number of attempts to retry connecting after there is a JDO datastore err
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,277,"    METASTOREATTEMPTS(""hive.metastore.ds.retry.attempts"", 1),"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,278,    // Number of miliseconds to wait between attepting
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,279,"    METASTOREINTERVAL(""hive.metastore.ds.retry.interval"", 1000),"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,1261,"        ""effect. Make sure to provide a valid value for hive.metastore.uris if you are "" +"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,1262,"        ""connecting to a remote metastore."");"
itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java,2501,"    int interval= HiveConf.getIntVar(hiveConf, HiveConf.ConfVars.METASTOREINTERVAL);"
itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java,2502,"    int attempts = HiveConf.getIntVar(hiveConf, HiveConf.ConfVars.METASTOREATTEMPTS);"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,444,"      return RetryingRawStore.getProxy(hiveConf, conf, rawStoreClassName, threadLocalId.get());"
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,365,    return currentTransaction.isActive();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,380,"      throw new RuntimeException(""commitTransaction was called but openTransactionCalls = """
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,382,"              ""calls to openTransaction/commitTransaction"");"
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,385,      throw new RuntimeException(
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,386,"          ""Commit is called, but transaction is not active. Either there are"""
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,387,"              + "" mismatching open and close calls or rollback was called in the same trasaction"");"
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6154,  /** Add this to code to debug lexer if needed. DebugTokenStream may also be added here. */
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6155,"  private void debugLexer(CommonTokenStream stream, FilterLexer lexer) {"
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6156,    try {
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6157,      stream.fill();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6158,      List<?> tokens = stream.getTokens();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6159,"      String report = ""LEXER: tokens ("" + ((tokens == null) ? ""null"" : tokens.size()) + ""): "";"
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6160,      if (tokens != null) {
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6161,        for (Object o : tokens) {
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6162,          if (o == null || !(o instanceof Token)) {
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6163,"            report += ""[not a token: "" + o + ""], "";"
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6164,          } else {
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6165,            Token t = (Token)o;
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6166,"            report += ""[at "" + t.getCharPositionInLine() + "": """
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6167,"                + t.getType() + "" "" + t.getText() + ""], "";"
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6168,          }
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6169,        }
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6170,      }
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6171,"      report += ""; lexer error: "" + lexer.errorMsg;"
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6172,      LOG.error(report);
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6173,    } catch (Throwable t) {
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6174,"      LOG.error(""LEXER: tokens (error)"", t);"
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,40,public class RetryingRawStore implements InvocationHandler {
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,41,
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,42,  private static final Log LOG = LogFactory.getLog(RetryingRawStore.class);
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,45,  private int retryInterval = 0;
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,46,  private int retryLimit = 0;
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,53,"  protected RetryingRawStore(HiveConf hiveConf, Configuration conf,"
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,71,"    RetryingRawStore handler = new RetryingRawStore(hiveConf, conf, baseClass, id);"
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,74,"    return (RawStore) Proxy.newProxyInstance(RetryingRawStore.class.getClassLoader(),"
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,89,"    retryInterval = HiveConf.getIntVar(hiveConf,"
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,90,        HiveConf.ConfVars.METASTOREINTERVAL);
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,91,"    retryLimit = HiveConf.getIntVar(hiveConf,"
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,92,        HiveConf.ConfVars.METASTOREATTEMPTS);
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,107,    boolean gotNewConnectUrl = false;
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,115,    int retryCount = 0;
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,116,    Exception caughtException = null;
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,117,    while (true) {
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,118,      try {
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,119,        if (reloadConf || gotNewConnectUrl) {
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,120,          initMS();
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,121,        }
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,122,"        ret = method.invoke(base, args);"
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,123,        break;
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,124,      } catch (javax.jdo.JDOException e) {
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,125,        caughtException = e;
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,126,      } catch (UndeclaredThrowableException e) {
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,127,        throw e.getCause();
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,128,      } catch (InvocationTargetException e) {
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,129,        if (e.getCause() instanceof javax.jdo.JDOException) {
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,130,"          // Due to reflection, the jdo exception is wrapped in"
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,131,          // invocationTargetException
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,132,          caughtException = (javax.jdo.JDOException) e.getCause();
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,133,        } else {
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,134,          throw e.getCause();
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,135,        }
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,136,      }
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,137,
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,138,      if (retryCount >= retryLimit ||
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,139,          (method.getAnnotation(RawStore.CanNotRetry.class) != null)) {
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,140,        throw  caughtException;
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,141,      }
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,142,
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,143,      assert (retryInterval >= 0);
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,144,      retryCount++;
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,145,      LOG.error(
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,146,          String.format(
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,147,"              ""JDO datastore error. Retrying metastore command "" +"
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,148,"                  ""after %d ms (attempt %d of %d)"", retryInterval, retryCount, retryLimit));"
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,149,      Thread.sleep(retryInterval);
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,150,"      // If we have a connection error, the JDO connection URL hook might"
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,151,      // provide us with a new URL to access the datastore.
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,152,      String lastUrl = MetaStoreInit.getConnectionURL(getConf());
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,153,"      gotNewConnectUrl = MetaStoreInit.updateConnectionURL(hiveConf, getConf(),"
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,154,"        lastUrl, metaStoreInitData);"
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,159,  private String addPrefix(String s) {
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,160,"    return id + "": "" + s;"
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,161,  }
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,162,
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingHMSHandler.java,48,"  protected RetryingHMSHandler(HiveConf hiveConf, String name) throws MetaException {"
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingHMSHandler.java,54,"    this.base = (IHMSHandler) new HiveMetaStore.HMSHandler(name, hiveConf);"
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingHMSHandler.java,79,"  public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {"
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingHMSHandler.java,147,        LOG.error(ExceptionUtils.getStackTrace(caughtException));
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingHMSHandler.java,156,          String.format(
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingHMSHandler.java,157,"              ""JDO datastore error. Retrying HMSHandler "" +"
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingHMSHandler.java,158,"                  ""after %d ms (attempt %d of %d)"", retryInterval, retryCount, retryLimit));"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,412,    private Configuration getConf() {
ql/src/java/org/apache/hadoop/hive/ql/security/HadoopDefaultMetastoreAuthenticator.java,28,    setConf(handler.getHiveConf());
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationPreEventListener.java,62,  private static HiveConf conf;
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationPreEventListener.java,63,  private static HiveMetastoreAuthorizationProvider authorizer;
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationPreEventListener.java,64,  private static HiveMetastoreAuthenticationProvider authenticator;
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationPreEventListener.java,68,
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationPreEventListener.java,69,    authenticator = (HiveMetastoreAuthenticationProvider) HiveUtils.getAuthenticator(
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationPreEventListener.java,70,"        config, HiveConf.ConfVars.HIVE_METASTORE_AUTHENTICATOR_MANAGER);"
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationPreEventListener.java,71,    authorizer = (HiveMetastoreAuthorizationProvider) HiveUtils.getAuthorizeProviderManager(
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationPreEventListener.java,72,"        config, HiveConf.ConfVars.HIVE_METASTORE_AUTHORIZATION_MANAGER, authenticator);"
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationPreEventListener.java,79,    authenticator.setMetaStoreHandler(context.getHandler());
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationPreEventListener.java,80,    authorizer.setMetaStoreHandler(context.getHandler());
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationPreEventListener.java,119,"      authorizer.authorize(new Database(context.getDatabase()),"
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationPreEventListener.java,132,"      authorizer.authorize(new Database(context.getDatabase()),"
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationPreEventListener.java,145,"      authorizer.authorize(getTableFromApiTable(context.getTable()),"
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationPreEventListener.java,158,"      authorizer.authorize(getTableFromApiTable(context.getTable()),"
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationPreEventListener.java,171,"      authorizer.authorize(getTableFromApiTable(context.getOldTable()),"
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationPreEventListener.java,185,"      authorizer.authorize(getPartitionFromApiPartition(mapiPart, context),"
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationPreEventListener.java,201,"      authorizer.authorize(getPartitionFromApiPartition(mapiPart, context),"
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationPreEventListener.java,217,"      authorizer.authorize(getPartitionFromApiPartition(mapiPart, context),"
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,94,"  private static String getTopicName(Partition partition,"
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,95,                     ListenerEvent partitionEvent) throws MetaException {
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,96,    try {
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,97,      return partitionEvent.getHandler()
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,98,"        .get_table(partition.getDbName(), partition.getTableName())"
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,99,        .getParameters().get(HCatConstants.HCAT_MSGBUS_TOPIC_NAME);
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,100,    } catch (NoSuchObjectException e) {
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,101,      throw new MetaException(e.toString());
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,102,    }
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,112,
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,113,      Partition partition = partitionEvent.getPartition();
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,114,"      String topicName = getTopicName(partition, partitionEvent);"
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,115,"      if (topicName != null && !topicName.equals("""")) {"
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,116,"        send(messageFactory.buildAddPartitionMessage(partitionEvent.getTable(), partition), topicName);"
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,117,      } else {
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,118,"        LOG.info(""Topic name not found in metastore. Suppressing HCatalog notification for """
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,119,          + partition.getDbName()
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,120,"          + ""."""
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,121,          + partition.getTableName()
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,122,"          + "" To enable notifications for this table, please do alter table set properties ("""
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,123,          + HCatConstants.HCAT_MSGBUS_TOPIC_NAME
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,124,"          + ""=<dbname>.<tablename>) or whatever you want topic name to be."");"
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,126,    }
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,127,
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,151,"      String topicName = getTopicName(partition, partitionEvent);"
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/messaging/MessageFactory.java,126,   * @param table The Table to which the partition is added.
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/messaging/MessageFactory.java,127,   * @param partition The Partition being added.
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/messaging/MessageFactory.java,130,"  public abstract AddPartitionMessage buildAddPartitionMessage(Table table, Partition partition);"
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/messaging/json/JSONMessageFactory.java,87,"  public AddPartitionMessage buildAddPartitionMessage(Table table, Partition partition) {"
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/messaging/json/JSONMessageFactory.java,88,"    return new JSONAddPartitionMessage(HCAT_SERVER_URL, HCAT_SERVICE_PRINCIPAL, partition.getDbName(),"
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/messaging/json/JSONMessageFactory.java,89,"        partition.getTableName(), Arrays.asList(getPartitionKeyValues(table, partition)),"
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/messaging/json/JSONMessageFactory.java,90,        System.currentTimeMillis()/1000);
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/messaging/json/JSONMessageFactory.java,107,}
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,117,"  private static String getTopicName(Partition partition,"
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,118,                     ListenerEvent partitionEvent) throws MetaException {
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,119,    try {
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,120,      return partitionEvent.getHandler()
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,121,"        .get_table(partition.getDbName(), partition.getTableName())"
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,122,        .getParameters().get(HCatConstants.HCAT_MSGBUS_TOPIC_NAME);
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,123,    } catch (NoSuchObjectException e) {
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,124,      throw new MetaException(e.toString());
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,125,    }
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,135,
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,136,      Partition partition = partitionEvent.getPartition();
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,137,"      String topicName = getTopicName(partition, partitionEvent);"
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,139,"        send(messageFactory.buildAddPartitionMessage(partitionEvent.getTable(), partition), topicName);"
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,142,          + partition.getDbName()
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,143,"          + ""."""
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,144,          + partition.getTableName()
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,145,"          + "" To enable notifications for this table, please do alter table set properties ("""
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,146,          + HCatConstants.HCAT_MSGBUS_TOPIC_NAME
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,147,"          + ""=<dbname>.<tablename>) or whatever you want topic name to be."");"
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,150,
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,174,"      String topicName = getTopicName(partition, partitionEvent);"
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/messaging/MessageFactory.java,123,  /**
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/messaging/MessageFactory.java,124,   * Factory method for AddPartitionMessage.
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/messaging/MessageFactory.java,125,   * @param table The Table to which the partition is added.
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/messaging/MessageFactory.java,126,   * @param partition The Partition being added.
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/messaging/MessageFactory.java,127,   * @return AddPartitionMessage instance.
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/messaging/MessageFactory.java,128,   */
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/messaging/MessageFactory.java,129,"  public abstract AddPartitionMessage buildAddPartitionMessage(Table table, Partition partition);"
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/messaging/json/JSONMessageFactory.java,34,import java.util.Arrays;
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/messaging/json/JSONMessageFactory.java,35,import java.util.LinkedHashMap;
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/messaging/json/JSONMessageFactory.java,36,import java.util.Map;
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/messaging/json/JSONMessageFactory.java,86,"  public AddPartitionMessage buildAddPartitionMessage(Table table, Partition partition) {"
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/messaging/json/JSONMessageFactory.java,87,"    return new JSONAddPartitionMessage(HCAT_SERVER_URL, HCAT_SERVICE_PRINCIPAL, partition.getDbName(),"
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/messaging/json/JSONMessageFactory.java,88,"        partition.getTableName(), Arrays.asList(getPartitionKeyValues(table, partition)),"
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/messaging/json/JSONMessageFactory.java,89,        System.currentTimeMillis()/1000);
itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreEventListener.java,226,"    validateAddPartition(part, partEvent.getPartition());"
itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreEventListener.java,230,"    validateAddPartition(part, prePartEvent.getPartition());"
itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreEventListener.java,263,"    validateAddPartition(newPart, appendPartEvent.getPartition());"
itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreEventListener.java,267,"    validateAddPartition(newPart, preAppendPartEvent.getPartition());"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1637,"        PreAddPartitionEvent event = new PreAddPartitionEvent(part, this);"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1638,        firePreEvent(event);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1639,
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1798,"        Table tbl = ms.getTable(dbName, tblName);"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1842,          for (Partition part : parts) {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1843,"            fireMetaStoreAddPartitionEvent(ms, part, null, success);"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1844,          }
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1846,          for (Partition part : result) {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1847,"            fireMetaStoreAddPartitionEvent(ms, part, null, success);"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1848,          }
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1851,            for (Partition part : existingParts) {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1852,"              fireMetaStoreAddPartitionEvent(ms, part, null, false);"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1853,            }
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1916,"      firePreEvent(new PreAddPartitionEvent(part, this));"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,2010,      Partition retPtn = null;
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,2013,"        Table tbl = ms.getTable(part.getDbName(), part.getTableName());"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,2019,        assert shouldAdd; // start would thrrow if it already existed here
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,2036,"        fireMetaStoreAddPartitionEvent(ms, part, envContext, success);"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,2041,"    private void fireMetaStoreAddPartitionEvent(final RawStore ms,"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,2042,"        final Partition part, final EnvironmentContext envContext, boolean success)"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,2044,"      final Table tbl = ms.getTable(part.getDbName(), part.getTableName());"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,2045,      for (MetaStoreEventListener listener : listeners) {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,2047,"            new AddPartitionEvent(tbl, part, success, this);"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,2049,        listener.onAddPartition(addPartitionEvent);
metastore/src/java/org/apache/hadoop/hive/metastore/events/AddPartitionEvent.java,28,  private final Partition partition;
metastore/src/java/org/apache/hadoop/hive/metastore/events/AddPartitionEvent.java,30,"  public AddPartitionEvent (Table table, Partition partition, boolean status, HMSHandler handler) {"
metastore/src/java/org/apache/hadoop/hive/metastore/events/AddPartitionEvent.java,31,"    super (status, handler);"
metastore/src/java/org/apache/hadoop/hive/metastore/events/AddPartitionEvent.java,33,    this.partition = partition;
metastore/src/java/org/apache/hadoop/hive/metastore/events/AddPartitionEvent.java,36,  /**
metastore/src/java/org/apache/hadoop/hive/metastore/events/AddPartitionEvent.java,37,   * @return the partition
metastore/src/java/org/apache/hadoop/hive/metastore/events/AddPartitionEvent.java,38,   */
metastore/src/java/org/apache/hadoop/hive/metastore/events/AddPartitionEvent.java,39,  public Partition getPartition() {
metastore/src/java/org/apache/hadoop/hive/metastore/events/AddPartitionEvent.java,40,    return partition;
metastore/src/java/org/apache/hadoop/hive/metastore/events/AddPartitionEvent.java,44,   * @return the table
metastore/src/java/org/apache/hadoop/hive/metastore/events/PreAddPartitionEvent.java,26,  private final Partition partition;
metastore/src/java/org/apache/hadoop/hive/metastore/events/PreAddPartitionEvent.java,28,"  public PreAddPartitionEvent (Partition partition, HMSHandler handler) {"
metastore/src/java/org/apache/hadoop/hive/metastore/events/PreAddPartitionEvent.java,30,    this.partition = partition;
metastore/src/java/org/apache/hadoop/hive/metastore/events/PreAddPartitionEvent.java,34,   * @return the partition
metastore/src/java/org/apache/hadoop/hive/metastore/events/PreAddPartitionEvent.java,36,  public Partition getPartition() {
metastore/src/java/org/apache/hadoop/hive/metastore/events/PreAddPartitionEvent.java,37,    return partition;
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationPreEventListener.java,227,      org.apache.hadoop.hive.metastore.api.Partition mapiPart = context.getPartition();
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationPreEventListener.java,228,"      tAuthorizer.get().authorize(getPartitionFromApiPartition(mapiPart, context),"
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationPreEventListener.java,229,"          HiveOperation.ALTERTABLE_ADDPARTS.getInputRequiredPrivileges(),"
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationPreEventListener.java,230,          HiveOperation.ALTERTABLE_ADDPARTS.getOutputRequiredPrivileges());
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/objectinspector/LazyBinaryMapObjectInspector.java,59,    if (data == null) {
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/objectinspector/LazyBinaryMapObjectInspector.java,60,      return -1;
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FosterStorageHandler.java,26,import org.apache.hadoop.hive.ql.io.RCFile;
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FosterStorageHandler.java,101,
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FosterStorageHandler.java,175,"      //TODO find a better home for this, RCFile specifc"
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FosterStorageHandler.java,176,"      jobProperties.put(RCFile.COLUMN_NUMBER_CONF_STR,"
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FosterStorageHandler.java,177,        Integer.toOctalString(
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FosterStorageHandler.java,178,          jobInfo.getOutputSchema().getFields().size()));
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java,109,  @Override
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java,110,"  public RecordWriter<NullWritable, OrcSerdeRow>"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java,111,"      getRecordWriter(FileSystem fileSystem, JobConf conf, String name,"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java,112,                      Progressable reporter) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java,113,    return new
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java,114,"      OrcRecordWriter(new Path(name), OrcFile.writerOptions(conf));"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java,117,  @Override
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java,118,  public FSRecordWriter
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java,119,"     getHiveRecordWriter(JobConf conf,"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java,120,"                         Path path,"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java,121,"                         Class<? extends Writable> valueClass,"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java,122,"                         boolean isCompressed,"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java,123,"                         Properties tableProperties,"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java,124,                         Progressable reporter) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java,126,    if (tableProperties.containsKey(OrcFile.STRIPE_SIZE)) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java,127,      options.stripeSize(Long.parseLong
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java,128,                           (tableProperties.getProperty(OrcFile.STRIPE_SIZE)));
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java,131,    if (tableProperties.containsKey(OrcFile.COMPRESSION)) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java,132,      options.compress(CompressionKind.valueOf
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java,133,                           (tableProperties.getProperty(OrcFile.COMPRESSION)));
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java,136,    if (tableProperties.containsKey(OrcFile.COMPRESSION_BLOCK_SIZE)) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java,137,      options.bufferSize(Integer.parseInt
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java,138,                         (tableProperties.getProperty
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java,139,                            (OrcFile.COMPRESSION_BLOCK_SIZE)));
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java,142,    if (tableProperties.containsKey(OrcFile.ROW_INDEX_STRIDE)) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java,143,      options.rowIndexStride(Integer.parseInt
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java,144,                             (tableProperties.getProperty
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java,145,                              (OrcFile.ROW_INDEX_STRIDE)));
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java,148,    if (tableProperties.containsKey(OrcFile.ENABLE_INDEXES)) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java,149,"      if (""false"".equals(tableProperties.getProperty"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java,150,                         (OrcFile.ENABLE_INDEXES))) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java,155,    if (tableProperties.containsKey(OrcFile.BLOCK_PADDING)) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java,156,      options.blockPadding(Boolean.parseBoolean
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java,157,                           (tableProperties.getProperty
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java,158,                            (OrcFile.BLOCK_PADDING)));
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java,161,"    return new OrcRecordWriter(path, options);"
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,218,    try {
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,219,      if (dynamicPartitioningUsed) {
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,220,        discoverPartitions(jobContext);
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,221,        // Commit each partition so it gets moved out of the job work
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,222,        // dir
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,223,        for (JobContext context : contextDiscoveredByPath.values()) {
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,224,          new JobConf(context.getConfiguration())
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,225,              .getOutputCommitter().commitJob(context);
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,226,        }
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,227,      }
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,228,      if (getBaseOutputCommitter() != null && !dynamicPartitioningUsed) {
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,229,        getBaseOutputCommitter().commitJob(
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,230,            HCatMapRedUtil.createJobContext(jobContext));
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,232,      registerPartitions(jobContext);
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,233,      // create _SUCCESS FILE if so requested.
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,234,      OutputJobInfo jobInfo = HCatOutputFormat.getJobInfo(jobContext);
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,235,      if (getOutputDirMarking(jobContext.getConfiguration())) {
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,236,        Path outputPath = new Path(jobInfo.getLocation());
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,237,        FileSystem fileSys = outputPath.getFileSystem(jobContext
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,238,            .getConfiguration());
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,239,        // create a file in the folder to mark it
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,240,        if (fileSys.exists(outputPath)) {
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,241,"          Path filePath = new Path(outputPath,"
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,242,              SUCCEEDED_FILE_NAME);
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,243,          if (!fileSys.exists(filePath)) { // may have been
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,244,                           // created by
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,245,                           // baseCommitter.commitJob()
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,246,            fileSys.create(filePath).close();
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,247,          }
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,250,    } finally {
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,251,      cancelDelegationTokens(jobContext);
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,973,"    LOG.info(""Cancelling deletgation token for the job."");"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,1794,"      throw new HiveException(""Partition spec should only be supplied for a "" +"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,1795,"          ""partitioned table"");"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,1848,"      throw new HiveException(""Partition spec should only be supplied for a "" +"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,1849,"                ""partitioned table"");"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,1873,"      throw new HiveException(""Partition spec should only be supplied for a "" +"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,1874,"          ""partitioned table"");"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,1925,"      throw new HiveException(""Partition spec should only be supplied for a "" +"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,1926,"          ""partitioned table"");"
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2950,"          LOG.error(""Got HiveException during obtaining list of partitions"");"
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2960,"          LOG.debug(""Wrong specification"");"
common/src/java/org/apache/hadoop/hive/common/FileUtils.java,300,      for (FileStatus stat : fs.listStatus(fileStatus.getPath())) {
ql/src/java/org/apache/hadoop/hive/ql/io/BucketizedHiveInputFormat.java,94,    FileStatus[] matches = fs.globStatus(dir);
ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java,65,    FileStatus[] srcs = fs.globStatus(path);
ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java,68,        srcs = fs.listStatus(srcs[0].getPath());
ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/JsonMetaDataFormatter.java,192,
ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/JsonMetaDataFormatter.java,193,"    putFileSystemsStats(builder, makeTableStatusLocations(tbl, db, par),"
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,453,          HadoopJobExecHelper.runningJobKillURIs.remove(rj.getJobID());
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,70,  public transient String jobId;
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,71,  private LogHelper console;
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,72,  private HadoopJobExecHook callBackObj;
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,92,  private static String getJobStartMsg(String jobId) {
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,102,  public static String getJobEndMsg(String jobId) {
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,123,  public String getJobId() {
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,127,  public void setJobId(String jobId) {
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,151,"  public static Map<String, String> runningJobKillURIs = Collections"
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,152,"      .synchronizedMap(new HashMap<String, String>());"
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,164,    if (new org.apache.hadoop.conf.Configuration()
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,165,"        .getBoolean(""webinterface.private.actions"", false)) {"
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,172,    }
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,176,    synchronized (runningJobKillURIs) {
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,177,      for (String uri : runningJobKillURIs.values()) {
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,179,"          System.err.println(""killing job with: "" + uri);"
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,180,          java.net.HttpURLConnection conn = (java.net.HttpURLConnection) new java.net.URL(uri)
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,181,               .openConnection();
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,182,"          conn.setRequestMethod(""POST"");"
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,183,          int retCode = conn.getResponseCode();
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,184,          if (retCode != 200) {
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,185,"            System.err.println(""Got an error trying to kill job with URI: "" + uri + "" = """
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,186,                + retCode);
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,187,          }
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,189,"          System.err.println(""trying to kill job, caught: "" + e);"
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,255,        TaskReport[] mappers = jc.getMapTaskReports(rj.getJobID());
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,267,        TaskReport[] reducers = jc.getReduceTaskReports(rj.getJobID());
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,284,      RunningJob newRj = jc.getJob(rj.getJobID());
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,290,"        throw new IOException(""Could not find status of job:"" + rj.getJobID());"
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,431,"            getId(), Keys.TASK_HADOOP_ID, rj.getJobID());"
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,433,"      console.printInfo(getJobStartMsg(rj.getJobID()) + "", Tracking URL = """
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,436,"          + "" job  -kill "" + rj.getJobID());"
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,512,    jobId = rj.getJobID();
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,530,"    runningJobKillURIs.put(rj.getJobID(), rj.getTrackingURL() + ""&action=kill"");"
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,551,    String statusMesg = getJobEndMsg(rj.getJobID());
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,595,"    ReducerTimeStatsPerJob reducerTimeStatsPerJob = new ReducerTimeStatsPerJob(reducersRunTimes,"
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,596,        new String(this.jobId));
ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/merge/BlockMergeTask.java,244,          HadoopJobExecHelper.runningJobKillURIs.remove(rj.getJobID());
ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/merge/BlockMergeTask.java,375,  }
ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/stats/PartialScanTask.java,251,          HadoopJobExecHelper.runningJobKillURIs.remove(rj.getJobID());
ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/truncate/ColumnTruncateTask.java,220,          HadoopJobExecHelper.runningJobKillURIs.remove(rj.getJobID());
ql/src/java/org/apache/hadoop/hive/ql/plan/ReducerTimeStatsPerJob.java,35,  // stores the JobId of the job
ql/src/java/org/apache/hadoop/hive/ql/plan/ReducerTimeStatsPerJob.java,36,  private final String jobId;
ql/src/java/org/apache/hadoop/hive/ql/plan/ReducerTimeStatsPerJob.java,37,
ql/src/java/org/apache/hadoop/hive/ql/plan/ReducerTimeStatsPerJob.java,50,"  public ReducerTimeStatsPerJob(List<Integer> reducersRunTimes, String jobId) {"
ql/src/java/org/apache/hadoop/hive/ql/plan/ReducerTimeStatsPerJob.java,51,    this.jobId = jobId;
ql/src/java/org/apache/hadoop/hive/ql/plan/ReducerTimeStatsPerJob.java,106,
ql/src/java/org/apache/hadoop/hive/ql/plan/ReducerTimeStatsPerJob.java,107,  public String getJobId() {
ql/src/java/org/apache/hadoop/hive/ql/plan/ReducerTimeStatsPerJob.java,108,    return this.jobId;
ql/src/java/org/apache/hadoop/hive/ql/plan/ReducerTimeStatsPerJob.java,109,  }
ql/src/java/org/apache/hadoop/hive/ql/plan/ReducerTimeStatsPerJob.java,110,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,34,import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,35,import org.apache.hadoop.hive.serde2.objectinspector.primitive.ByteObjectInspector;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,36,import org.apache.hadoop.hive.serde2.objectinspector.primitive.DoubleObjectInspector;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,37,import org.apache.hadoop.hive.serde2.objectinspector.primitive.FloatObjectInspector;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,38,import org.apache.hadoop.hive.serde2.objectinspector.primitive.IntObjectInspector;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,39,import org.apache.hadoop.hive.serde2.objectinspector.primitive.LongObjectInspector;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,41,import org.apache.hadoop.hive.serde2.objectinspector.primitive.ShortObjectInspector;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,42,import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,43,import org.apache.hadoop.hive.serde2.objectinspector.primitive.TimestampObjectInspector;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,70,    if (arguments[0].getTypeName() != serdeConstants.STRING_TYPE_NAME
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,71,      && arguments[0].getTypeName() != serdeConstants.VOID_TYPE_NAME) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,73,"        + "" of function PRINTF must be \"""" + serdeConstants.STRING_TYPE_NAME"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,74,"        + ""\"", but \"""" + arguments[0].getTypeName() + ""\"" was found."");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,86,    return PrimitiveObjectInspectorFactory.writableStringObjectInspector;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,94,    String pattern = ((StringObjectInspector) argumentOIs[0])
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,95,        .getPrimitiveJavaObject(arguments[0].get());
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,97,    ArrayList argumentList = new ArrayList();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,101,          argumentList.add(((BooleanObjectInspector)argumentOIs[i]).get(arguments[i].get()));
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,102,          break;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,104,          argumentList.add(((ByteObjectInspector)argumentOIs[i]).get(arguments[i].get()));
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,105,          break;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,107,          argumentList.add(((ShortObjectInspector)argumentOIs[i]).get(arguments[i].get()));
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,108,          break;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,110,          argumentList.add(((IntObjectInspector)argumentOIs[i]).get(arguments[i].get()));
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,111,          break;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,113,          argumentList.add(((LongObjectInspector)argumentOIs[i]).get(arguments[i].get()));
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,114,          break;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,116,          argumentList.add(((FloatObjectInspector)argumentOIs[i]).get(arguments[i].get()));
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,117,          break;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,119,          argumentList.add(((DoubleObjectInspector)argumentOIs[i]).get(arguments[i].get()));
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,120,          break;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,122,          argumentList.add(((StringObjectInspector)argumentOIs[i])
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,123,            .getPrimitiveJavaObject(arguments[i].get()));
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,124,          break;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,126,          argumentList.add(((TimestampObjectInspector)argumentOIs[i])
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,129,        case BINARY:
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,130,          argumentList.add(arguments[i].get());
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,131,          break;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPrintf.java,137,"    formatter.format(pattern, argumentList.toArray());"
hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hcatalog/pig/HCatLoader.java,180,      PigHCatUtil.getHCatServerPrincipal(job));
hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hcatalog/pig/HCatLoader.java,196,      PigHCatUtil.getHCatServerPrincipal(job));
hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hcatalog/pig/PigHCatUtil.java,136,"                             String serverKerberosPrincipal, Class<?> clazz) throws Exception {"
hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hcatalog/pig/PigHCatUtil.java,137,    HiveConf hiveConf = new HiveConf(clazz);
hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hcatalog/pig/PigHCatUtil.java,173,"  public Table getTable(String location, String hcatServerUri, String hcatServerPrincipal) throws IOException {"
hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hcatalog/pig/PigHCatUtil.java,186,"      client = getHiveMetaClient(hcatServerUri, hcatServerPrincipal, PigHCatUtil.class);"
beeline/src/java/org/apache/hive/beeline/BeeLine.java,570,        files.add(args[i]);
itests/test-serde/src/main/java/org/apache/hadoop/hive/serde2/TestSerDe.java,97,          .getInstance(columnNames);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,162,import org.apache.hadoop.hive.serde2.SerDeUtils;
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,2902,        boolean getColsFromSerDe = SerDeUtils.shouldGetColsFromSerDe(
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,2903,            tbl.getSd().getSerdeInfo().getSerializationLib());
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,2904,        if (!getColsFromSerDe) {
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java,205,          params.remove(StatsSetupConst.STATS_GENERATED_VIA_STATS_TASK);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java,236,
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java,475,
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java,482,
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java,1153,    return (comment == null || comment.isEmpty()) ? FROM_SERIALIZER : comment;
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,3201,            if (!descTbl.isFormatted()) {
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,3202,              if (tableName.equals(colPath)) {
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,3203,                cols.addAll(tbl.getPartCols());
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,3204,              }
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,3205,            }
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,3507,"        tbl.setFields(Hive.getFieldsFromDeserializer(tbl.getTableName(), tbl."
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,3508,            getDeserializer()));
ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java,502,    if (!SerDeUtils.shouldGetColsFromSerDe(
ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java,503,        tPartition.getSd().getSerdeInfo().getSerializationLib())) {
ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java,504,      return tPartition.getSd().getCols();
ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java,505,    }
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java,613,    boolean getColsFromSerDe = SerDeUtils.shouldGetColsFromSerDe(
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java,614,      getSerializationLib());
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java,615,    if (!getColsFromSerDe) {
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java,616,      return tTable.getSd().getCols();
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java,617,    } else {
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java,618,      try {
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java,620,      } catch (HiveException e) {
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java,621,"        LOG.error(""Unable to get field from serde: "" + getSerializationLib(), e);"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java,623,      return new ArrayList<FieldSchema>();
ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/MetaDataFormatUtils.java,314,"    return col.getComment() != null ? col.getComment() : ""None"";"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1554,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1877,"  private void parseJoinCondition(QBJoinTree joinTree, ASTNode joinCond, List<String> leftSrc,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2041,"  private void extractJoinCondsFromWhereClause(QBJoinTree joinTree, QB qb, String dest, ASTNode predicate,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2046,"      extractJoinCondsFromWhereClause(joinTree, qb, dest,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2048,"      extractJoinCondsFromWhereClause(joinTree, qb, dest,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8943,"            extractJoinCondsFromWhereClause(joinTree, qb, dest,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,9910,      crtTblDesc.validate();
ql/src/java/org/apache/hadoop/hive/ql/parse/TaskCompiler.java,224,      crtTblDesc.validate();
ql/src/java/org/apache/hadoop/hive/ql/parse/TaskCompiler.java,342,"  protected void optimizeOperatorPlan(ParseContext pCtxSet, Set<ReadEntity> inputs,"
ql/src/java/org/apache/hadoop/hive/ql/plan/CreateTableDesc.java,406,  public void validate()
ql/src/java/org/apache/hadoop/hive/ql/plan/CreateTableDesc.java,412,          || !SerDeUtils.shouldGetColsFromSerDe(this.getSerName())) {
serde/src/java/org/apache/hadoop/hive/serde2/RegexSerDe.java,144,"        columnNames, columnOIs);"
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,71,"  public static void registerSerDe(String name, Class<?> serde) {}"
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,72,
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,73,  private static List<String> nativeSerDeNames = new ArrayList<String>();
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,74,  static {
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,75,    nativeSerDeNames
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,76,        .add(org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe.class
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,77,        .getName());
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,78,    nativeSerDeNames
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,79,        .add(org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe.class
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,80,        .getName());
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,81,    // For backward compatibility
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,82,"    nativeSerDeNames.add(""org.apache.hadoop.hive.serde.thrift.columnsetSerDe"");"
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,83,    nativeSerDeNames
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,84,        .add(org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.class.getName());
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,85,    nativeSerDeNames.add(org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe.class.getName());
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,86,  }
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,87,
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,88,  public static boolean shouldGetColsFromSerDe(String serde) {
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,89,    return (serde != null) && !nativeSerDeNames.contains(serde);
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,90,  }
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,91,
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,92,  private static boolean initCoreSerDes = registerCoreSerDes();
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,93,
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,94,  protected static boolean registerCoreSerDes() {
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,95,    // Eagerly load SerDes so they will register their symbolic names even on
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,96,    // Lazy Loading JVMs
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,97,    try {
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,98,      // loading these classes will automatically register the short names
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,99,      Class
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,100,          .forName(org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe.class
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,101,          .getName());
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,102,      Class.forName(org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.class
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,103,          .getName());
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,104,      Class
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,105,          .forName(org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.class
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,106,          .getName());
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,107,    } catch (ClassNotFoundException e) {
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,108,      throw new RuntimeException(
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,109,"          ""IMPOSSIBLE Exception: Unable to initialize core serdes"", e);"
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,110,    }
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,111,    return true;
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,112,  }
serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java,113,
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,842,
ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/MetaDataFormatUtils.java,95,"      List<FieldSchema> partCols, boolean printHeader, boolean isOutputPadded) {"
ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/MetaDataFormatUtils.java,102,    if ((partCols != null) && (!partCols.isEmpty())) {
ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/MetaDataFormatUtils.java,374,      return new TextMetaDataFormatter(conf.getIntVar(HiveConf.ConfVars.CLIPRETTYOUTPUTNUMCOLS));
ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/TextMetaDataFormatter.java,61,  public TextMetaDataFormatter(int prettyOutputNumCols) {
ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/TextMetaDataFormatter.java,126,"                  MetaDataFormatUtils.getAllColumnsInformation(cols, partCols, isFormatted, isOutputPadded);"
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseTableInputFormat.java,273,    // There should be no residual since we already negotiated
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseTableInputFormat.java,274,    // that earlier in HBaseStorageHandler.decomposePredicate.
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseTableInputFormat.java,276,      throw new RuntimeException(
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseTableInputFormat.java,277,"        ""Unexpected residual predicate "" + residualPredicate.getExprString());"
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseTableInputFormat.java,278,    }
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseTableInputFormat.java,279,
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseTableInputFormat.java,280,    // There should be exactly one predicate since we already
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseTableInputFormat.java,281,    // negotiated that also.
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseTableInputFormat.java,282,    if (searchConditions.size() < 1 || searchConditions.size() > 2) {
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseTableInputFormat.java,283,      throw new RuntimeException(
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseTableInputFormat.java,284,"        ""Either one or two search conditions expected in push down"");"
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseTableInputFormat.java,295,      try{
serde/src/test/org/apache/hadoop/hive/serde2/lazy/TestLazyArrayMapStruct.java,182,"        assertEquals(""{'2':'d\\tf','2':'d','-1':null,'0':'0','8':'abc'}"""
serde/src/test/org/apache/hadoop/hive/serde2/lazy/TestLazyArrayMapStruct.java,246,"        assertEquals(""{'a':null,'b':['',''],'c':{'':null,'':null},'d':':'}"""
ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java,1486,        int len = insp.getMapSize(obj);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java,1487,        lengths.write(len);
itests/hive-unit/src/test/java/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java,520,"        assertEquals(res.getInt(1), res.getInt(""under_col""));"
itests/hive-unit/src/test/java/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java,521,"        assertEquals(res.getString(1), res.getString(""under_col""));"
itests/hive-unit/src/test/java/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java,522,"        assertEquals(res.getString(2), res.getString(""value""));"
itests/hive-unit/src/test/java/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java,525,"          assertEquals(res.getString(3), res.getString(partitionedColumnName));"
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,918,"        assertEquals(res.getInt(1), res.getInt(""under_col""));"
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,919,"        assertEquals(res.getString(1), res.getString(""under_col""));"
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,920,"        assertEquals(res.getString(2), res.getString(""value""));"
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,923,"          assertEquals(res.getString(3), res.getString(partitionedColumnName));"
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,1869,"    execFetchFirst(""select * from "" + dataTypeTableName, ""c4"", false);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,9220,    resultSchema =
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,9221,        convertRowSchemaToViewSchema(opParseCtx.get(sinkOp).getRowResolver());
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,9409,  private List<FieldSchema> convertRowSchemaToViewSchema(RowResolver rr) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,9415,      String colName = rr.reverseLookup(colInfo.getInternalName())[1];
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,9416,"      fieldSchemas.add(new FieldSchema(colName,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,9417,"          colInfo.getType().getTypeName(), null));"
hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hive/hcatalog/pig/HCatStorer.java,152,    HCatContext.INSTANCE.setConf(job.getConfiguration()).getConf().get()
hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hive/hcatalog/pig/HCatStorer.java,153,"      .setBoolean(HCatConstants.HCAT_DATA_TINY_SMALL_INT_PROMOTION, false);"
hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hive/hcatalog/pig/HCatStorer.java,154,
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatLoader.java,97,"    driver.run(""drop table "" + tablename);"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatLoaderStorer.java,140,  }
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatStorer.java,361,  private static void dumpFile(String fileName) throws Exception {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/JobSubmissionConstants.java,36,"  public static final String TOKEN_FILE_ARG_PLACEHOLDER = ""__WEBHCAT_TOKEN_FILE_LOCATION__"";"
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapredLocalTask.java,84,"  private Map<String, FetchOperator> fetchOperators = new HashMap<String, FetchOperator>();"
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapredLocalTask.java,141,
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapredLocalTask.java,461,"    BucketMatcher bucketMatcher = (BucketMatcher) ReflectionUtils.newInstance(bucketMatcherCls,"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,2281,        fs.mkdirs(destf);
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,2283,"          fs.setPermission(destf, fs.getFileStatus(destf.getParent()).getPermission());"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TrivialExecService.java,68,"            ""log4j.configuration=file://"" + log4jProps.getAbsolutePath());"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java,450,"   * backing buffer, for avoiding string encoding and decoding. Shamelessly copy"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java,451,"   * from {@link org.apache.hadoop.io.Text#find(String, int)}."
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java,466,"    ByteBuffer src = ByteBuffer.wrap(text.getBytes(), 0, text.getLength());"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java,467,    ByteBuffer tgt = ByteBuffer
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java,468,"        .wrap(subtext.getBytes(), 0, subtext.getLength());"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java,469,    byte b = tgt.get();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java,470,    src.position(start);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java,471,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java,472,    while (src.hasRemaining()) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java,473,      if (b == src.get()) { // matching first byte
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java,474,        src.mark(); // save position in loop
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java,475,        tgt.mark(); // save position in target
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java,476,        boolean found = true;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java,477,        int pos = src.position() - 1;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java,478,        while (tgt.hasRemaining()) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java,479,          if (!src.hasRemaining()) { // src expired first
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java,480,            tgt.reset();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java,481,            src.reset();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java,482,            found = false;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java,483,            break;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java,484,          }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java,485,          if (!(tgt.get() == src.get())) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java,486,            tgt.reset();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java,487,            src.reset();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java,488,            found = false;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java,489,            break; // no match
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java,490,          }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java,491,        }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java,492,        if (found) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java,493,          return pos;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java,494,        }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java,495,      }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java,497,    return -1; // not found
itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/history/TestHiveHistory.java,106,"        db.loadTable(hadoopDataFile[i], src, false, false, false);"
ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java,276,"              .getTableName(), tbd.getReplace(), tbd.getHoldDDLTime(), work.isSrcLocal());"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,1473,"      boolean holdDDLTime, boolean isSrcLocal) throws HiveException {"
ql/src/java/org/apache/hadoop/hive/ql/optimizer/listbucketingpruner/LBPartitionProcFactory.java,55,      //Run partition pruner to get partitions
ql/src/java/org/apache/hadoop/hive/ql/optimizer/listbucketingpruner/LBPartitionProcFactory.java,56,      ParseContext parseCtx = owc.getParseContext();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/listbucketingpruner/LBPartitionProcFactory.java,57,      PrunedPartitionList prunedPartList;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/listbucketingpruner/LBPartitionProcFactory.java,58,      try {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/listbucketingpruner/LBPartitionProcFactory.java,59,        String alias = (String) parseCtx.getTopOps().keySet().toArray()[0];
ql/src/java/org/apache/hadoop/hive/ql/optimizer/listbucketingpruner/LBPartitionProcFactory.java,60,"        prunedPartList = PartitionPruner.prune(top, parseCtx, alias);"
ql/src/java/org/apache/hadoop/hive/ql/optimizer/listbucketingpruner/LBPartitionProcFactory.java,61,      } catch (HiveException e) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/listbucketingpruner/LBPartitionProcFactory.java,62,        // Has to use full name to make sure it does not conflict with
ql/src/java/org/apache/hadoop/hive/ql/optimizer/listbucketingpruner/LBPartitionProcFactory.java,63,        // org.apache.commons.lang.StringUtils
ql/src/java/org/apache/hadoop/hive/ql/optimizer/listbucketingpruner/LBPartitionProcFactory.java,64,"        throw new SemanticException(e.getMessage(), e);"
ql/src/java/org/apache/hadoop/hive/ql/optimizer/listbucketingpruner/LBPartitionProcFactory.java,66,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/listbucketingpruner/LBPartitionProcFactory.java,67,      if (prunedPartList != null) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/listbucketingpruner/LBPartitionProcFactory.java,68,        owc.setPartitions(prunedPartList);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/listbucketingpruner/LBPartitionProcFactory.java,69,      }
ql/src/java/org/apache/hadoop/hive/ql/optimizer/listbucketingpruner/LBPartitionProcFactory.java,70,
ql/src/test/org/apache/hadoop/hive/ql/exec/TestExecDriver.java,135,"        db.loadTable(hadoopDataFile[i], src, false, false, true);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/AppConfig.java,123,"  public static final String TEMPLETON_JAR_NAME  = ""templeton.jar"";"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/AppConfig.java,244,  public String templetonJar()     { return get(TEMPLETON_JAR_NAME); }
ql/src/java/org/apache/hadoop/hive/ql/parse/ExplainSemanticAnalyzer.java,69,    Task<? extends Serializable> fetchTask = sem.getFetchTask();
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1119,      String indexTableName =
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1120,          crtIndex.getIndexTableName() != null ? crtIndex.getIndexTableName() :
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1121,"            MetaStoreUtils.getIndexTableName(SessionState.get().getCurrentDatabase(),"
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1122,"                crtIndex.getTableName(), crtIndex.getIndexName());"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,1549,  private static org.apache.hadoop.hive.metastore.api.Partition convertAddSpecToMetaPartition(
ql/src/java/org/apache/hadoop/hive/ql/exec/PTFOperator.java,58, transient HiveConf hiveConf;
ql/src/java/org/apache/hadoop/hive/ql/exec/PTFOperator.java,69,"  hiveConf = new HiveConf(jobConf, PTFOperator.class);"
ql/src/java/org/apache/hadoop/hive/ql/exec/PTFOperator.java,70,"  // if the parent is ExtractOperator, this invocation is from reduce-side"
ql/src/java/org/apache/hadoop/hive/ql/exec/PTFOperator.java,75,    inputPart = createFirstPartitionForChain(
ql/src/java/org/apache/hadoop/hive/ql/exec/PTFOperator.java,76,"        inputObjInspectors[0], hiveConf, isMapOperator);"
ql/src/java/org/apache/hadoop/hive/ql/exec/PTFOperator.java,150, protected void reconstructQueryDef(HiveConf hiveConf) throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/exec/PTFOperator.java,194,
ql/src/java/org/apache/hadoop/hive/ql/exec/PTFOperator.java,195,    Iterator<Object> pItr = tDef.getTFunction().canIterateOutput() ?
ql/src/java/org/apache/hadoop/hive/ql/exec/PTFOperator.java,198,
ql/src/java/org/apache/hadoop/hive/ql/exec/PTFOperator.java,283,"      HiveConf hiveConf, boolean isMapSide) throws HiveException {"
ql/src/java/org/apache/hadoop/hive/ql/exec/PTFOperator.java,285,    TableFunctionEvaluator tEval = tabDef.getTFunction();
ql/src/java/org/apache/hadoop/hive/ql/exec/PTFPartition.java,49,"  protected PTFPartition(HiveConf cfg,"
ql/src/java/org/apache/hadoop/hive/ql/exec/PTFPartition.java,225,"  public static PTFPartition create(HiveConf cfg,"
ql/src/java/org/apache/hadoop/hive/ql/plan/PTFDesc.java,45,  transient HiveConf cfg;
ql/src/java/org/apache/hadoop/hive/ql/plan/PTFDesc.java,84,  public HiveConf getCfg() {
ql/src/java/org/apache/hadoop/hive/ql/plan/PTFDesc.java,88,  public void setCfg(HiveConf cfg) {
ql/src/java/org/apache/hadoop/hive/ql/plan/PTFDeserializer.java,67,  HiveConf hConf;
ql/src/java/org/apache/hadoop/hive/ql/plan/PTFDeserializer.java,70,"  public PTFDeserializer(PTFDesc ptfDesc, StructObjectInspector inputOI, HiveConf hConf) {"
ql/src/java/org/apache/hadoop/hive/ql/plan/PTFDeserializer.java,291,"      return (TableFunctionResolver) ReflectionUtils.newInstance(rCls, null);"
ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java,246,  // This 'neededColumnIDs' field is included in this operator class instead of
ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java,247,"  // its desc class.The reason is that 1)tableScanDesc can not be instantiated,"
ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java,248,  // and 2) it will fail some join and union queries if this is added forcibly
ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java,249,  // into tableScanDesc.
ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java,250,  // Both neededColumnIDs and neededColumns should never be null.
ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java,251,"  // When neededColumnIDs is an empty list,"
ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java,252,  // it means no needed column (e.g. we do not need any column to evaluate
ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java,253,  // SELECT count(*) FROM t).
ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java,254,  List<Integer> neededColumnIDs;
ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java,255,  List<String> neededColumns;
ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java,256,
ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java,258,    neededColumnIDs = orign_columns;
ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java,262,    return neededColumnIDs;
ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java,266,    neededColumns = columnNames;
ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java,270,    return neededColumns;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java,325,      for (int i = 0; i < cols.size(); i++) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java,326,        String[] tabCol = inputRR.reverseLookup(cols.get(i));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java,327,        if(tabCol == null) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java,343,        int position = inputRR.getPosition(cols.get(i));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java,347,          neededColumnNames.add(cols.get(i));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRTableScan1.java,150,"              PrunedPartitionList partList = new PrunedPartitionList(source, confirmedPartns, false);"
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,215,      return (ExprNodeGenericFuncDesc)expr;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,217,"        throw new IllegalStateException(""Unexpected type of ExprNodeDesc: "" + expr.getExprString());"
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,230,"  static private ExprNodeDesc removeNonPartCols(ExprNodeDesc expr, List<String> partCols) {"
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,231,    if (expr instanceof ExprNodeColumnDesc
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,232,        && !partCols.contains(((ExprNodeColumnDesc) expr).getColumn())) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,233,      // Column doesn't appear to be a partition column for the table.
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,234,"      return new ExprNodeConstantDesc(expr.getTypeInfo(), null);"
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,239,"        children.set(i, removeNonPartCols(children.get(i), partCols));"
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,269,"        return new PrunedPartitionList(tab, getAllPartitions(tab), false);"
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,282,"        return new PrunedPartitionList(tab, getAllPartitions(tab), false);"
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,286,"      prunerExpr = removeNonPartCols(prunerExpr, extractPartColNames(tab));"
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,293,"        return new PrunedPartitionList(tab, getAllPartitions(tab), true);"
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnAccessAnalyzer.java,25,import org.apache.hadoop.hive.metastore.api.FieldSchema;
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnAccessAnalyzer.java,47,      List<FieldSchema> tableCols = table.getCols();
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnAccessAnalyzer.java,48,      for (int i : op.getNeededColumnIDs()) {
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnAccessAnalyzer.java,49,"        columnAccessInfo.add(tableName, tableCols.get(i).getName());"
ql/src/java/org/apache/hadoop/hive/ql/parse/ProcessAnalyzeTable.java,31,import org.apache.hadoop.hive.ql.ErrorMsg;
ql/src/java/org/apache/hadoop/hive/ql/parse/ProcessAnalyzeTable.java,43,import org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.tableSpec;
ql/src/java/org/apache/hadoop/hive/ql/parse/ProcessAnalyzeTable.java,44,import org.apache.hadoop.hive.ql.parse.GenTezWork;
ql/src/java/org/apache/hadoop/hive/ql/parse/ProcessAnalyzeTable.java,45,import org.apache.hadoop.hive.ql.parse.ParseContext;
ql/src/java/org/apache/hadoop/hive/ql/parse/ProcessAnalyzeTable.java,46,import org.apache.hadoop.hive.ql.parse.PrunedPartitionList;
ql/src/java/org/apache/hadoop/hive/ql/parse/ProcessAnalyzeTable.java,47,import org.apache.hadoop.hive.ql.parse.QBParseInfo;
ql/src/java/org/apache/hadoop/hive/ql/parse/ProcessAnalyzeTable.java,48,import org.apache.hadoop.hive.ql.parse.SemanticException;
ql/src/java/org/apache/hadoop/hive/ql/parse/ProcessAnalyzeTable.java,153,"        partitions = new PrunedPartitionList(source, confirmedPartns, false);"
ql/src/java/org/apache/hadoop/hive/ql/parse/PrunedPartitionList.java,42,"  public PrunedPartitionList(Table source, Set<Partition> partitions, boolean hasUnknowns) {"
ql/src/java/org/apache/hadoop/hive/ql/parse/QBParseInfo.java,534,   * This method is used only for the anlayze command to get the partition specs
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,29,import org.apache.hadoop.record.Buffer;
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,54,   *
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,74,   *
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,85,    if (code == Type.BYTES.code) {
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,86,      return new Buffer(readBytes());
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,87,    } else if (code == Type.BYTE.code) {
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,111,    } else if (50 <= code && code <= 200) { // application-specific typecodes
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,112,      return new Buffer(readBytes());
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,122,   *
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,150,    } else if (code == Type.VECTOR.code) {
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,151,      return readRawVector();
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,152,    } else if (code == Type.LIST.code) {
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,153,      return readRawList();
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,154,    } else if (code == Type.MAP.code) {
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,155,      return readRawMap();
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,167,   *
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,188,   *
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,203,   *
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,216,   *
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,234,   *
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,244,   *
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,257,   *
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,267,   *
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,280,   *
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,290,   *
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,300,   *
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,313,   *
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,323,   *
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,336,   *
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,346,   *
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,359,   *
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,369,   *
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,382,   *
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,392,   *
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,410,   *
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,424,  /**
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,425,   * Reads the raw bytes following a <code>Type.VECTOR</code> code.
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,426,   *
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,427,   * @return the obtained bytes sequence
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,428,   * @throws IOException
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,429,   */
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,430,  public byte[] readRawVector() throws IOException {
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,431,    Buffer buffer = new Buffer();
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,432,    int length = readVectorHeader();
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,433,"    buffer.append(new byte[] {(byte) Type.VECTOR.code,"
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,434,"        (byte) (0xff & (length >> 24)), (byte) (0xff & (length >> 16)),"
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,435,"        (byte) (0xff & (length >> 8)), (byte) (0xff & length)});"
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,436,    for (int i = 0; i < length; i++) {
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,437,      buffer.append(readRaw());
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,438,    }
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,439,    return buffer.get();
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,440,  }
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,441,
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,444,   *
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,454,   *
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,469,  /**
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,470,   * Reads the raw bytes following a <code>Type.LIST</code> code.
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,471,   *
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,472,   * @return the obtained bytes sequence
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,473,   * @throws IOException
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,474,   */
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,475,  public byte[] readRawList() throws IOException {
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,476,    Buffer buffer = new Buffer(new byte[] {(byte) Type.LIST.code});
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,477,    byte[] bytes = readRaw();
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,478,    while (bytes != null) {
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,479,      buffer.append(bytes);
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,480,      bytes = readRaw();
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,481,    }
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,482,    buffer.append(new byte[] {(byte) Type.MARKER.code});
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,483,    return buffer.get();
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,484,  }
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,485,
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,488,   *
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,504,  /**
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,505,   * Reads the raw bytes following a <code>Type.MAP</code> code.
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,506,   *
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,507,   * @return the obtained bytes sequence
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,508,   * @throws IOException
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,509,   */
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,510,  public byte[] readRawMap() throws IOException {
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,511,    Buffer buffer = new Buffer();
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,512,    int length = readMapHeader();
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,513,"    buffer.append(new byte[] {(byte) Type.MAP.code,"
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,514,"        (byte) (0xff & (length >> 24)), (byte) (0xff & (length >> 16)),"
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,515,"        (byte) (0xff & (length >> 8)), (byte) (0xff & length)});"
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,516,    for (int i = 0; i < length; i++) {
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,517,      buffer.append(readRaw());
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,518,      buffer.append(readRaw());
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,519,    }
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,520,    return buffer.get();
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,521,  }
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,522,
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java,525,   *
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesOutput.java,30,import org.apache.hadoop.record.Buffer;
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesOutput.java,80,    if (obj instanceof Buffer) {
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesOutput.java,81,      writeBytes(((Buffer) obj).get());
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesOutput.java,82,    } else if (obj instanceof Byte) {
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,24,import org.apache.hadoop.record.Buffer;
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,25,import org.apache.hadoop.record.Index;
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,26,import org.apache.hadoop.record.RecordInput;
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,27,
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,31,public class TypedBytesRecordInput implements RecordInput {
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,52,   *
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,67,   *
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,92,  public Buffer readBuffer(String tag) throws IOException {
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,93,    in.skipType();
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,94,    return new Buffer(in.readBytes());
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,95,  }
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,96,
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,131,  public Index startVector(String tag) throws IOException {
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,132,    in.skipType();
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,133,    return new TypedBytesIndex(in.readVectorHeader());
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,134,  }
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,135,
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,136,  public Index startMap(String tag) throws IOException {
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,137,    in.skipType();
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,138,    return new TypedBytesIndex(in.readMapHeader());
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,139,  }
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,140,
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,149,
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,150,  private static final class TypedBytesIndex implements Index {
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,151,    private int nelems;
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,152,
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,153,    private TypedBytesIndex(int nelems) {
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,154,      this.nelems = nelems;
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,155,    }
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,156,
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,157,    public boolean done() {
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,158,      return (nelems <= 0);
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,159,    }
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,160,
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,161,    public void incr() {
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,162,      nelems--;
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,163,    }
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,164,  }
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java,165,
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordOutput.java,26,import org.apache.hadoop.record.Buffer;
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordOutput.java,27,import org.apache.hadoop.record.Record;
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordOutput.java,28,import org.apache.hadoop.record.RecordOutput;
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordOutput.java,29,
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordOutput.java,33,public class TypedBytesRecordOutput implements RecordOutput {
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordOutput.java,54,   *
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordOutput.java,69,   *
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordOutput.java,93,"  public void writeBuffer(Buffer buf, String tag) throws IOException {"
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordOutput.java,94,    out.writeBytes(buf.get());
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordOutput.java,95,  }
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordOutput.java,96,
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordOutput.java,121,"  public void startRecord(Record r, String tag) throws IOException {"
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordOutput.java,122,    out.writeListHeader();
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordOutput.java,123,  }
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordOutput.java,124,
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordOutput.java,133,"  public void endRecord(Record r, String tag) throws IOException {"
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordOutput.java,134,    out.writeListFooter();
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordOutput.java,135,  }
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordOutput.java,136,
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesWritableOutput.java,47,import org.apache.hadoop.record.Record;
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesWritableOutput.java,142,    } else if (w instanceof Record) {
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesWritableOutput.java,143,      writeRecord((Record) w);
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesWritableOutput.java,224,  public void writeRecord(Record r) throws IOException {
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesWritableOutput.java,225,    r.serialize(TypedBytesRecordOutput.get(out));
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesWritableOutput.java,226,  }
contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesWritableOutput.java,227,
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,48,  private Decimal lowValue; // required
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,49,  private Decimal highValue; // required
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,127,"    tmpMap.put(_Fields.LOW_VALUE, new org.apache.thrift.meta_data.FieldMetaData(""lowValue"", org.apache.thrift.TFieldRequirementType.REQUIRED,"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,129,"    tmpMap.put(_Fields.HIGH_VALUE, new org.apache.thrift.meta_data.FieldMetaData(""highValue"", org.apache.thrift.TFieldRequirementType.REQUIRED,"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,143,"    Decimal lowValue,"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,144,"    Decimal highValue,"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,149,    this.lowValue = lowValue;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,150,    this.highValue = highValue;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,497,"    sb.append(""lowValue:"");"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,498,    if (this.lowValue == null) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,499,"      sb.append(""null"");"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,500,    } else {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,501,      sb.append(this.lowValue);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,503,    first = false;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,504,"    if (!first) sb.append("", "");"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,505,"    sb.append(""highValue:"");"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,506,    if (this.highValue == null) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,507,"      sb.append(""null"");"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,508,    } else {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,509,      sb.append(this.highValue);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,511,    first = false;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,526,    if (!isSetLowValue()) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,527,"      throw new org.apache.thrift.protocol.TProtocolException(""Required field 'lowValue' is unset! Struct:"" + toString());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,528,    }
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,529,
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,530,    if (!isSetHighValue()) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,531,"      throw new org.apache.thrift.protocol.TProtocolException(""Required field 'highValue' is unset! Struct:"" + toString());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,532,    }
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,533,
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,635,        oprot.writeFieldBegin(LOW_VALUE_FIELD_DESC);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,636,        struct.lowValue.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,637,        oprot.writeFieldEnd();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,640,        oprot.writeFieldBegin(HIGH_VALUE_FIELD_DESC);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,641,        struct.highValue.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,642,        oprot.writeFieldEnd();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,667,      struct.lowValue.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,668,      struct.highValue.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,676,      struct.lowValue = new Decimal();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,677,      struct.lowValue.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,678,      struct.setLowValueIsSet(true);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,679,      struct.highValue = new Decimal();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,680,      struct.highValue.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java,681,      struct.setHighValueIsSet(true);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,48,  private double lowValue; // required
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,49,  private double highValue; // required
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,129,"    tmpMap.put(_Fields.LOW_VALUE, new org.apache.thrift.meta_data.FieldMetaData(""lowValue"", org.apache.thrift.TFieldRequirementType.REQUIRED,"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,131,"    tmpMap.put(_Fields.HIGH_VALUE, new org.apache.thrift.meta_data.FieldMetaData(""highValue"", org.apache.thrift.TFieldRequirementType.REQUIRED,"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,145,"    double lowValue,"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,146,"    double highValue,"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,151,    this.lowValue = lowValue;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,152,    setLowValueIsSet(true);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,153,    this.highValue = highValue;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,154,    setHighValueIsSet(true);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,363,    boolean this_present_lowValue = true;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,364,    boolean that_present_lowValue = true;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,372,    boolean this_present_highValue = true;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,373,    boolean that_present_highValue = true;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,406,    boolean present_lowValue = true;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,411,    boolean present_highValue = true;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,497,"    sb.append(""lowValue:"");"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,498,    sb.append(this.lowValue);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,499,    first = false;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,500,"    if (!first) sb.append("", "");"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,501,"    sb.append(""highValue:"");"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,502,    sb.append(this.highValue);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,503,    first = false;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,518,    if (!isSetLowValue()) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,519,"      throw new org.apache.thrift.protocol.TProtocolException(""Required field 'lowValue' is unset! Struct:"" + toString());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,520,    }
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,521,
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,522,    if (!isSetHighValue()) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,523,"      throw new org.apache.thrift.protocol.TProtocolException(""Required field 'highValue' is unset! Struct:"" + toString());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,524,    }
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,525,
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,618,      oprot.writeFieldBegin(LOW_VALUE_FIELD_DESC);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,619,      oprot.writeDouble(struct.lowValue);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,620,      oprot.writeFieldEnd();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,621,      oprot.writeFieldBegin(HIGH_VALUE_FIELD_DESC);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,622,      oprot.writeDouble(struct.highValue);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,623,      oprot.writeFieldEnd();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,647,      oprot.writeDouble(struct.lowValue);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,648,      oprot.writeDouble(struct.highValue);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,656,      struct.lowValue = iprot.readDouble();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,657,      struct.setLowValueIsSet(true);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,658,      struct.highValue = iprot.readDouble();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java,659,      struct.setHighValueIsSet(true);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,48,  private long lowValue; // required
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,49,  private long highValue; // required
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,129,"    tmpMap.put(_Fields.LOW_VALUE, new org.apache.thrift.meta_data.FieldMetaData(""lowValue"", org.apache.thrift.TFieldRequirementType.REQUIRED,"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,131,"    tmpMap.put(_Fields.HIGH_VALUE, new org.apache.thrift.meta_data.FieldMetaData(""highValue"", org.apache.thrift.TFieldRequirementType.REQUIRED,"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,145,"    long lowValue,"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,146,"    long highValue,"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,151,    this.lowValue = lowValue;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,152,    setLowValueIsSet(true);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,153,    this.highValue = highValue;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,154,    setHighValueIsSet(true);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,363,    boolean this_present_lowValue = true;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,364,    boolean that_present_lowValue = true;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,372,    boolean this_present_highValue = true;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,373,    boolean that_present_highValue = true;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,406,    boolean present_lowValue = true;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,411,    boolean present_highValue = true;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,497,"    sb.append(""lowValue:"");"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,498,    sb.append(this.lowValue);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,499,    first = false;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,500,"    if (!first) sb.append("", "");"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,501,"    sb.append(""highValue:"");"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,502,    sb.append(this.highValue);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,503,    first = false;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,518,    if (!isSetLowValue()) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,519,"      throw new org.apache.thrift.protocol.TProtocolException(""Required field 'lowValue' is unset! Struct:"" + toString());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,520,    }
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,521,
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,522,    if (!isSetHighValue()) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,523,"      throw new org.apache.thrift.protocol.TProtocolException(""Required field 'highValue' is unset! Struct:"" + toString());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,524,    }
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,525,
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,618,      oprot.writeFieldBegin(LOW_VALUE_FIELD_DESC);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,619,      oprot.writeI64(struct.lowValue);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,620,      oprot.writeFieldEnd();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,621,      oprot.writeFieldBegin(HIGH_VALUE_FIELD_DESC);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,622,      oprot.writeI64(struct.highValue);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,623,      oprot.writeFieldEnd();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,647,      oprot.writeI64(struct.lowValue);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,648,      oprot.writeI64(struct.highValue);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,656,      struct.lowValue = iprot.readI64();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,657,      struct.setLowValueIsSet(true);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,658,      struct.highValue = iprot.readI64();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java,659,      struct.setHighValueIsSet(true);
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,24,import java.util.ArrayList;
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,25,import java.util.List;
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,29,import org.apache.hadoop.hive.metastore.api.ColumnStatistics;
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,41,import org.apache.hadoop.hive.metastore.model.MFieldSchema;
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,44,import org.apache.hadoop.hive.metastore.model.MStorageDescriptor;
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,77,"           longStats.getLowValue(), longStats.getHighValue());"
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,81,"           doubleStats.getLowValue(), doubleStats.getHighValue());"
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,84,"       String low = createJdoDecimalString(decimalStats.getLowValue()),"
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,85,           high = createJdoDecimalString(decimalStats.getHighValue());
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,102,    oldStatsObj.setLongHighValue(mStatsObj.getLongHighValue());
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,103,    oldStatsObj.setDoubleHighValue(mStatsObj.getDoubleHighValue());
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,104,    oldStatsObj.setLastAnalyzed(mStatsObj.getLastAnalyzed());
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,105,    oldStatsObj.setLongLowValue(mStatsObj.getLongLowValue());
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,106,    oldStatsObj.setDoubleLowValue(mStatsObj.getDoubleLowValue());
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,107,    oldStatsObj.setDecimalLowValue(mStatsObj.getDecimalLowValue());
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,108,    oldStatsObj.setDecimalHighValue(mStatsObj.getDecimalHighValue());
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,166,      longStats.setHighValue(mStatsObj.getLongHighValue());
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,167,      longStats.setLowValue(mStatsObj.getLongLowValue());
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,173,      doubleStats.setHighValue(mStatsObj.getDoubleHighValue());
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,174,      doubleStats.setLowValue(mStatsObj.getDoubleLowValue());
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,180,      decimalStats.setHighValue(createThriftDecimal(mStatsObj.getDecimalHighValue()));
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,181,      decimalStats.setLowValue(createThriftDecimal(mStatsObj.getDecimalLowValue()));
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,222,"          longStats.getLowValue(), longStats.getHighValue());"
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,226,"          doubleStats.getLowValue(), doubleStats.getHighValue());"
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,229,"      String low = createJdoDecimalString(decimalStats.getLowValue()),"
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,230,          high = createJdoDecimalString(decimalStats.getHighValue());
metastore/src/model/org/apache/hadoop/hive/metastore/model/MPartitionColumnStatistics.java,43,  private long longLowValue;
metastore/src/model/org/apache/hadoop/hive/metastore/model/MPartitionColumnStatistics.java,44,  private long longHighValue;
metastore/src/model/org/apache/hadoop/hive/metastore/model/MPartitionColumnStatistics.java,45,  private double doubleLowValue;
metastore/src/model/org/apache/hadoop/hive/metastore/model/MPartitionColumnStatistics.java,46,  private double doubleHighValue;
metastore/src/model/org/apache/hadoop/hive/metastore/model/MPartitionColumnStatistics.java,169,"  public void setLongStats(long numNulls, long numNDVs, long lowValue, long highValue) {"
metastore/src/model/org/apache/hadoop/hive/metastore/model/MPartitionColumnStatistics.java,176,"  public void setDoubleStats(long numNulls, long numNDVs, double lowValue, double highValue) {"
metastore/src/model/org/apache/hadoop/hive/metastore/model/MPartitionColumnStatistics.java,203,  public long getLongLowValue() {
metastore/src/model/org/apache/hadoop/hive/metastore/model/MPartitionColumnStatistics.java,211,  public long getLongHighValue() {
metastore/src/model/org/apache/hadoop/hive/metastore/model/MPartitionColumnStatistics.java,219,  public double getDoubleLowValue() {
metastore/src/model/org/apache/hadoop/hive/metastore/model/MPartitionColumnStatistics.java,227,  public double getDoubleHighValue() {
metastore/src/model/org/apache/hadoop/hive/metastore/model/MTableColumnStatistics.java,41,  private long longLowValue;
metastore/src/model/org/apache/hadoop/hive/metastore/model/MTableColumnStatistics.java,42,  private long longHighValue;
metastore/src/model/org/apache/hadoop/hive/metastore/model/MTableColumnStatistics.java,43,  private double doubleLowValue;
metastore/src/model/org/apache/hadoop/hive/metastore/model/MTableColumnStatistics.java,44,  private double doubleHighValue;
metastore/src/model/org/apache/hadoop/hive/metastore/model/MTableColumnStatistics.java,159,"  public void setLongStats(long numNulls, long numNDVs, long lowValue, long highValue) {"
metastore/src/model/org/apache/hadoop/hive/metastore/model/MTableColumnStatistics.java,166,"  public void setDoubleStats(long numNulls, long numNDVs, double lowValue, double highValue) {"
metastore/src/model/org/apache/hadoop/hive/metastore/model/MTableColumnStatistics.java,194,  public long getLongLowValue() {
metastore/src/model/org/apache/hadoop/hive/metastore/model/MTableColumnStatistics.java,202,  public long getLongHighValue() {
metastore/src/model/org/apache/hadoop/hive/metastore/model/MTableColumnStatistics.java,210,  public double getDoubleLowValue() {
metastore/src/model/org/apache/hadoop/hive/metastore/model/MTableColumnStatistics.java,218,  public double getDoubleHighValue() {
ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsTask.java,23,import java.math.BigDecimal;
ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsTask.java,24,import java.math.BigInteger;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,345,                  oneRow.add(statData.getLongStats().getHighValue());
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,350,                  oneRow.add(statData.getDoubleStats().getHighValue());
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,365,                  long maxVal = Long.MIN_VALUE;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,374,                    long curVal = statData.getLongStats().getHighValue();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,375,"                    maxVal = Math.max(maxVal, curVal);"
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,383,                  double maxVal = Double.MIN_VALUE;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,393,"                    maxVal = Math.max(maxVal, curVal);"
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,421,                  oneRow.add(statData.getLongStats().getLowValue());
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,426,                  oneRow.add(statData.getDoubleStats().getLowValue());
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,439,                  long minVal = Long.MAX_VALUE;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,448,                    long curVal = statData.getLongStats().getLowValue();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,449,"                    minVal = Math.min(minVal, curVal);"
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,457,                  double minVal = Double.MAX_VALUE;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,467,"                    minVal = Math.min(minVal, curVal);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,20,import java.math.BigDecimal;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,33,import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,302,  /**
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,303,   * GenericUDAFLongStatsEvaluator.
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,304,   *
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,305,   */
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,306,  public static class GenericUDAFLongStatsEvaluator extends GenericUDAFEvaluator {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,310,    private transient PrimitiveObjectInspector inputOI;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,311,    private transient PrimitiveObjectInspector numVectorsOI;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,312,    private final static int MAX_BIT_VECTORS = 1024;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,314,    /* Partial aggregation result returned by TerminatePartial. Partial result is a struct
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,315,"     * containing a long field named ""count""."
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,316,     */
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,317,    private transient Object[] partialResult;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,322,    private transient StructObjectInspector soi;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,324,    private transient StructField minField;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,325,    private transient WritableLongObjectInspector minFieldOI;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,327,    private transient StructField maxField;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,328,    private transient WritableLongObjectInspector maxFieldOI;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,330,    private transient StructField countNullsField;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,331,    private transient WritableLongObjectInspector countNullsFieldOI;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,333,    private transient StructField ndvField;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,334,    private transient WritableStringObjectInspector ndvFieldOI;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,336,    private transient StructField numBitVectorsField;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,337,    private transient WritableIntObjectInspector numBitVectorsFieldOI;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,341,    private transient Object[] result;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,355,        minFieldOI = (WritableLongObjectInspector) minField.getFieldObjectInspector();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,358,        maxFieldOI = (WritableLongObjectInspector) maxField.getFieldObjectInspector();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,368,                                  numBitVectorsField.getFieldObjectInspector();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,375,        foi.add(PrimitiveObjectInspectorFactory.writableLongObjectInspector);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,376,        foi.add(PrimitiveObjectInspectorFactory.writableLongObjectInspector);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,391,        partialResult[1] = new LongWritable(0);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,392,        partialResult[2] = new LongWritable(0);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,398,          foi);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,402,        foi.add(PrimitiveObjectInspectorFactory.writableLongObjectInspector);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,403,        foi.add(PrimitiveObjectInspectorFactory.writableLongObjectInspector);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,416,        result[1] = new LongWritable(0);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,417,        result[2] = new LongWritable(0);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,427,    @AggregationType(estimable = true)
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,428,    public static class LongStatsAgg extends AbstractAggregationBuffer {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,430,      public long min;                              /* Minimum value seen so far */
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,431,      public long max;                              /* Maximum value seen so far */
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,432,      public long countNulls;      /* Count of number of null values seen so far */
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,433,      public LongNumDistinctValueEstimator numDV;    /* Distinct value estimator */
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,434,      public boolean firstItem;                     /* First item in the aggBuf? */
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,435,      public int numBitVectors;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,439,        return model.primitive1() * 2 + model.primitive2() * 3 +
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,440,            model.lengthFor(columnType) + model.lengthFor(numDV);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,442,    };
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,444,    @Override
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,445,    public AggregationBuffer getNewAggregationBuffer() throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,446,      LongStatsAgg result = new LongStatsAgg();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,447,      reset(result);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,448,      return result;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,449,    }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,450,"    public void initNDVEstimator(LongStatsAgg aggBuffer, int numBitVectors) {"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,451,      aggBuffer.numDV = new LongNumDistinctValueEstimator(numBitVectors);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,452,      aggBuffer.numDV.reset();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,453,    }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,455,    @Override
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,456,    public void reset(AggregationBuffer agg) throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,457,      LongStatsAgg myagg = (LongStatsAgg) agg;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,458,"      myagg.columnType = new String(""Long"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,459,      myagg.min = 0;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,460,      myagg.max = 0;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,461,      myagg.countNulls = 0;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,462,      myagg.firstItem = true;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,463,    }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,465,    boolean warned = false;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,467,    @Override
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,468,"    public void iterate(AggregationBuffer agg, Object[] parameters) throws HiveException {"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,469,      Object p = parameters[0];
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,470,      LongStatsAgg myagg = (LongStatsAgg) agg;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,471,      boolean emptyTable = false;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,473,      if (parameters[1] == null) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,474,        emptyTable = true;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,477,      if (myagg.firstItem) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,478,        int numVectors = 0;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,479,        if (!emptyTable) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,480,"          numVectors = PrimitiveObjectInspectorUtils.getInt(parameters[1], numVectorsOI);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,481,        }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,484,"            "" is "" + MAX_BIT_VECTORS + "", but was passed "" + numVectors + "" bit vectors"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,486,"        initNDVEstimator(myagg, numVectors);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,487,        myagg.firstItem = false;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,488,        myagg.numBitVectors = numVectors;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,491,      if (!emptyTable) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,492,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,494,      if (p == null) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,496,      }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,497,      else {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,499,"          long v = PrimitiveObjectInspectorUtils.getLong(p, inputOI);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,500,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,501,          //Update min counter if new value is less than min seen so far
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,502,          if (v < myagg.min) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,503,            myagg.min = v;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,504,          }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,505,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,506,          //Update max counter if new value is greater than max seen so far
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,507,          if (v > myagg.max) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,508,            myagg.max = v;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,509,          }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,510,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,511,          // Add value to NumDistinctValue Estimator
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,512,          myagg.numDV.addToEstimator(v);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,513,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,524,      }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,529,      LongStatsAgg myagg = (LongStatsAgg) agg;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,530,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,531,      // Serialize numDistinctValue Estimator
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,532,      Text t = myagg.numDV.serialize();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,533,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,534,      // Serialize rest of the field in the AggBuffer
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,535,      ((Text) partialResult[0]).set(myagg.columnType);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,536,      ((LongWritable) partialResult[1]).set(myagg.min);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,537,      ((LongWritable) partialResult[2]).set(myagg.max);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,538,      ((LongWritable) partialResult[3]).set(myagg.countNulls);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,539,      ((Text) partialResult[4]).set(t);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,540,      ((IntWritable) partialResult[5]).set(myagg.numDV.getnumBitVectors());
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,542,      return partialResult;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,548,        LongStatsAgg myagg = (LongStatsAgg) agg;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,550,        if (myagg.firstItem) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,553,"          initNDVEstimator(myagg, numVectors);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,554,          myagg.firstItem = false;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,555,          myagg.numBitVectors = numVectors;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,556,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,560,"        Object partialValue = soi.getStructFieldData(partial, minField);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,561,        if (myagg.min > minFieldOI.get(partialValue)) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,562,          myagg.min = minFieldOI.get(partialValue);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,563,        }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,566,"        partialValue = soi.getStructFieldData(partial, maxField);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,567,        if (myagg.max < maxFieldOI.get(partialValue)) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,568,          myagg.max = maxFieldOI.get(partialValue);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,569,        }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,572,"        partialValue = soi.getStructFieldData(partial, countNullsField);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,573,        myagg.countNulls += countNullsFieldOI.get(partialValue);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,576,"        partialValue = soi.getStructFieldData(partial, ndvField);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,577,        String v = ndvFieldOI.getPrimitiveJavaObject(partialValue);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,578,"        NumDistinctValueEstimator o = new NumDistinctValueEstimator(v, myagg.numBitVectors);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,582,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,583,    @Override
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,584,    public Object terminate(AggregationBuffer agg) throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,585,      LongStatsAgg myagg = (LongStatsAgg) agg;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,586,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,587,      long numDV = 0;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,588,      if (myagg.numBitVectors != 0) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,589,        numDV = myagg.numDV.estimateNumDistinctValues();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,590,      }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,591,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,592,      // Serialize the result struct
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,593,      ((Text) result[0]).set(myagg.columnType);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,594,      ((LongWritable) result[1]).set(myagg.min);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,595,      ((LongWritable) result[2]).set(myagg.max);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,596,      ((LongWritable) result[3]).set(myagg.countNulls);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,597,      ((LongWritable) result[4]).set(numDV);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,598,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,599,      return result;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,600,    }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,604,   * GenericUDAFDoubleStatsEvaluator.
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,607,  public static class GenericUDAFDoubleStatsEvaluator extends GenericUDAFEvaluator {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,608,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,609,    /* Object Inspector corresponding to the input parameter.
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,610,     */
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,611,    private transient PrimitiveObjectInspector inputOI;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,612,    private transient PrimitiveObjectInspector numVectorsOI;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,613,    private final static int MAX_BIT_VECTORS = 1024;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,614,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,615,    /* Partial aggregation result returned by TerminatePartial. Partial result is a struct
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,616,"     * containing a long field named ""count""."
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,617,     */
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,618,    private transient Object[] partialResult;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,619,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,620,    /* Object Inspectors corresponding to the struct returned by TerminatePartial and the long
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,621,"     * field within the struct - ""count"""
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,622,     */
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,623,    private transient StructObjectInspector soi;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,624,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,625,    private transient StructField minField;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,626,    private transient WritableDoubleObjectInspector minFieldOI;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,627,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,628,    private transient StructField maxField;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,629,    private transient WritableDoubleObjectInspector maxFieldOI;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,630,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,631,    private transient StructField countNullsField;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,632,    private transient WritableLongObjectInspector countNullsFieldOI;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,633,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,634,    private transient StructField ndvField;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,635,    private transient WritableStringObjectInspector ndvFieldOI;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,636,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,637,    private transient StructField numBitVectorsField;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,638,    private transient WritableIntObjectInspector numBitVectorsFieldOI;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,639,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,640,    /* Output of final result of the aggregation
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,641,     */
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,642,    private transient Object[] result;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,645,"    public ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveException {"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,646,"      super.init(m, parameters);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,647,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,648,      // initialize input
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,649,      if (mode == Mode.PARTIAL1 || mode == Mode.COMPLETE) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,650,        inputOI = (PrimitiveObjectInspector) parameters[0];
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,651,        numVectorsOI = (PrimitiveObjectInspector) parameters[1];
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,652,      } else {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,653,        soi = (StructObjectInspector) parameters[0];
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,654,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,655,"        minField = soi.getStructFieldRef(""Min"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,656,        minFieldOI = (WritableDoubleObjectInspector) minField.getFieldObjectInspector();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,657,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,658,"        maxField = soi.getStructFieldRef(""Max"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,659,        maxFieldOI = (WritableDoubleObjectInspector) maxField.getFieldObjectInspector();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,660,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,661,"        countNullsField = soi.getStructFieldRef(""CountNulls"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,662,        countNullsFieldOI = (WritableLongObjectInspector) countNullsField.getFieldObjectInspector();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,663,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,664,"        ndvField = soi.getStructFieldRef(""BitVector"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,665,        ndvFieldOI = (WritableStringObjectInspector) ndvField.getFieldObjectInspector();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,666,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,667,"        numBitVectorsField = soi.getStructFieldRef(""NumBitVectors"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,668,        numBitVectorsFieldOI = (WritableIntObjectInspector)
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,669,                                  numBitVectorsField.getFieldObjectInspector();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,670,      }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,671,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,672,      // initialize output
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,673,      if (mode == Mode.PARTIAL1 || mode == Mode.PARTIAL2) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,674,        List<ObjectInspector> foi = new ArrayList<ObjectInspector>();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,675,        foi.add(PrimitiveObjectInspectorFactory.writableStringObjectInspector);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,676,        foi.add(PrimitiveObjectInspectorFactory.writableDoubleObjectInspector);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,677,        foi.add(PrimitiveObjectInspectorFactory.writableDoubleObjectInspector);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,678,        foi.add(PrimitiveObjectInspectorFactory.writableLongObjectInspector);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,679,        foi.add(PrimitiveObjectInspectorFactory.writableStringObjectInspector);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,680,        foi.add(PrimitiveObjectInspectorFactory.writableIntObjectInspector);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,681,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,682,        List<String> fname = new ArrayList<String>();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,683,"        fname.add(""ColumnType"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,684,"        fname.add(""Min"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,685,"        fname.add(""Max"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,686,"        fname.add(""CountNulls"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,687,"        fname.add(""BitVector"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,688,"        fname.add(""NumBitVectors"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,689,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,690,        partialResult = new Object[6];
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,691,        partialResult[0] = new Text();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,692,        partialResult[1] = new DoubleWritable(0);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,693,        partialResult[2] = new DoubleWritable(0);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,694,        partialResult[3] = new LongWritable(0);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,695,        partialResult[4] = new Text();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,696,        partialResult[5] = new IntWritable(0);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,697,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,698,"        return ObjectInspectorFactory.getStandardStructObjectInspector(fname,"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,699,          foi);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,700,      } else {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,701,        List<ObjectInspector> foi = new ArrayList<ObjectInspector>();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,702,        foi.add(PrimitiveObjectInspectorFactory.writableStringObjectInspector);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,703,        foi.add(PrimitiveObjectInspectorFactory.writableDoubleObjectInspector);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,704,        foi.add(PrimitiveObjectInspectorFactory.writableDoubleObjectInspector);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,705,        foi.add(PrimitiveObjectInspectorFactory.writableLongObjectInspector);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,706,        foi.add(PrimitiveObjectInspectorFactory.writableLongObjectInspector);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,707,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,708,        List<String> fname = new ArrayList<String>();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,709,"        fname.add(""ColumnType"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,710,"        fname.add(""Min"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,711,"        fname.add(""Max"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,712,"        fname.add(""CountNulls"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,713,"        fname.add(""NumDistinctValues"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,714,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,715,        result = new Object[5];
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,716,        result[0] = new Text();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,717,        result[1] = new DoubleWritable(0);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,718,        result[2] = new DoubleWritable(0);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,719,        result[3] = new LongWritable(0);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,720,        result[4] = new LongWritable(0);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,721,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,722,"        return ObjectInspectorFactory.getStandardStructObjectInspector(fname,"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,723,            foi);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,724,      }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,728,    public static class DoubleStatsAgg extends AbstractAggregationBuffer {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,729,      public String columnType;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,730,      public double min;                            /* Minimum value seen so far */
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,731,      public double max;                            /* Maximum value seen so far */
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,732,      public long countNulls;      /* Count of number of null values seen so far */
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,733,      public DoubleNumDistinctValueEstimator numDV;  /* Distinct value estimator */
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,734,      public boolean firstItem;                     /* First item in the aggBuf? */
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,735,      public int numBitVectors;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,739,        return model.primitive1() * 2 + model.primitive2() * 3 +
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,740,            model.lengthFor(columnType) + model.lengthFor(numDV);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,742,    };
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,744,    @Override
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,745,    public AggregationBuffer getNewAggregationBuffer() throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,746,      DoubleStatsAgg result = new DoubleStatsAgg();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,747,      reset(result);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,748,      return result;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,749,    }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,750,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,751,"    public void initNDVEstimator(DoubleStatsAgg aggBuffer, int numBitVectors) {"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,752,      aggBuffer.numDV = new DoubleNumDistinctValueEstimator(numBitVectors);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,753,      aggBuffer.numDV.reset();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,754,    }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,755,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,756,    @Override
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,757,    public void reset(AggregationBuffer agg) throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,758,      DoubleStatsAgg myagg = (DoubleStatsAgg) agg;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,759,"      myagg.columnType = new String(""Double"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,760,      myagg.min = 0.0;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,761,      myagg.max = 0.0;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,762,      myagg.countNulls = 0;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,763,      myagg.firstItem = true;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,764,    }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,765,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,766,    boolean warned = false;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,767,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,768,    @Override
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,769,"    public void iterate(AggregationBuffer agg, Object[] parameters) throws HiveException {"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,770,      Object p = parameters[0];
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,771,      DoubleStatsAgg myagg = (DoubleStatsAgg) agg;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,772,      boolean emptyTable = false;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,773,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,774,      if (parameters[1] == null) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,775,        emptyTable = true;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,776,      }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,777,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,778,      if (myagg.firstItem) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,779,        int numVectors = 0;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,780,        if (!emptyTable) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,781,"          numVectors = PrimitiveObjectInspectorUtils.getInt(parameters[1], numVectorsOI);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,783,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,784,        if (numVectors > MAX_BIT_VECTORS) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,785,"          throw new HiveException(""The maximum allowed value for number of bit vectors "" +"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,786,"            "" is "" + MAX_BIT_VECTORS + "", but was passed "" + numVectors + "" bit vectors"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,788,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,789,"        initNDVEstimator(myagg, numVectors);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,790,        myagg.firstItem = false;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,791,        myagg.numBitVectors = numVectors;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,794,      if (!emptyTable) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,795,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,796,        //Update null counter if a null value is seen
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,797,        if (p == null) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,798,          myagg.countNulls++;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,800,        else {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,801,          try {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,802,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,803,"            double v = PrimitiveObjectInspectorUtils.getDouble(p, inputOI);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,804,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,805,            //Update min counter if new value is less than min seen so far
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,806,            if (v < myagg.min) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,807,              myagg.min = v;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,808,            }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,809,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,810,            //Update max counter if new value is greater than max seen so far
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,811,            if (v > myagg.max) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,812,              myagg.max = v;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,813,            }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,814,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,815,            // Add value to NumDistinctValue Estimator
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,816,            myagg.numDV.addToEstimator(v);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,818,          } catch (NumberFormatException e) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,819,            if (!warned) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,820,              warned = true;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,821,"              LOG.warn(getClass().getSimpleName() + "" """
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,822,                  + StringUtils.stringifyException(e));
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,823,              LOG.warn(getClass().getSimpleName()
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,824,"                  + "" ignoring similar exceptions."");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,825,            }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,826,          }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,829,    }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,832,    public Object terminatePartial(AggregationBuffer agg) throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,833,      DoubleStatsAgg myagg = (DoubleStatsAgg) agg;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,834,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,835,      // Serialize numDistinctValue Estimator
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,836,      Text t = myagg.numDV.serialize();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,837,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,838,      // Serialize the rest of the values in the AggBuffer
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,839,      ((Text) partialResult[0]).set(myagg.columnType);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,840,      ((DoubleWritable) partialResult[1]).set(myagg.min);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,841,      ((DoubleWritable) partialResult[2]).set(myagg.max);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,842,      ((LongWritable) partialResult[3]).set(myagg.countNulls);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,843,      ((Text) partialResult[4]).set(t);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,844,      ((IntWritable) partialResult[5]).set(myagg.numBitVectors);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,845,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,846,      return partialResult;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,850,"    public void merge(AggregationBuffer agg, Object partial) throws HiveException {"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,851,      if (partial != null) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,852,        DoubleStatsAgg myagg = (DoubleStatsAgg) agg;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,853,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,854,        if (myagg.firstItem) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,855,"          Object partialValue = soi.getStructFieldData(partial, numBitVectorsField);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,856,          int numVectors = numBitVectorsFieldOI.get(partialValue);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,857,"          initNDVEstimator(myagg, numVectors);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,858,          myagg.firstItem = false;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,859,          myagg.numBitVectors = numVectors;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,860,        }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,861,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,862,        // Update min if min is lesser than the smallest value seen so far
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,863,"        Object partialValue = soi.getStructFieldData(partial, minField);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,864,        if (myagg.min > minFieldOI.get(partialValue)) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,865,          myagg.min = minFieldOI.get(partialValue);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,866,        }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,867,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,868,        // Update max if max is greater than the largest value seen so far
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,869,"        partialValue = soi.getStructFieldData(partial, maxField);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,870,        if (myagg.max < maxFieldOI.get(partialValue)) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,871,          myagg.max = maxFieldOI.get(partialValue);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,872,        }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,874,        // Update the null counter
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,875,"        partialValue = soi.getStructFieldData(partial, countNullsField);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,876,        myagg.countNulls += countNullsFieldOI.get(partialValue);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,878,        // Merge numDistinctValue Estimators
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,879,"        partialValue = soi.getStructFieldData(partial, ndvField);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,880,        String v = ndvFieldOI.getPrimitiveJavaObject(partialValue);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,882,"        NumDistinctValueEstimator o = new NumDistinctValueEstimator(v, myagg.numBitVectors);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,883,        myagg.numDV.mergeEstimators(o);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,885,    }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,887,    @Override
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,888,    public Object terminate(AggregationBuffer agg) throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,889,      DoubleStatsAgg myagg = (DoubleStatsAgg) agg;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,890,      long numDV = 0;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,892,      if (myagg.numBitVectors != 0) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,893,        numDV = myagg.numDV.estimateNumDistinctValues();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,896,      // Serialize the result struct
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,897,      ((Text) result[0]).set(myagg.columnType);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,898,      ((DoubleWritable) result[1]).set(myagg.min);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,899,      ((DoubleWritable) result[2]).set(myagg.max);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,900,      ((LongWritable) result[3]).set(myagg.countNulls);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,901,      ((LongWritable) result[4]).set(numDV);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1478,  public static class GenericUDAFDecimalStatsEvaluator extends GenericUDAFEvaluator {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1479,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1480,    /*
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1481,     * Object Inspector corresponding to the input parameter.
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1482,     */
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1483,    private transient PrimitiveObjectInspector inputOI;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1484,    private transient PrimitiveObjectInspector numVectorsOI;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1485,    private final static int MAX_BIT_VECTORS = 1024;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1486,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1487,    /* Partial aggregation result returned by TerminatePartial. Partial result is a struct
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1488,"     * containing a long field named ""count""."
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1489,     */
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1490,    private transient Object[] partialResult;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1491,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1492,    /* Object Inspectors corresponding to the struct returned by TerminatePartial and the long
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1493,"     * field within the struct - ""count"""
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1494,     */
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1495,    private transient StructObjectInspector soi;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1496,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1497,    private transient StructField minField;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1498,    private transient WritableHiveDecimalObjectInspector minFieldOI;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1499,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1500,    private transient StructField maxField;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1501,    private transient WritableHiveDecimalObjectInspector maxFieldOI;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1502,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1503,    private transient StructField countNullsField;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1504,    private transient WritableLongObjectInspector countNullsFieldOI;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1505,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1506,    private transient StructField ndvField;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1507,    private transient WritableStringObjectInspector ndvFieldOI;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1508,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1509,    private transient StructField numBitVectorsField;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1510,    private transient WritableIntObjectInspector numBitVectorsFieldOI;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1511,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1512,    /* Output of final result of the aggregation
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1513,     */
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1514,    private transient Object[] result;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1515,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1516,    private boolean warned = false;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1519,"    public ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveException {"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1520,"      super.init(m, parameters);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1521,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1522,      // initialize input
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1523,      if (mode == Mode.PARTIAL1 || mode == Mode.COMPLETE) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1524,        inputOI = (PrimitiveObjectInspector) parameters[0];
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1525,        numVectorsOI = (PrimitiveObjectInspector) parameters[1];
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1526,      } else {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1527,        soi = (StructObjectInspector) parameters[0];
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1528,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1529,"        minField = soi.getStructFieldRef(""Min"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1530,        minFieldOI = (WritableHiveDecimalObjectInspector) minField.getFieldObjectInspector();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1531,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1532,"        maxField = soi.getStructFieldRef(""Max"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1533,        maxFieldOI = (WritableHiveDecimalObjectInspector) maxField.getFieldObjectInspector();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1534,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1535,"        countNullsField = soi.getStructFieldRef(""CountNulls"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1536,        countNullsFieldOI = (WritableLongObjectInspector) countNullsField.getFieldObjectInspector();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1537,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1538,"        ndvField = soi.getStructFieldRef(""BitVector"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1539,        ndvFieldOI = (WritableStringObjectInspector) ndvField.getFieldObjectInspector();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1540,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1541,"        numBitVectorsField = soi.getStructFieldRef(""NumBitVectors"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1542,        numBitVectorsFieldOI = (WritableIntObjectInspector)
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1543,            numBitVectorsField.getFieldObjectInspector();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1544,      }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1545,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1546,      // initialize output
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1547,      if (mode == Mode.PARTIAL1 || mode == Mode.PARTIAL2) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1548,        List<ObjectInspector> foi = new ArrayList<ObjectInspector>();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1549,        foi.add(PrimitiveObjectInspectorFactory.writableStringObjectInspector);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1550,        foi.add(PrimitiveObjectInspectorFactory.writableHiveDecimalObjectInspector);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1551,        foi.add(PrimitiveObjectInspectorFactory.writableHiveDecimalObjectInspector);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1552,        foi.add(PrimitiveObjectInspectorFactory.writableLongObjectInspector);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1553,        foi.add(PrimitiveObjectInspectorFactory.writableStringObjectInspector);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1554,        foi.add(PrimitiveObjectInspectorFactory.writableIntObjectInspector);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1555,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1556,        List<String> fname = new ArrayList<String>();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1557,"        fname.add(""ColumnType"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1558,"        fname.add(""Min"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1559,"        fname.add(""Max"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1560,"        fname.add(""CountNulls"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1561,"        fname.add(""BitVector"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1562,"        fname.add(""NumBitVectors"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1563,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1564,        partialResult = new Object[6];
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1565,        partialResult[0] = new Text();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1566,        partialResult[1] = new HiveDecimalWritable(HiveDecimal.create(0));
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1567,        partialResult[2] = new HiveDecimalWritable(HiveDecimal.create(0));
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1568,        partialResult[3] = new LongWritable(0);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1569,        partialResult[4] = new Text();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1570,        partialResult[5] = new IntWritable(0);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1571,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1572,"        return ObjectInspectorFactory.getStandardStructObjectInspector(fname,"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1573,            foi);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1574,      } else {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1575,        List<ObjectInspector> foi = new ArrayList<ObjectInspector>();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1576,        foi.add(PrimitiveObjectInspectorFactory.writableStringObjectInspector);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1577,        foi.add(PrimitiveObjectInspectorFactory.writableHiveDecimalObjectInspector);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1578,        foi.add(PrimitiveObjectInspectorFactory.writableHiveDecimalObjectInspector);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1579,        foi.add(PrimitiveObjectInspectorFactory.writableLongObjectInspector);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1580,        foi.add(PrimitiveObjectInspectorFactory.writableLongObjectInspector);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1581,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1582,        List<String> fname = new ArrayList<String>();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1583,"        fname.add(""ColumnType"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1584,"        fname.add(""Min"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1585,"        fname.add(""Max"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1586,"        fname.add(""CountNulls"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1587,"        fname.add(""NumDistinctValues"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1588,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1589,        result = new Object[5];
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1590,        result[0] = new Text();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1591,        result[1] = new HiveDecimalWritable(HiveDecimal.create(0));
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1592,        result[2] = new HiveDecimalWritable(HiveDecimal.create(0));
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1593,        result[3] = new LongWritable(0);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1594,        result[4] = new LongWritable(0);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1595,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1596,"        return ObjectInspectorFactory.getStandardStructObjectInspector(fname,"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1597,            foi);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1598,      }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1602,    public static class DecimalStatsAgg extends AbstractAggregationBuffer {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1603,      public String columnType;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1604,      public HiveDecimal min;                            /* Minimum value seen so far */
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1605,      public HiveDecimal max;                            /* Maximum value seen so far */
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1606,      public long countNulls;      /* Count of number of null values seen so far */
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1607,      public DecimalNumDistinctValueEstimator numDV;  /* Distinct value estimator */
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1608,      public boolean firstItem;                     /* First item in the aggBuf? */
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1609,      public int numBitVectors;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1613,        return model.primitive1() * 2 + model.primitive2() + model.lengthOfDecimal() * 2 +
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1614,            model.lengthFor(columnType) + model.lengthFor(numDV);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1615,      }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1616,    };
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1617,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1618,    @Override
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1619,    public AggregationBuffer getNewAggregationBuffer() throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1620,      DecimalStatsAgg result = new DecimalStatsAgg();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1621,      reset(result);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1622,      return result;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1623,    }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1624,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1625,"    public void initNDVEstimator(DecimalStatsAgg aggBuffer, int numBitVectors) {"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1626,      aggBuffer.numDV = new DecimalNumDistinctValueEstimator(numBitVectors);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1627,      aggBuffer.numDV.reset();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1628,    }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1629,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1630,    @Override
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1631,    public void reset(AggregationBuffer agg) throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1632,      DecimalStatsAgg myagg = (DecimalStatsAgg) agg;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1633,"      myagg.columnType = new String(""Decimal"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1634,      myagg.min = HiveDecimal.create(0);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1635,      myagg.max = HiveDecimal.create(0);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1636,      myagg.countNulls = 0;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1637,      myagg.firstItem = true;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1638,    }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1639,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1640,    @Override
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1641,"    public void iterate(AggregationBuffer agg, Object[] parameters) throws HiveException {"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1642,      Object p = parameters[0];
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1643,      DecimalStatsAgg myagg = (DecimalStatsAgg) agg;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1644,      boolean emptyTable = false;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1645,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1646,      if (parameters[1] == null) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1647,        emptyTable = true;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1650,      if (myagg.firstItem) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1651,        int numVectors = 0;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1652,        if (!emptyTable) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1653,"          numVectors = PrimitiveObjectInspectorUtils.getInt(parameters[1], numVectorsOI);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1655,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1656,        if (numVectors > MAX_BIT_VECTORS) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1657,"          throw new HiveException(""The maximum allowed value for number of bit vectors "" +"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1658,"              "" is "" + MAX_BIT_VECTORS + "", but was passed "" + numVectors + "" bit vectors"");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1660,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1661,"        initNDVEstimator(myagg, numVectors);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1662,        myagg.firstItem = false;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1663,        myagg.numBitVectors = numVectors;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1666,      if (!emptyTable) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1667,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1668,        //Update null counter if a null value is seen
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1669,        if (p == null) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1670,          myagg.countNulls++;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1672,        else {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1673,          try {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1674,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1675,"            HiveDecimal v = PrimitiveObjectInspectorUtils.getHiveDecimal(p, inputOI);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1676,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1677,            //Update min counter if new value is less than min seen so far
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1678,            if (v.compareTo(myagg.min) < 0) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1679,              myagg.min = v;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1680,            }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1681,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1682,            //Update max counter if new value is greater than max seen so far
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1683,            if (v.compareTo(myagg.max) > 0) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1684,              myagg.max = v;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1685,            }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1686,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1687,            // Add value to NumDistinctValue Estimator
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1688,            myagg.numDV.addToEstimator(v);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1690,          } catch (NumberFormatException e) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1691,            if (!warned) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1692,              warned = true;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1693,"              LOG.warn(getClass().getSimpleName() + "" """
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1694,                  + StringUtils.stringifyException(e));
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1695,              LOG.warn(getClass().getSimpleName()
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1696,"                  + "" ignoring similar exceptions."");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1697,            }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1698,          }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1701,    }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1702,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1703,    @Override
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1704,    public Object terminatePartial(AggregationBuffer agg) throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1705,      DecimalStatsAgg myagg = (DecimalStatsAgg) agg;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1706,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1707,      // Serialize numDistinctValue Estimator
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1708,      Text t = myagg.numDV.serialize();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1709,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1710,      // Serialize the rest of the values in the AggBuffer
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1711,      ((Text) partialResult[0]).set(myagg.columnType);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1712,      ((HiveDecimalWritable) partialResult[1]).set(myagg.min);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1713,      ((HiveDecimalWritable) partialResult[2]).set(myagg.max);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1714,      ((LongWritable) partialResult[3]).set(myagg.countNulls);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1715,      ((Text) partialResult[4]).set(t);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1716,      ((IntWritable) partialResult[5]).set(myagg.numBitVectors);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1717,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1718,      return partialResult;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1719,    }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1722,"    public void merge(AggregationBuffer agg, Object partial) throws HiveException {"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1723,      if (partial != null) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1724,        DecimalStatsAgg myagg = (DecimalStatsAgg) agg;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1725,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1726,        if (myagg.firstItem) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1727,"          Object partialValue = soi.getStructFieldData(partial, numBitVectorsField);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1728,          int numVectors = numBitVectorsFieldOI.get(partialValue);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1729,"          initNDVEstimator(myagg, numVectors);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1730,          myagg.firstItem = false;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1731,          myagg.numBitVectors = numVectors;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1732,        }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1733,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1734,        // Update min if min is lesser than the smallest value seen so far
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1735,"        Object partialValue = soi.getStructFieldData(partial, minField);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1736,        if (myagg.min.compareTo(minFieldOI.getPrimitiveJavaObject(partialValue)) > 0) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1737,          myagg.min = minFieldOI.getPrimitiveJavaObject(partialValue);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1738,        }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1739,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1740,        // Update max if max is greater than the largest value seen so far
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1741,"        partialValue = soi.getStructFieldData(partial, maxField);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1742,        if (myagg.max.compareTo(maxFieldOI.getPrimitiveJavaObject(partialValue)) < 0) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1743,          myagg.max = maxFieldOI.getPrimitiveJavaObject(partialValue);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1744,        }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1745,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1746,        // Update the null counter
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1747,"        partialValue = soi.getStructFieldData(partial, countNullsField);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1748,        myagg.countNulls += countNullsFieldOI.get(partialValue);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1749,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1750,        // Merge numDistinctValue Estimators
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1751,"        partialValue = soi.getStructFieldData(partial, ndvField);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1752,        String v = ndvFieldOI.getPrimitiveJavaObject(partialValue);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1753,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1754,"        NumDistinctValueEstimator o = new NumDistinctValueEstimator(v, myagg.numBitVectors);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1755,        myagg.numDV.mergeEstimators(o);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1756,      }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1760,    public Object terminate(AggregationBuffer agg) throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1761,      DecimalStatsAgg myagg = (DecimalStatsAgg) agg;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1762,      long numDV = 0;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1763,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1764,      if (myagg.numBitVectors != 0) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1765,        numDV = myagg.numDV.estimateNumDistinctValues();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1766,      }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1767,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1768,      // Serialize the result struct
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1769,      ((Text) result[0]).set(myagg.columnType);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1770,      ((HiveDecimalWritable) result[1]).set(myagg.min);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1771,      ((HiveDecimalWritable) result[2]).set(myagg.max);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1772,      ((LongWritable) result[3]).set(myagg.countNulls);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1773,      ((LongWritable) result[4]).set(numDV);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1774,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1775,      return result;
hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/schema/HCatSchema.java,61,      String fieldName = field.getName();
hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/schema/HCatSchema.java,63,"        throw new IllegalArgumentException(""Field named "" + fieldName +"
hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/schema/HCatSchema.java,75,    String fieldName = hfs.getName();
hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/schema/HCatSchema.java,101,    return fieldPositionMap.get(fieldName);
hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/schema/HCatSchema.java,137,"    reAlignPositionMap(fieldPositionMap.get(hcatFieldSchema.getName())+1, -1);"
hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/schema/HCatSchema.java,138,    fieldPositionMap.remove(hcatFieldSchema.getName());
hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/schema/HCatSchema.java,139,    fieldNames.remove(hcatFieldSchema.getName());
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTranslate.java,153,"            ""A string argument was expected but an argument of type "" + arguments[i].getTypeName()"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTranslate.java,154,"                + "" was given."");"
ql/src/java/org/apache/hadoop/hive/ql/exec/ExprNodeEvaluator.java,52,"   * Return initialized ObjectInspector. If it's not initilized, throws runtime exception"
ql/src/java/org/apache/hadoop/hive/ql/exec/ExprNodeEvaluatorFactory.java,108,      String key = eval.getExpr().getExprString();
ql/src/java/org/apache/hadoop/hive/ql/ppd/OpProcFactory.java,80,import org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction;
ql/src/java/org/apache/hadoop/hive/ql/ppd/OpProcFactory.java,82,import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;
ql/src/java/org/apache/hadoop/hive/ql/ppd/OpProcFactory.java,199,"      ExprWalkerInfo childInfo = getChildWalkerInfo((Operator<?>) ptfOp, owi);"
ql/src/java/org/apache/hadoop/hive/ql/ppd/OpProcFactory.java,415,      Operator<? extends OperatorDesc> op =
ql/src/java/org/apache/hadoop/hive/ql/ppd/OpProcFactory.java,416,        (Operator<? extends OperatorDesc>) nd;
ql/src/java/org/apache/hadoop/hive/ql/ppd/OpProcFactory.java,417,      ExprNodeDesc predicate = (((FilterOperator) nd).getConf()).getPredicate();
ql/src/java/org/apache/hadoop/hive/ql/ppd/OpProcFactory.java,418,      ExprWalkerInfo ewi = new ExprWalkerInfo();
ql/src/java/org/apache/hadoop/hive/ql/ppd/OpProcFactory.java,422,      if (!((FilterOperator)op).getConf().getIsSamplingPred()) {
ql/src/java/org/apache/hadoop/hive/ql/ppd/OpProcFactory.java,1051,    return (ExprNodeGenericFuncDesc)decomposed.residualPredicate;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4396,"        OperatorFactory.getAndMakeChild(PlanUtils.getReduceSinkDesc(reduceKeys,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4397,"            keyLength, reduceValues, distinctColIndices,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4398,"            outputKeyColumnNames, outputValueColumnNames, true, -1, keyLength,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4399,"            -1), new RowSchema(reduceSinkOutputRowResolver"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFToBinary.java,34,"             extended = ""Currently only string or binary can be cast into binary"")"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFToBinary.java,57,"      throw new UDFArgumentException(""Only string or binary data can be cast into binary "" +"
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,358,"          hdfs.setReplication(hdfsFilePath, replication);"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,159,"        if (! equalsFileSystem(srcFs, destFs)) {"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,254,  /**
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,255,   * @param fs1
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,256,   * @param fs2
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,257,   * @return return true if both file system arguments point to same file system
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,258,   */
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,259,"  private boolean equalsFileSystem(FileSystem fs1, FileSystem fs2) {"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,260,"    //When file system cache is disabled, you get different FileSystem objects"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,261,"    // for same file system, so '==' can't be used in such cases"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,262,"    //FileSystem api doesn't have a .equals() function implemented, so using"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,263,    //the uri for comparison. FileSystem already uses uri+Configuration for
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,264,    //equality in its CACHE .
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,265,"    //Once equality has been added in HDFS-4321, we should make use of it"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,266,    return fs1.getUri().equals(fs2.getUri());
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,267,  }
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,268,
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,1245,          if (oldPartPathFS.equals(loadPathFS)) {
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/StorageBasedAuthorizationProvider.java,151,"    // Table path can be null in the case of a new create table - in this case,"
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/StorageBasedAuthorizationProvider.java,152,    // we try to determine what the path would be after the create table is issued.
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/StorageBasedAuthorizationProvider.java,153,    Path path = null;
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/StorageBasedAuthorizationProvider.java,157,      if (location == null || location.isEmpty()) {
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/StorageBasedAuthorizationProvider.java,158,"        path = wh.getTablePath(hive_db.getDatabase(table.getDbName()), table.getTableName());"
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/StorageBasedAuthorizationProvider.java,159,      } else {
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/StorageBasedAuthorizationProvider.java,160,        path = new Path(location);
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/StorageBasedAuthorizationProvider.java,165,
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/StorageBasedAuthorizationProvider.java,166,"    authorize(path, readRequiredPriv, writeRequiredPriv);"
itests/hive-unit/src/main/java/org/apache/hive/jdbc/miniHS2/MiniHS2.java,110,"      return new MiniHS2(hiveConf, useMiniMR, useMiniKdc, serverPrincipal, serverKeytab);"
itests/hive-unit/src/main/java/org/apache/hive/jdbc/miniHS2/MiniHS2.java,142,"  private MiniHS2(HiveConf hiveConf, boolean useMiniMR, boolean useMiniKdc, String serverPrincipal, String serverKeytab) throws Exception {"
itests/hive-unit/src/main/java/org/apache/hive/jdbc/miniHS2/MiniHS2.java,183,    fs.mkdirs(scratchDir);
itests/hive-unit/src/main/java/org/apache/hive/jdbc/miniHS2/MiniHS2.java,185,"    System.setProperty(HiveConf.ConfVars.LOCALSCRATCHDIR.varname,"
itests/hive-unit/src/main/java/org/apache/hive/jdbc/miniHS2/MiniHS2.java,186,"        baseDir.getPath() + File.separator + ""scratch"");"
itests/hive-unit/src/main/java/org/apache/hive/jdbc/miniHS2/MiniHS2.java,194,"    this(hiveConf, useMiniMR, false, null, null);"
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java,1050,"          HiveMetaStore.startMetaStore(port, bridge);"
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,118,      // create a new metastore connection using the delegation token
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,119,      Hive.set(null);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,120,      try {
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,121,        sessionHive = Hive.get(getHiveConf());
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,122,      } catch (HiveException e) {
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,123,"        throw new HiveSQLException(""Failed to setup metastore connection"", e);"
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,124,      }
beeline/src/java/org/apache/hive/beeline/BeeLine.java,60,import java.util.LinkedList;
beeline/src/java/org/apache/hive/beeline/BeeLine.java,621,  boolean initArgs(String[] args) {
beeline/src/java/org/apache/hive/beeline/BeeLine.java,622,    List<String> commands = new LinkedList<String>();
beeline/src/java/org/apache/hive/beeline/BeeLine.java,623,    List<String> files = new LinkedList<String>();
beeline/src/java/org/apache/hive/beeline/BeeLine.java,633,      return false;
beeline/src/java/org/apache/hive/beeline/BeeLine.java,641,"      // Return false here, so usage will be printed."
beeline/src/java/org/apache/hive/beeline/BeeLine.java,642,      return false;
beeline/src/java/org/apache/hive/beeline/BeeLine.java,693,    if (commands.size() > 0) {
beeline/src/java/org/apache/hive/beeline/BeeLine.java,701,        dispatch(command);
beeline/src/java/org/apache/hive/beeline/BeeLine.java,705,    return true;
beeline/src/java/org/apache/hive/beeline/BeeLine.java,723,      if (!initArgs(args)) {
beeline/src/java/org/apache/hive/beeline/BeeLine.java,724,        usage();
beeline/src/java/org/apache/hive/beeline/BeeLine.java,725,        return ERRNO_ARGS;
beeline/src/test/org/apache/hive/beeline/TestBeelineArgParsing.java,59,    Assert.assertTrue(bl.initArgs(args));
beeline/src/test/org/apache/hive/beeline/TestBeelineArgParsing.java,72,    Assert.assertTrue(bl.initArgs(args));
beeline/src/test/org/apache/hive/beeline/TestBeelineArgParsing.java,81,    Assert.assertTrue(bl.initArgs(args));
beeline/src/test/org/apache/hive/beeline/TestBeelineArgParsing.java,96,    Assert.assertTrue(bl.initArgs(args));
beeline/src/test/org/apache/hive/beeline/TestBeelineArgParsing.java,110,    Assert.assertTrue(bl.initArgs(args));
beeline/src/test/org/apache/hive/beeline/TestBeelineArgParsing.java,125,    Assert.assertTrue(bl.initArgs(args));
beeline/src/test/org/apache/hive/beeline/TestBeelineArgParsing.java,137,    Assert.assertFalse(bl.initArgs(args));
beeline/src/test/org/apache/hive/beeline/TestBeelineArgParsing.java,147,    Assert.assertFalse(bl.initArgs(args));
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,1032,"    HIVESAMPLINGFORORDERBY(""hive.optimize.sampling.orderby"", false, """"),"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,1033,"    HIVESAMPLINGNUMBERFORORDERBY(""hive.optimize.sampling.orderby.number"", 1000, """"),"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,1034,"    HIVESAMPLINGPERCENTFORORDERBY(""hive.optimize.sampling.orderby.percent"", 0.1f, """"),"
common/src/java/org/apache/hadoop/hive/conf/Validator.java,150,        if (fvalue <= 0 || fvalue >= 1) {
ql/src/java/org/apache/hadoop/hive/ql/exec/HiveTotalOrderPartitioner.java,29,"public class HiveTotalOrderPartitioner implements Partitioner<HiveKey, Object> {"
ql/src/java/org/apache/hadoop/hive/ql/exec/HiveTotalOrderPartitioner.java,31,"  private Partitioner<BytesWritable, Object> partitioner"
ql/src/java/org/apache/hadoop/hive/ql/exec/HiveTotalOrderPartitioner.java,32,"      = new TotalOrderPartitioner<BytesWritable, Object>();"
ql/src/java/org/apache/hadoop/hive/ql/exec/HiveTotalOrderPartitioner.java,35,    JobConf newconf = new JobConf(job);
ql/src/java/org/apache/hadoop/hive/ql/exec/HiveTotalOrderPartitioner.java,36,    newconf.setMapOutputKeyClass(BytesWritable.class);
ql/src/java/org/apache/hadoop/hive/ql/exec/HiveTotalOrderPartitioner.java,37,    partitioner.configure(newconf);
ql/src/java/org/apache/hadoop/hive/ql/exec/PartitionKeySampler.java,77,  // copied from org.apache.hadoop.mapred.lib.InputSampler
ql/src/java/org/apache/hadoop/hive/ql/exec/PartitionKeySampler.java,84,    byte[][] partitionKeys = new byte[numReduce - 1][];
ql/src/java/org/apache/hadoop/hive/ql/exec/PartitionKeySampler.java,85,    float stepSize = sorted.length / (float) numReduce;
ql/src/java/org/apache/hadoop/hive/ql/exec/PartitionKeySampler.java,86,    int last = -1;
ql/src/java/org/apache/hadoop/hive/ql/exec/PartitionKeySampler.java,87,    for(int i = 1; i < numReduce; ++i) {
ql/src/java/org/apache/hadoop/hive/ql/exec/PartitionKeySampler.java,88,      int k = Math.round(stepSize * i);
ql/src/java/org/apache/hadoop/hive/ql/exec/PartitionKeySampler.java,89,"      while (last >= k && C.compare(sorted[last], sorted[k]) == 0) {"
ql/src/java/org/apache/hadoop/hive/ql/exec/PartitionKeySampler.java,90,        k++;
ql/src/java/org/apache/hadoop/hive/ql/exec/PartitionKeySampler.java,92,      if (k >= sorted.length) {
ql/src/java/org/apache/hadoop/hive/ql/exec/PartitionKeySampler.java,93,"        throw new IllegalStateException(""not enough number of sample"");"
ql/src/java/org/apache/hadoop/hive/ql/exec/PartitionKeySampler.java,95,      partitionKeys[i - 1] = sorted[k];
ql/src/java/org/apache/hadoop/hive/ql/exec/PartitionKeySampler.java,96,      last = k;
ql/src/java/org/apache/hadoop/hive/ql/exec/PartitionKeySampler.java,101,"  public void writePartitionKeys(Path path, JobConf job) throws IOException {"
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,374,      if (mWork.getSamplingType() > 0 && rWork != null && rWork.getNumReduceTasks() > 1) {
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,542,"    sampler.writePartitionKeys(partitionFile, job);"
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,329,        if (ShimLoader.getHadoopShims().isLocalMode(conf)) {
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,349,"  public ColumnStatsSemanticAnalyzer(HiveConf conf, ASTNode tree) throws SemanticException {"
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,350,    super(conf);
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,351,    // check if it is no scan. grammar prevents coexit noscan/columns
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,352,    super.processNoScanCommand(tree);
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,353,    // check if it is partial scan. grammar prevents coexit partialscan/columns
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,354,    super.processPartialScanCommand(tree);
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,355,    /* Rewrite only analyze table <> column <> compute statistics; Don't rewrite analyze table
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,356,     * command - table stats are collected by the table scan operator and is not rewritten to
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,357,     * an aggregation.
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,358,     */
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,359,    if (shouldRewrite(tree)) {
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,360,      tbl = getTable(tree);
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,361,      colNames = getColumnName(tree);
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,362,      // Save away the original AST
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,363,      originalTree = tree;
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,364,      boolean isPartitionStats = isPartitionLevelStats(tree);
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,365,"      Map<String,String> partSpec = null;"
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,366,"      checkForPartitionColumns(colNames, Utilities.getColumnNamesFromFieldSchema(tbl.getPartitionKeys()));"
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,367,      validateSpecifiedColumnNames(colNames);
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,368,      if (conf.getBoolVar(ConfVars.HIVE_STATS_COLLECT_PART_LEVEL_STATS) && tbl.isPartitioned()) {
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,369,        isPartitionStats = true;
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,370,      }
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,371,
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,372,      if (isPartitionStats) {
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,373,        isTableLevel = false;
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,374,        partSpec = getPartKeyValuePairsFromAST(tree);
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,375,        handlePartialPartitionSpec(partSpec);
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,376,      } else {
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,377,        isTableLevel = true;
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,378,      }
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,379,      colType = getColumnTypes(colNames);
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,380,      int numBitVectors = getNumBitVectorsForNDVEstimation(conf);
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,381,"      rewrittenQuery = genRewrittenQuery(colNames, numBitVectors, partSpec, isPartitionStats);"
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,382,      rewrittenTree = genRewrittenTree(rewrittenQuery);
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,383,    } else {
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,384,      // Not an analyze table column compute statistics statement - don't do any rewrites
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,385,      originalTree = rewrittenTree = tree;
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,386,      rewrittenQuery = null;
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,387,      isRewritten = false;
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,388,    }
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,389,  }
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,390,
ql/src/java/org/apache/hadoop/hive/ql/parse/GlobalLimitCtx.java,28,  private boolean enable = false;
ql/src/java/org/apache/hadoop/hive/ql/parse/GlobalLimitCtx.java,29,  private int globalLimit = -1;
ql/src/java/org/apache/hadoop/hive/ql/parse/GlobalLimitCtx.java,30,  private boolean hasTransformOrUDTF = false;
ql/src/java/org/apache/hadoop/hive/ql/parse/GlobalLimitCtx.java,31,  private LimitDesc lastReduceLimitDesc = null;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,231,"   * a map for the split sampling, from ailias to an instance of SplitSample"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,242,  private final GlobalLimitCtx globalLimitCtx = new GlobalLimitCtx();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,253,"  private final Map<String, ReadEntity> viewAliasToInput = new HashMap<String, ReadEntity>();"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,254,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,255,  // Max characters when auto generating the column name with func name
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,256,  private static final int AUTOGEN_COLALIAS_PRFX_MAXLENGTH = 20;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,259,  protected boolean noscan = false;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,262,  protected boolean partialscan = false;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,279,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzerFactory.java,266,"        return new ColumnStatsSemanticAnalyzer(conf, tree);"
beeline/src/java/org/apache/hive/beeline/Commands.java,735,
itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestHs2HooksWithMiniKdc.java,46,    public static String userName = null;
itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestHs2HooksWithMiniKdc.java,47,    public static String ipAddress = null;
itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestHs2HooksWithMiniKdc.java,50,      if (hookContext.getHookType().equals(HookType.POST_EXEC_HOOK)) {
itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestHs2HooksWithMiniKdc.java,51,"        Assert.assertNotNull(hookContext.getIpAddress(), ""IP Address is null"");"
itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestHs2HooksWithMiniKdc.java,52,        ipAddress = hookContext.getIpAddress();
itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestHs2HooksWithMiniKdc.java,53,"        Assert.assertNotNull(hookContext.getUserName(), ""Username is null"");"
itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestHs2HooksWithMiniKdc.java,54,        userName = hookContext.getUserName();
itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestHs2HooksWithMiniKdc.java,60,    public static String userName = null;
itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestHs2HooksWithMiniKdc.java,61,    public static String ipAddress = null;
itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestHs2HooksWithMiniKdc.java,64,      if (hookContext.getHookType().equals(HookType.PRE_EXEC_HOOK)) {
itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestHs2HooksWithMiniKdc.java,65,"        Assert.assertNotNull(hookContext.getIpAddress(), ""IP Address is null"");"
itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestHs2HooksWithMiniKdc.java,66,        ipAddress = hookContext.getIpAddress();
itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestHs2HooksWithMiniKdc.java,67,"        Assert.assertNotNull(hookContext.getUserName(), ""Username is null"");"
itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestHs2HooksWithMiniKdc.java,68,        userName = hookContext.getUserName();
itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestHs2HooksWithMiniKdc.java,111,   * @throws Exception
itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestHs2HooksWithMiniKdc.java,114,  public void testIpUserName() throws Exception {
itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestHs2HooksWithMiniKdc.java,122,    Assert.assertNotNull(PostExecHook.ipAddress);
itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestHs2HooksWithMiniKdc.java,123,"    Assert.assertTrue(PostExecHook.ipAddress.contains(""127.0.0.1""));"
itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestHs2HooksWithMiniKdc.java,126,    Assert.assertNotNull(PreExecHook.ipAddress);
itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestHs2HooksWithMiniKdc.java,127,"    Assert.assertTrue(PreExecHook.ipAddress.contains(""127.0.0.1""));"
itests/hive-unit/src/test/java/org/apache/hadoop/hive/hooks/TestHs2Hooks.java,40,
itests/hive-unit/src/test/java/org/apache/hadoop/hive/hooks/TestHs2Hooks.java,43,  public static class PreExecHook implements ExecuteWithHookContext {
itests/hive-unit/src/test/java/org/apache/hadoop/hive/hooks/TestHs2Hooks.java,44,    public static String userName = null;
itests/hive-unit/src/test/java/org/apache/hadoop/hive/hooks/TestHs2Hooks.java,45,    public static String ipAddress = null;
itests/hive-unit/src/test/java/org/apache/hadoop/hive/hooks/TestHs2Hooks.java,48,      if (hookContext.getHookType().equals(HookType.PRE_EXEC_HOOK)) {
itests/hive-unit/src/test/java/org/apache/hadoop/hive/hooks/TestHs2Hooks.java,49,"        Assert.assertNotNull(hookContext.getIpAddress(), ""IP Address is null"");"
itests/hive-unit/src/test/java/org/apache/hadoop/hive/hooks/TestHs2Hooks.java,50,        ipAddress = hookContext.getIpAddress();
itests/hive-unit/src/test/java/org/apache/hadoop/hive/hooks/TestHs2Hooks.java,51,"        Assert.assertNotNull(hookContext.getUserName(), ""Username is null"");"
itests/hive-unit/src/test/java/org/apache/hadoop/hive/hooks/TestHs2Hooks.java,52,        userName = hookContext.getUserName();
itests/hive-unit/src/test/java/org/apache/hadoop/hive/hooks/TestHs2Hooks.java,57,  public static class PostExecHook implements ExecuteWithHookContext {
itests/hive-unit/src/test/java/org/apache/hadoop/hive/hooks/TestHs2Hooks.java,58,    public static String userName = null;
itests/hive-unit/src/test/java/org/apache/hadoop/hive/hooks/TestHs2Hooks.java,59,    public static String ipAddress = null;
itests/hive-unit/src/test/java/org/apache/hadoop/hive/hooks/TestHs2Hooks.java,62,      if (hookContext.getHookType().equals(HookType.POST_EXEC_HOOK)) {
itests/hive-unit/src/test/java/org/apache/hadoop/hive/hooks/TestHs2Hooks.java,63,"        Assert.assertNotNull(hookContext.getIpAddress(), ""IP Address is null"");"
itests/hive-unit/src/test/java/org/apache/hadoop/hive/hooks/TestHs2Hooks.java,64,        ipAddress = hookContext.getIpAddress();
itests/hive-unit/src/test/java/org/apache/hadoop/hive/hooks/TestHs2Hooks.java,65,"        Assert.assertNotNull(hookContext.getUserName(), ""Username is null"");"
itests/hive-unit/src/test/java/org/apache/hadoop/hive/hooks/TestHs2Hooks.java,66,        userName = hookContext.getUserName();
itests/hive-unit/src/test/java/org/apache/hadoop/hive/hooks/TestHs2Hooks.java,97,   * @throws Exception
itests/hive-unit/src/test/java/org/apache/hadoop/hive/hooks/TestHs2Hooks.java,100,  public void testIpUserName() throws Exception {
itests/hive-unit/src/test/java/org/apache/hadoop/hive/hooks/TestHs2Hooks.java,106,"    connection.createStatement().execute(""show tables"");"
itests/hive-unit/src/test/java/org/apache/hadoop/hive/hooks/TestHs2Hooks.java,109,    Assert.assertNotNull(PostExecHook.ipAddress);
itests/hive-unit/src/test/java/org/apache/hadoop/hive/hooks/TestHs2Hooks.java,110,"    Assert.assertTrue(PostExecHook.ipAddress.contains(""127.0.0.1""));"
itests/hive-unit/src/test/java/org/apache/hadoop/hive/hooks/TestHs2Hooks.java,113,    Assert.assertNotNull(PreExecHook.ipAddress);
itests/hive-unit/src/test/java/org/apache/hadoop/hive/hooks/TestHs2Hooks.java,114,"    Assert.assertTrue(PreExecHook.ipAddress.contains(""127.0.0.1""));"
itests/hive-unit/src/test/java/org/apache/hadoop/hive/hooks/TestHs2Hooks.java,115,
itests/hive-unit/src/test/java/org/apache/hadoop/hive/hooks/TestHs2Hooks.java,116,    connection.close();
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,433,"      plan = new QueryPlan(command, sem, perfLogger.getStartTime(PerfLogger.DRIVER_RUN), queryId);"
ql/src/java/org/apache/hadoop/hive/ql/QueryPlan.java,112,"  public QueryPlan(String queryString, BaseSemanticAnalyzer sem, Long startTime) {"
ql/src/java/org/apache/hadoop/hive/ql/QueryPlan.java,113,"    this(queryString, sem, startTime, null);"
ql/src/java/org/apache/hadoop/hive/ql/QueryPlan.java,114,  }
ql/src/java/org/apache/hadoop/hive/ql/QueryPlan.java,115,
ql/src/java/org/apache/hadoop/hive/ql/QueryPlan.java,116,"  public QueryPlan(String queryString, BaseSemanticAnalyzer sem, Long startTime, String queryId) {"
ql/src/java/org/apache/hadoop/hive/ql/hooks/HookContext.java,150,    return SessionState.get().getHiveOperation().name();
ql/src/test/org/apache/hadoop/hive/ql/parse/TestUpdateDeleteSemanticAnalyzer.java,286,"    QueryPlan plan = new QueryPlan(query, sem, 0L, testName);"
serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroDeserializer.java,43,import org.apache.avro.util.Utf8;
serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroDeserializer.java,373,"    Map<Utf8, Object> mapDatum = (Map)datum;"
serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroDeserializer.java,376,    for (Utf8 key : mapDatum.keySet()) {
cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java,129,          this.processFile(cmd_1);
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/DynamicPartitionFileRecordWriterContainer.java,30,import org.apache.hadoop.hive.ql.metadata.HiveStorageHandler;
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/DynamicPartitionFileRecordWriterContainer.java,34,import org.apache.hadoop.io.NullWritable;
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/DynamicPartitionFileRecordWriterContainer.java,47,import org.apache.hive.hcatalog.common.HCatUtil;
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/DynamicPartitionFileRecordWriterContainer.java,100,"    for (Map.Entry<String, org.apache.hadoop.mapred.OutputCommitter> entry : baseDynamicCommitters"
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/DynamicPartitionFileRecordWriterContainer.java,101,        .entrySet()) {
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/DynamicPartitionFileRecordWriterContainer.java,102,      org.apache.hadoop.mapred.TaskAttemptContext currContext = dynamicContexts.get(entry.getKey());
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/DynamicPartitionFileRecordWriterContainer.java,103,      OutputCommitter baseOutputCommitter = entry.getValue();
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/DynamicPartitionFileRecordWriterContainer.java,104,      if (baseOutputCommitter.needsTaskCommit(currContext)) {
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/DynamicPartitionFileRecordWriterContainer.java,105,        baseOutputCommitter.commitTask(currContext);
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/DynamicPartitionFileRecordWriterContainer.java,107,    }
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,139,      return false;
ql/src/java/org/apache/hadoop/hive/ql/exec/ExprNodeGenericFuncEvaluator.java,117,"      deferredChildren[i] = new DeferredExprObject(children[i], isEager);"
ql/src/java/org/apache/hadoop/hive/ql/exec/ExprNodeGenericFuncEvaluator.java,166,    for (int i = 0; i < deferredChildren.length; i++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/ExprNodeGenericFuncEvaluator.java,167,      deferredChildren[i].prepare(version);
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16486,            case 64002: {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16633,      return ((bitField0_ & 0x00000010) == 0x00000010);
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16892,"        magic_ = """";"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16986,          bitField0_ |= 0x00000020;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17228,        return ((bitField0_ & 0x00000020) == 0x00000020);
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17280,  bitField0_ |= 0x00000020;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17293,        bitField0_ = (bitField0_ & ~0x00000020);
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17310,  bitField0_ |= 0x00000020;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17516,"      ""\""\305\001\n\nPostScript\022\024\n\014footerLength\030\001 \001(\004\022F\n"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17520,"      ""\022\026\n\016metadataLength\030\005 \001(\004\022\016\n\005magic\030\300> \001(\t"","
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17521,"      ""*:\n\017CompressionKind\022\010\n\004NONE\020\000\022\010\n\004ZLIB\020\001\022"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17522,"      ""\n\n\006SNAPPY\020\002\022\007\n\003LZO\020\003"""
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17654,"              new java.lang.String[] { ""FooterLength"", ""Compression"", ""CompressionBlockSize"", ""Version"", ""MetadataLength"", ""Magic"", });"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,397,        } else if (maximum.compareTo(str.maximum) < 0) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,566,        } else if (maximum.compareTo(dec.maximum) < 0) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,674,        } else if (maximum < dateStats.maximum) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,770,        } else if (maximum < timestampStats.maximum) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,778,          if (options.getSearchArgument() != null) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,889,"                    metadata, types, fileMetaInfo));"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,989,             ReaderImpl.FileMetaInfo fileMetaInfo) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcNewSplit.java,116,"          metadataSize, footerBuff);"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSplit.java,125,"          metadataSize, footerBuff);"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,394,        ps.getVersionList()
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,456,        ByteBuffer footerBuffer) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,457,"      this(compressionType, bufferSize, metadataSize, footerBuffer, null);"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,461,"                 ByteBuffer footerBuffer, List<Integer> versionList){"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,472,"        metadataSize, footerByteBuffer, versionList);"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java,2233,        .addVersion(version.getMinor());
ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java,1623,"    assertEquals(580, split.getLength());"
ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java,1630,"    assertEquals(601, split.getLength());"
ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java,1638,"      assertEquals(225, combineSplit.getLength(bucket));"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,751,      org.apache.hadoop.hive.metastore.api.StorageDescriptor storageDescriptor = baseTbl.getSd().deepCopy();
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,752,      SerDeInfo serdeInfo = storageDescriptor.getSerdeInfo();
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,791,      storageDescriptor.setLocation(null);
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,792,      if (location != null) {
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,793,        storageDescriptor.setLocation(location);
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,794,      }
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,795,      storageDescriptor.setInputFormat(inputFormat);
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,796,      storageDescriptor.setOutputFormat(outputFormat);
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,797,
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,798,"      Map<String, String> params = new HashMap<String,String>();"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,799,
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,802,      storageDescriptor.setBucketCols(null);
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,818,      storageDescriptor.setCols(indexTblCols);
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,819,      storageDescriptor.setSortCols(sortCols);
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,820,
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,855,"          storageDescriptor, params, deferredRebuild);"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java,698,"           dropTable(name, table, deleteData, false);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumericHistogram.java,22,import java.util.Arrays;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumericHistogram.java,45,      Coord o = (Coord) other;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumericHistogram.java,46,      if(x < o.x) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumericHistogram.java,47,        return -1;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumericHistogram.java,48,      }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumericHistogram.java,49,      if(x > o.x) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumericHistogram.java,50,        return 1;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumericHistogram.java,51,      }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumericHistogram.java,52,      return 0;
service/src/java/org/apache/hive/service/cli/session/HiveSession.java,30,  void open();
service/src/java/org/apache/hive/service/cli/session/HiveSessionBase.java,57,  /**
service/src/java/org/apache/hive/service/cli/session/HiveSessionBase.java,58,   * Initialize the session
service/src/java/org/apache/hive/service/cli/session/HiveSessionBase.java,59,   * @param sessionConfMap
service/src/java/org/apache/hive/service/cli/session/HiveSessionBase.java,60,   */
service/src/java/org/apache/hive/service/cli/session/HiveSessionBase.java,61,"  void initialize(Map<String, String> sessionConfMap) throws Exception;"
service/src/java/org/apache/hive/service/cli/session/HiveSessionBase.java,62,
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,50,import org.apache.hive.service.cli.*;
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,69,
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,71,
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,75,  private final SessionState sessionState;
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,77,
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,81,
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,82,
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,88,
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,109,
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,118,    /**
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,119,     * Create a new SessionState object that will be associated with this HiveServer2 session.
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,120,"     * When the server executes multiple queries in the same session,"
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,121,     * this SessionState object is reused across multiple queries.
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,122,     */
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,126,
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,127,    lastAccessTime = System.currentTimeMillis();
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,129,  }
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,130,
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,131,  @Override
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,132,"  public void initialize(Map<String, String> sessionConfMap) throws Exception {"
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,133,    // Process global init file: .hiverc
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,134,    processGlobalInitFile();
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,138,"      String msg = ""fail to load reloadable jar file path"" + e;"
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,140,"      throw new Exception(msg, e);"
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,142,    SessionState.setCurrentSessionState(sessionState);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,143,
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,144,    // Set conf properties specified by user from client side
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,202,"  private void configureSession(Map<String, String> sessionConfMap) throws Exception {"
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,207,"        SetProcessor.setVariable(key.substring(4), entry.getValue());"
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,220,
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,228,
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,268,  @Override
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,269,  /**
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,270,   * Opens a new HiveServer2 session for the client connection.
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,271,"   * Note that if doAs is true, this call goes through a proxy object,"
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,272,   * which wraps the method logic in a UserGroupInformation#doAs.
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,273,   * That is why it is important to call SessionState#start here rather than the constructor.
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,274,   */
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,275,  public void open() {
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,276,    SessionState.start(sessionState);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,277,  }
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,278,
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,280,    // Need to make sure that the this HiveServer2's session's session state is
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,559,
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,100,   * close the file systems for the session
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,101,   * cancel the session's delegation token and close the metastore connection
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,106,    acquire(true);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,107,    FileSystem.closeAllForUGI(sessionUgi);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,108,    cancelDelegationToken();
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,109,    } catch (IOException ioe) {
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,110,"      LOG.error(""Could not clean up file-system handles for UGI: "" + sessionUgi, ioe);"
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,112,      release(true);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,113,      super.close();
service/src/java/org/apache/hive/service/cli/session/SessionManager.java,270,      session.initialize(sessionConf);
service/src/java/org/apache/hive/service/cli/session/SessionManager.java,271,      if (isOperationLogEnabled) {
service/src/java/org/apache/hive/service/cli/session/SessionManager.java,272,        session.setOperationLogSessionDir(operationLogRootDir);
service/src/java/org/apache/hive/service/cli/session/SessionManager.java,273,      }
service/src/java/org/apache/hive/service/cli/session/SessionManager.java,274,      session.open();
service/src/java/org/apache/hive/service/cli/session/SessionManager.java,276,"      throw new HiveSQLException(""Failed to open new session"", e);"
service/src/java/org/apache/hive/service/cli/session/SessionManager.java,283,
service/src/java/org/apache/hive/service/cli/session/SessionManager.java,285,
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,689,"    HIVE_OPTIMIZE_MULTI_GROUPBY_COMMON_DISTINCTS(""hive.optimize.multigroupby.common.distincts"", true,"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,690,"        ""Whether to optimize a multi-groupby query with the same distinct.\n"" +"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,691,"        ""Consider a query like:\n"" +"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,692,"        ""\n"" +"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,693,"        ""  from src\n"" +"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,694,"        ""    insert overwrite table dest1 select col1, count(distinct colx) group by col1\n"" +"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,695,"        ""    insert overwrite table dest2 select col2, count(distinct colx) group by col2;\n"" +"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,696,"        ""\n"" +"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,697,"        ""With this parameter set to true, first we spray by the distinct value (colx), and then\n"" +"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,698,"        ""perform the 2 groups bys. This makes sense if map-side aggregation is turned off. However,\n"" +"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,699,"        ""with maps-side aggregation, it might be useful in some cases to treat the 2 inserts independently, \n"" +"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,700,"        ""thereby performing the query above in 2MR jobs instead of 3 (due to spraying by distinct key first).\n"" +"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,701,"        ""If this parameter is turned off, we don't consider the fact that the distinct key is the same across\n"" +"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,702,"        ""different MR jobs.""),"
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,83,
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,110,  // Used by hash distinct aggregations when hashGrpKeyNotRedKey is true
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,111,  private transient HashSet<KeyWrapper> keysCurrentGroup;
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,112,
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,115,"  // The reduction is happening on the reducer, and the grouping key and"
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,116,  // reduction keys are different.
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,117,"  // For example: select a, count(distinct b) from T group by a"
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,118,  // The data is sprayed by 'b' and the reducer is grouping it by 'a'
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,119,  private transient boolean groupKeyIsNotReduceKey;
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,120,  private transient boolean firstRowInGroup;
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,136,  private transient List<Integer> groupingSets;       // declared grouping set values
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,137,  private transient FastBitSet[] groupingSetsBitSet;  // bitsets acquired from grouping set values
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,365,      groupKeyIsNotReduceKey = conf.getGroupKeyNotReductionKey();
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,366,      if (groupKeyIsNotReduceKey) {
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,367,        keysCurrentGroup = new HashSet<KeyWrapper>();
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,368,      }
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,378,    ObjectInspector[] objectInspectors =
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,699,  @Override
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,700,  public void startGroup() throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,701,    firstRowInGroup = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,702,    super.startGroup();
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,703,  }
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,704,
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,705,  @Override
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,706,  public void endGroup() throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,707,    if (groupKeyIsNotReduceKey) {
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,708,      keysCurrentGroup.clear();
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,709,    }
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,710,  }
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,711,
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,721,    firstRowInGroup = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,722,
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,735,    if (hashAggr && !groupKeyIsNotReduceKey) {
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,811,"    // If the grouping key and the reduction key are different, a set of"
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,812,    // grouping keys for the current reduction key are maintained in
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,813,    // keysCurrentGroup
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,814,    // Peek into the set to find out if a new grouping key is seen for the given
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,815,    // reduction key
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,816,    if (groupKeyIsNotReduceKey) {
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,817,      newEntryForHashAggr = keysCurrentGroup.add(newKeys.copyKey());
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,818,    }
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,819,
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,829,"    // If the grouping key is not the same as reduction key, flushing can only"
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,830,    // happen at boundaries
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,831,    if ((!groupKeyIsNotReduceKey || firstRowInGroup)
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,832,        && shouldBeFlushed(newKeys)) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1089,      if (desc.getGroupKeyNotReductionKey()) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1090,"        LOG.info(""Reduce vector mode not supported when group key is not reduction key"");"
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1091,        return false;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1092,      }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,233,"  private LinkedHashMap<Operator<? extends OperatorDesc>, OpParseContext> opParseCtx;"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4173,   * @param distPartAggr
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4174,   *          partial aggregation for distincts
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4187,"      boolean distPartAgg,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4286,      boolean partialAggDone = !(distPartAgg || isDistinct);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4287,      if (!partialAggDone) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4306,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4319,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4321,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4337,      boolean isAllColumns = value.getType() == HiveParser.TOK_FUNCTIONSTAR;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4340,"      // For distincts, partial aggregations have not been done"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4341,      if (distPartAgg) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4342,"        genericUDAFEvaluator = getGenericUDAFEvaluator(aggName, aggParameters,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4343,"            value, isDistinct, isAllColumns);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4344,        assert (genericUDAFEvaluator != null);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4345,"        genericUDAFEvaluators.put(entry.getKey(), genericUDAFEvaluator);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4346,      } else {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4347,        genericUDAFEvaluator = genericUDAFEvaluators.get(entry.getKey());
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4348,        assert (genericUDAFEvaluator != null);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4349,      }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4373,"            distPartAgg, groupByMemoryUsage, memoryThreshold,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5261,  /**
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5262,   * Generate a Multi Group-By plan using a 2 map-reduce jobs.
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5263,   *
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5264,   * @param dest
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5265,   * @param qb
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5266,   * @param input
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5267,   * @return
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5268,   * @throws SemanticException
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5269,   *
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5270,   *           Generate a Group-By plan using a 2 map-reduce jobs. Spray by the
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5271,"   *           distinct key in hope of getting a uniform distribution, and"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5272,   *           compute partial aggregates by the grouping key. Evaluate partial
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5273,"   *           aggregates first, and spray by the grouping key to compute actual"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5274,   *           aggregates in the second phase. The aggregation evaluation
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5275,   *           functions are as follows: Partitioning Key: distinct key
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5276,   *
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5277,   *           Sorting Key: distinct key
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5278,   *
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5279,   *           Reducer: iterate/terminatePartial (mode = PARTIAL1)
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5280,   *
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5281,   *           STAGE 2
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5282,   *
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5283,   *           Partitioning Key: grouping key
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5284,   *
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5285,   *           Sorting Key: grouping key
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5286,   *
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5287,   *           Reducer: merge/terminate (mode = FINAL)
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5288,   */
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5289,"  @SuppressWarnings(""nls"")"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5290,"  private Operator genGroupByPlan2MRMultiGroupBy(String dest, QB qb,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5291,      Operator input) throws SemanticException {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5292,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5293,    // ////// Generate GroupbyOperator for a map-side partial aggregation
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5294,"    Map<String, GenericUDAFEvaluator> genericUDAFEvaluators ="
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5295,"        new LinkedHashMap<String, GenericUDAFEvaluator>();"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5296,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5297,    QBParseInfo parseInfo = qb.getParseInfo();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5298,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5299,    // ////// 2. Generate GroupbyOperator
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5300,"    Operator groupByOperatorInfo = genGroupByPlanGroupByOperator1(parseInfo,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5301,"        dest, input, GroupByDesc.Mode.HASH, genericUDAFEvaluators, true,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5302,"        null, false, false);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5303,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5304,    int numReducers = -1;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5305,"    List<ASTNode> grpByExprs = getGroupByForClause(parseInfo, dest);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5306,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5307,    // ////// 3. Generate ReduceSinkOperator2
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5308,    Operator reduceSinkOperatorInfo2 = genGroupByPlanReduceSinkOperator2MR(
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5309,"        parseInfo, dest, groupByOperatorInfo, grpByExprs.size(), numReducers, false);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5310,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5311,    // ////// 4. Generate GroupbyOperator2
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5312,"    Operator groupByOperatorInfo2 = genGroupByPlanGroupByOperator2MR(parseInfo,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5313,"        dest, reduceSinkOperatorInfo2, GroupByDesc.Mode.FINAL,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5314,"        genericUDAFEvaluators, false);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5315,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5316,    return groupByOperatorInfo2;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5317,  }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5318,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5643,"          genericUDAFEvaluators, false,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5656,"              genericUDAFEvaluators, false,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5787,"          genericUDAFEvaluators, false,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8422,  // Return the common distinct expression
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8423,"  // There should be more than 1 destination, with group bys in all of them."
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8424,"  private List<ASTNode> getCommonDistinctExprs(QB qb, Operator input) {"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8425,    QBParseInfo qbp = qb.getParseInfo();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8426,"    // If a grouping set aggregation is present, common processing is not possible"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8427,    if (!qbp.getDestCubes().isEmpty() || !qbp.getDestRollups().isEmpty()
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8428,        || !qbp.getDestToLateralView().isEmpty()) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8429,      return null;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8430,    }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8431,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8432,    RowResolver inputRR = opParseCtx.get(input).getRowResolver();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8433,    TreeSet<String> ks = new TreeSet<String>();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8434,    ks.addAll(qbp.getClauseNames());
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8435,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8436,    // Go over all the destination tables
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8437,    if (ks.size() <= 1) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8438,      return null;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8439,    }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8440,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8441,    List<ExprNodeDesc> oldList = null;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8442,    List<ASTNode> oldASTList = null;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8443,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8444,    for (String dest : ks) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8445,"      // If a filter is present, common processing is not possible"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8446,      if (qbp.getWhrForClause(dest) != null) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8447,        return null;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8448,      }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8449,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8450,      if (qbp.getAggregationExprsForClause(dest).size() == 0
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8451,"          && getGroupByForClause(qbp, dest).size() == 0) {"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8452,        return null;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8453,      }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8454,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8455,      // All distinct expressions must be the same
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8456,      List<ASTNode> list = qbp.getDistinctFuncExprsForClause(dest);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8457,      if (list.isEmpty()) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8458,        return null;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8459,      }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8460,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8461,      List<ExprNodeDesc> currDestList;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8462,      try {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8463,"        currDestList = getDistinctExprs(qbp, dest, inputRR);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8464,      } catch (SemanticException e) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8465,        return null;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8466,      }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8467,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8468,      List<ASTNode> currASTList = new ArrayList<ASTNode>();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8469,      for (ASTNode value : list) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8470,        // 0 is function name
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8471,        for (int i = 1; i < value.getChildCount(); i++) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8472,          ASTNode parameter = (ASTNode) value.getChild(i);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8473,          currASTList.add(parameter);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8474,        }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8475,        if (oldList == null) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8476,          oldList = currDestList;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8477,          oldASTList = currASTList;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8478,        } else {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8479,"          if (!matchExprLists(oldList, currDestList)) {"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8480,            return null;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8481,          }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8482,        }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8483,      }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8484,    }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8485,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8486,    return oldASTList;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8487,  }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8488,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8489,"  private Operator createCommonReduceSink(QB qb, Operator input)"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8490,      throws SemanticException {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8491,    // Go over all the tables and extract the common distinct key
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8492,"    List<ASTNode> distExprs = getCommonDistinctExprs(qb, input);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8493,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8494,    QBParseInfo qbp = qb.getParseInfo();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8495,    TreeSet<String> ks = new TreeSet<String>();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8496,    ks.addAll(qbp.getClauseNames());
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8497,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8498,    // Pass the entire row
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8499,    RowResolver inputRR = opParseCtx.get(input).getRowResolver();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8500,    RowResolver reduceSinkOutputRowResolver = new RowResolver();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8501,    reduceSinkOutputRowResolver.setIsExprResolver(true);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8502,    ArrayList<ExprNodeDesc> reduceKeys = new ArrayList<ExprNodeDesc>();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8503,    ArrayList<ExprNodeDesc> reduceValues = new ArrayList<ExprNodeDesc>();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8504,"    Map<String, ExprNodeDesc> colExprMap = new HashMap<String, ExprNodeDesc>();"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8505,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8506,    // Pre-compute distinct group-by keys and store in reduceKeys
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8507,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8508,    List<String> outputColumnNames = new ArrayList<String>();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8509,    for (ASTNode distn : distExprs) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8510,"      ExprNodeDesc distExpr = genExprNodeDesc(distn, inputRR);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8511,      if (reduceSinkOutputRowResolver.getExpression(distn) == null) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8512,        reduceKeys.add(distExpr);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8513,        outputColumnNames.add(getColumnInternalName(reduceKeys.size() - 1));
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8514,"        String field = Utilities.ReduceField.KEY.toString() + ""."""
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8515,            + getColumnInternalName(reduceKeys.size() - 1);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8516,"        ColumnInfo colInfo = new ColumnInfo(field, reduceKeys.get("
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8517,"            reduceKeys.size() - 1).getTypeInfo(), """", false);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8518,"        reduceSinkOutputRowResolver.putExpression(distn, colInfo);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8519,"        colExprMap.put(colInfo.getInternalName(), distExpr);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8520,      }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8521,    }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8522,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8523,    // Go over all the grouping keys and aggregations
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8524,    for (String dest : ks) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8525,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8526,"      List<ASTNode> grpByExprs = getGroupByForClause(qbp, dest);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8527,      for (int i = 0; i < grpByExprs.size(); ++i) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8528,        ASTNode grpbyExpr = grpByExprs.get(i);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8529,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8530,        if (reduceSinkOutputRowResolver.getExpression(grpbyExpr) == null) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8531,"          ExprNodeDesc grpByExprNode = genExprNodeDesc(grpbyExpr, inputRR);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8532,          reduceValues.add(grpByExprNode);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8533,"          String field = Utilities.ReduceField.VALUE.toString() + ""."""
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8534,              + getColumnInternalName(reduceValues.size() - 1);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8535,"          ColumnInfo colInfo = new ColumnInfo(field, reduceValues.get("
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8536,"              reduceValues.size() - 1).getTypeInfo(), """", false);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8537,"          reduceSinkOutputRowResolver.putExpression(grpbyExpr, colInfo);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8538,          outputColumnNames.add(getColumnInternalName(reduceValues.size() - 1));
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8539,"          colExprMap.put(field, grpByExprNode);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8540,        }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8541,      }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8542,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8543,      // For each aggregation
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8544,"      HashMap<String, ASTNode> aggregationTrees = qbp"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8545,          .getAggregationExprsForClause(dest);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8546,      assert (aggregationTrees != null);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8547,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8548,"      for (Map.Entry<String, ASTNode> entry : aggregationTrees.entrySet()) {"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8549,        ASTNode value = entry.getValue();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8550,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8551,        // 0 is the function name
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8552,        for (int i = 1; i < value.getChildCount(); i++) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8553,          ASTNode paraExpr = (ASTNode) value.getChild(i);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8554,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8555,          if (reduceSinkOutputRowResolver.getExpression(paraExpr) == null) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8556,"            ExprNodeDesc paraExprNode = genExprNodeDesc(paraExpr, inputRR);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8557,            reduceValues.add(paraExprNode);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8558,"            String field = Utilities.ReduceField.VALUE.toString() + ""."""
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8559,                + getColumnInternalName(reduceValues.size() - 1);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8560,"            ColumnInfo colInfo = new ColumnInfo(field, reduceValues.get("
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8561,"                reduceValues.size() - 1).getTypeInfo(), """", false);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8562,"            reduceSinkOutputRowResolver.putExpression(paraExpr, colInfo);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8563,            outputColumnNames
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8564,                .add(getColumnInternalName(reduceValues.size() - 1));
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8565,"            colExprMap.put(field, paraExprNode);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8566,          }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8567,        }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8568,      }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8569,    }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8570,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8571,    ReduceSinkOperator rsOp = (ReduceSinkOperator) putOpInsertMap(
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8572,"        OperatorFactory.getAndMakeChild(PlanUtils.getReduceSinkDesc(reduceKeys,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8573,"            reduceValues, outputColumnNames, true, -1, reduceKeys.size(), -1,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8574,"                AcidUtils.Operation.NOT_ACID),"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8575,"            new RowSchema(reduceSinkOutputRowResolver.getColumnInfos()), input),"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8576,        reduceSinkOutputRowResolver);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8577,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8578,    rsOp.setColumnExprMap(colExprMap);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8579,    return rsOp;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8580,  }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8581,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8759,"    // For multi-group by with the same distinct, we ignore all user hints"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8760,    // currently. It doesnt matter whether he has asked to do
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8761,    // map-side aggregation or not. Map side aggregation is turned off
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8762,"    List<ASTNode> commonDistinctExprs = getCommonDistinctExprs(qb, input);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8763,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8764,    // Consider a query like:
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8765,    //
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8766,    //  from src
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8767,"    //    insert overwrite table dest1 select col1, count(distinct colx) group by col1"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8768,"    //    insert overwrite table dest2 select col2, count(distinct colx) group by col2;"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8769,    //
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8770,"    // With HIVE_OPTIMIZE_MULTI_GROUPBY_COMMON_DISTINCTS set to true, first we spray by the distinct"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8771,"    // value (colx), and then perform the 2 groups bys. This makes sense if map-side aggregation is"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8772,"    // turned off. However, with maps-side aggregation, it might be useful in some cases to treat"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8773,"    // the 2 inserts independently, thereby performing the query above in 2MR jobs instead of 3"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8774,    // (due to spraying by distinct key first).
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8775,    boolean optimizeMultiGroupBy = commonDistinctExprs != null &&
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8776,        conf.getBoolVar(HiveConf.ConfVars.HIVE_OPTIMIZE_MULTI_GROUPBY_COMMON_DISTINCTS);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8780,"    // if there is a single distinct, optimize that. Spray initially by the"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8781,"    // distinct key,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8782,    // no computation at the mapper. Have multiple group by operators at the
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8783,    // reducer - and then
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8784,    // proceed
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8785,    if (optimizeMultiGroupBy) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8786,"      curr = createCommonReduceSink(qb, input);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8787,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8788,      RowResolver currRR = opParseCtx.get(curr).getRowResolver();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8789,      // create a forward operator
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8790,"      input = putOpInsertMap(OperatorFactory.getAndMakeChild(new ForwardDesc(),"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8791,"          new RowSchema(currRR.getColumnInfos()), curr), currRR);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8792,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8793,      for (String dest : ks) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8794,        curr = input;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8795,"        curr = genGroupByPlan2MRMultiGroupBy(dest, qb, curr);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8796,"        curr = genSelectPlan(dest, qb, curr, null); // TODO: we may need to pass ""input"" here instead of null"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8797,        Integer limit = qbp.getDestLimit(dest);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8798,        if (limit != null) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8799,"          curr = genLimitMapRedPlan(dest, qb, curr, limit.intValue(), true);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8800,          qb.getParseInfo().setOuterQueryLimit(limit.intValue());
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8801,        }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8802,"        curr = genFileSinkPlan(dest, qb, curr);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8803,      }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8804,    } else {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8805,      List<List<String>> commonGroupByDestGroups = null;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8807,"      // If we can put multiple group bys in a single reducer, determine suitable groups of"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8808,"      // expressions, otherwise treat all the expressions as a single group"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8809,      if (conf.getBoolVar(HiveConf.ConfVars.HIVEMULTIGROUPBYSINGLEREDUCER)) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8810,        try {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8811,"          commonGroupByDestGroups = getCommonGroupByDestGroups(qb, inputs);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8812,        } catch (SemanticException e) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8813,"          LOG.error(""Failed to group clauses by common spray keys."", e);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8814,        }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8817,      if (commonGroupByDestGroups == null) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8818,        commonGroupByDestGroups = new ArrayList<List<String>>();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8819,        commonGroupByDestGroups.add(new ArrayList<String>(ks));
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8820,      }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8822,      if (!commonGroupByDestGroups.isEmpty()) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8824,        // Iterate over each group of subqueries with the same group by/distinct keys
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8825,        for (List<String> commonGroupByDestGroup : commonGroupByDestGroups) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8826,          if (commonGroupByDestGroup.isEmpty()) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8827,            continue;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8828,          }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8830,          String firstDest = commonGroupByDestGroup.get(0);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8831,          input = inputs.get(firstDest);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8832,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8833,          // Constructs a standard group by plan if:
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8834,          // There is no other subquery with the same group by/distinct keys or
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8835,          // (There are no aggregations in a representative query for the group and
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8836,          // There is no group by in that representative query) or
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8837,          // The data is skewed or
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8838,          // The conf variable used to control combining group bys into a single reducer is false
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8839,          if (commonGroupByDestGroup.size() == 1 ||
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8840,              (qbp.getAggregationExprsForClause(firstDest).size() == 0 &&
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8841,"              getGroupByForClause(qbp, firstDest).size() == 0) ||"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8842,              conf.getBoolVar(HiveConf.ConfVars.HIVEGROUPBYSKEW) ||
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8843,              !conf.getBoolVar(HiveConf.ConfVars.HIVEMULTIGROUPBYSINGLEREDUCER)) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8844,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8845,            // Go over all the destination tables
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8846,            for (String dest : commonGroupByDestGroup) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8847,              curr = inputs.get(dest);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8848,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8849,              if (qbp.getWhrForClause(dest) != null) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8850,                ASTNode whereExpr = qb.getParseInfo().getWhrForClause(dest);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8851,"                curr = genFilterPlan((ASTNode) whereExpr.getChild(0), qb, curr, aliasToOpInfo, false);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8853,              // Preserve operator before the GBY - we'll use it to resolve '*'
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8854,              Operator<?> gbySource = curr;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8855,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8856,              if (qbp.getAggregationExprsForClause(dest).size() != 0
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8857,"                  || getGroupByForClause(qbp, dest).size() > 0) {"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8858,                // multiple distincts is not supported with skew in data
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8859,                if (conf.getBoolVar(HiveConf.ConfVars.HIVEGROUPBYSKEW) &&
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8860,                    qbp.getDistinctFuncExprsForClause(dest).size() > 1) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8861,                  throw new SemanticException(ErrorMsg.UNSUPPORTED_MULTIPLE_DISTINCTS.
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8862,                      getMsg());
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8864,                // insert a select operator here used by the ColumnPruner to reduce
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8865,                // the data to shuffle
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8866,                curr = insertSelectAllPlanForGroupBy(curr);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8867,                // Check and transform group by *. This will only happen for select distinct *.
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8868,"                // Here the ""genSelectPlan"" is being leveraged."
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8869,                // The main benefits are (1) remove virtual columns that should
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8870,                // not be included in the group by; (2) add the fully qualified column names to unParseTranslator
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8871,                // so that view is supported. The drawback is that an additional SEL op is added. If it is
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8872,"                // not necessary, it will be removed by NonBlockingOpDeDupProc Optimizer because it will match"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8873,                // SEL%SEL% rule.
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8874,                ASTNode selExprList = qbp.getSelForClause(dest);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8875,                if (selExprList.getToken().getType() == HiveParser.TOK_SELECTDI
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8876,                    && selExprList.getChildCount() == 1 && selExprList.getChild(0).getChildCount() == 1) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8877,                  ASTNode node = (ASTNode) selExprList.getChild(0).getChild(0);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8878,                  if (node.getToken().getType() == HiveParser.TOK_ALLCOLREF) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8879,"                    curr = genSelectPlan(dest, qb, curr, curr);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8880,                    RowResolver rr = opParseCtx.get(curr).getRowResolver();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8881,"                    qbp.setSelExprForClause(dest, SemanticAnalyzer.genSelectDIAST(rr));"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8882,                  }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8883,                }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8884,                if (conf.getBoolVar(HiveConf.ConfVars.HIVEMAPSIDEAGGREGATE)) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8885,                  if (!conf.getBoolVar(HiveConf.ConfVars.HIVEGROUPBYSKEW)) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8886,"                    curr = genGroupByPlanMapAggrNoSkew(dest, qb, curr);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8887,                  } else {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8888,"                    curr = genGroupByPlanMapAggr2MR(dest, qb, curr);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8889,                  }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8890,                } else if (conf.getBoolVar(HiveConf.ConfVars.HIVEGROUPBYSKEW)) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8891,"                  curr = genGroupByPlan2MR(dest, qb, curr);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8893,"                  curr = genGroupByPlan1MR(dest, qb, curr);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8896,              if (LOG.isDebugEnabled()) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8897,"                LOG.debug(""RR before GB "" + opParseCtx.get(gbySource).getRowResolver()"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8898,"                    + "" after GB "" + opParseCtx.get(curr).getRowResolver());"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8899,              }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8900,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8901,"              curr = genPostGroupByBodyPlan(curr, dest, qb, aliasToOpInfo, gbySource);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8903,          } else {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8904,"            curr = genGroupByPlan1ReduceMultiGBY(commonGroupByDestGroup, qb, input, aliasToOpInfo);"
ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java,57,  private boolean groupKeyNotReductionKey;
ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java,85,"      final boolean groupKeyNotReductionKey,"
ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java,92,"    this(mode, outputColumnNames, keys, aggregators, groupKeyNotReductionKey,"
ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java,102,"      final boolean groupKeyNotReductionKey,"
ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java,115,    this.groupKeyNotReductionKey = groupKeyNotReductionKey;
ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java,183,    return groupingSetPosition >= 0 &&
ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java,232,
ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java,233,  public boolean getGroupKeyNotReductionKey() {
ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java,234,    return groupKeyNotReductionKey;
ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java,235,  }
ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java,236,
ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java,237,  public void setGroupKeyNotReductionKey(final boolean groupKeyNotReductionKey) {
ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java,238,    this.groupKeyNotReductionKey = groupKeyNotReductionKey;
ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java,239,  }
ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java,240,
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12,   * Protobuf enum {@code org.apache.hadoop.hive.ql.io.orc.CompressionKind}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,108,    // @@protoc_insertion_point(enum_scope:org.apache.hadoop.hive.ql.io.orc.CompressionKind)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,145,   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.IntegerStatistics}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,224,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_IntegerStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,229,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_IntegerStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,424,     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.IntegerStatistics}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,431,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_IntegerStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,436,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_IntegerStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,476,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_IntegerStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,658,      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.IntegerStatistics)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,666,    // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.IntegerStatistics)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,703,   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.DoubleStatistics}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,782,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DoubleStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,787,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DoubleStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,982,     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.DoubleStatistics}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,989,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DoubleStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,994,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DoubleStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,1034,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DoubleStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,1216,      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.DoubleStatistics)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,1224,    // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.DoubleStatistics)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,1279,   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.StringStatistics}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,1358,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StringStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,1363,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StringStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,1620,     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.StringStatistics}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,1627,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StringStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,1632,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StringStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,1672,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StringStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,1956,      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.StringStatistics)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,1964,    // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.StringStatistics)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,1985,   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.BucketStatistics}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,2073,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_BucketStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,2078,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_BucketStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,2246,     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.BucketStatistics}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,2253,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_BucketStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,2258,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_BucketStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,2294,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_BucketStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,2435,      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.BucketStatistics)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,2443,    // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.BucketStatistics)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,2495,   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.DecimalStatistics}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,2574,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DecimalStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,2579,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DecimalStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,2855,     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.DecimalStatistics}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,2862,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DecimalStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,2867,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DecimalStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,2907,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DecimalStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,3218,      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.DecimalStatistics)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,3226,    // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.DecimalStatistics)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,3261,   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.DateStatistics}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,3335,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DateStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,3340,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DateStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,3519,     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.DateStatistics}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,3526,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DateStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,3531,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DateStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,3569,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DateStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,3727,      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.DateStatistics)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,3735,    // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.DateStatistics)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,3770,   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.TimestampStatistics}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,3844,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_TimestampStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,3849,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_TimestampStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4028,     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.TimestampStatistics}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4035,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_TimestampStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4040,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_TimestampStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4078,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_TimestampStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4236,      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.TimestampStatistics)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4244,    // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.TimestampStatistics)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4269,   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.BinaryStatistics}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4338,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_BinaryStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4343,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_BinaryStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4498,     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.BinaryStatistics}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4505,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_BinaryStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4510,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_BinaryStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4546,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_BinaryStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4664,      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.BinaryStatistics)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4672,    // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.BinaryStatistics)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4688,    // optional .org.apache.hadoop.hive.ql.io.orc.IntegerStatistics intStatistics = 2;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4690,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.IntegerStatistics intStatistics = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4694,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.IntegerStatistics intStatistics = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4698,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.IntegerStatistics intStatistics = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4702,    // optional .org.apache.hadoop.hive.ql.io.orc.DoubleStatistics doubleStatistics = 3;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4704,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.DoubleStatistics doubleStatistics = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4708,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.DoubleStatistics doubleStatistics = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4712,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.DoubleStatistics doubleStatistics = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4716,    // optional .org.apache.hadoop.hive.ql.io.orc.StringStatistics stringStatistics = 4;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4718,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.StringStatistics stringStatistics = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4722,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.StringStatistics stringStatistics = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4726,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.StringStatistics stringStatistics = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4730,    // optional .org.apache.hadoop.hive.ql.io.orc.BucketStatistics bucketStatistics = 5;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4732,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.BucketStatistics bucketStatistics = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4736,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.BucketStatistics bucketStatistics = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4740,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.BucketStatistics bucketStatistics = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4744,    // optional .org.apache.hadoop.hive.ql.io.orc.DecimalStatistics decimalStatistics = 6;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4746,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.DecimalStatistics decimalStatistics = 6;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4750,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.DecimalStatistics decimalStatistics = 6;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4754,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.DecimalStatistics decimalStatistics = 6;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4758,    // optional .org.apache.hadoop.hive.ql.io.orc.DateStatistics dateStatistics = 7;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4760,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.DateStatistics dateStatistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4764,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.DateStatistics dateStatistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4768,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.DateStatistics dateStatistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4772,    // optional .org.apache.hadoop.hive.ql.io.orc.BinaryStatistics binaryStatistics = 8;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4774,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.BinaryStatistics binaryStatistics = 8;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4778,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.BinaryStatistics binaryStatistics = 8;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4782,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.BinaryStatistics binaryStatistics = 8;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4786,    // optional .org.apache.hadoop.hive.ql.io.orc.TimestampStatistics timestampStatistics = 9;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4788,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.TimestampStatistics timestampStatistics = 9;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4792,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.TimestampStatistics timestampStatistics = 9;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4796,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.TimestampStatistics timestampStatistics = 9;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4811,   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.ColumnStatistics}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4989,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4994,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5031,    // optional .org.apache.hadoop.hive.ql.io.orc.IntegerStatistics intStatistics = 2;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5035,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.IntegerStatistics intStatistics = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5041,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.IntegerStatistics intStatistics = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5047,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.IntegerStatistics intStatistics = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5053,    // optional .org.apache.hadoop.hive.ql.io.orc.DoubleStatistics doubleStatistics = 3;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5057,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.DoubleStatistics doubleStatistics = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5063,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.DoubleStatistics doubleStatistics = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5069,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.DoubleStatistics doubleStatistics = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5075,    // optional .org.apache.hadoop.hive.ql.io.orc.StringStatistics stringStatistics = 4;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5079,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.StringStatistics stringStatistics = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5085,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.StringStatistics stringStatistics = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5091,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.StringStatistics stringStatistics = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5097,    // optional .org.apache.hadoop.hive.ql.io.orc.BucketStatistics bucketStatistics = 5;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5101,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.BucketStatistics bucketStatistics = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5107,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.BucketStatistics bucketStatistics = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5113,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.BucketStatistics bucketStatistics = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5119,    // optional .org.apache.hadoop.hive.ql.io.orc.DecimalStatistics decimalStatistics = 6;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5123,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.DecimalStatistics decimalStatistics = 6;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5129,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.DecimalStatistics decimalStatistics = 6;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5135,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.DecimalStatistics decimalStatistics = 6;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5141,    // optional .org.apache.hadoop.hive.ql.io.orc.DateStatistics dateStatistics = 7;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5145,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.DateStatistics dateStatistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5151,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.DateStatistics dateStatistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5157,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.DateStatistics dateStatistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5163,    // optional .org.apache.hadoop.hive.ql.io.orc.BinaryStatistics binaryStatistics = 8;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5167,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.BinaryStatistics binaryStatistics = 8;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5173,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.BinaryStatistics binaryStatistics = 8;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5179,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.BinaryStatistics binaryStatistics = 8;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5185,    // optional .org.apache.hadoop.hive.ql.io.orc.TimestampStatistics timestampStatistics = 9;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5189,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.TimestampStatistics timestampStatistics = 9;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5195,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.TimestampStatistics timestampStatistics = 9;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5201,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.TimestampStatistics timestampStatistics = 9;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5405,     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.ColumnStatistics}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5412,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5417,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5511,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5708,      // optional .org.apache.hadoop.hive.ql.io.orc.IntegerStatistics intStatistics = 2;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5713,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.IntegerStatistics intStatistics = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5719,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.IntegerStatistics intStatistics = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5729,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.IntegerStatistics intStatistics = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5745,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.IntegerStatistics intStatistics = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5759,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.IntegerStatistics intStatistics = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5778,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.IntegerStatistics intStatistics = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5791,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.IntegerStatistics intStatistics = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5799,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.IntegerStatistics intStatistics = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5809,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.IntegerStatistics intStatistics = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5825,      // optional .org.apache.hadoop.hive.ql.io.orc.DoubleStatistics doubleStatistics = 3;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5830,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DoubleStatistics doubleStatistics = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5836,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DoubleStatistics doubleStatistics = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5846,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DoubleStatistics doubleStatistics = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5862,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DoubleStatistics doubleStatistics = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5876,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DoubleStatistics doubleStatistics = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5895,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DoubleStatistics doubleStatistics = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5908,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DoubleStatistics doubleStatistics = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5916,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DoubleStatistics doubleStatistics = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5926,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DoubleStatistics doubleStatistics = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5942,      // optional .org.apache.hadoop.hive.ql.io.orc.StringStatistics stringStatistics = 4;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5947,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.StringStatistics stringStatistics = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5953,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.StringStatistics stringStatistics = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5963,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.StringStatistics stringStatistics = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5979,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.StringStatistics stringStatistics = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5993,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.StringStatistics stringStatistics = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6012,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.StringStatistics stringStatistics = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6025,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.StringStatistics stringStatistics = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6033,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.StringStatistics stringStatistics = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6043,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.StringStatistics stringStatistics = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6059,      // optional .org.apache.hadoop.hive.ql.io.orc.BucketStatistics bucketStatistics = 5;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6064,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.BucketStatistics bucketStatistics = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6070,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.BucketStatistics bucketStatistics = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6080,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.BucketStatistics bucketStatistics = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6096,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.BucketStatistics bucketStatistics = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6110,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.BucketStatistics bucketStatistics = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6129,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.BucketStatistics bucketStatistics = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6142,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.BucketStatistics bucketStatistics = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6150,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.BucketStatistics bucketStatistics = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6160,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.BucketStatistics bucketStatistics = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6176,      // optional .org.apache.hadoop.hive.ql.io.orc.DecimalStatistics decimalStatistics = 6;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6181,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DecimalStatistics decimalStatistics = 6;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6187,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DecimalStatistics decimalStatistics = 6;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6197,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DecimalStatistics decimalStatistics = 6;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6213,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DecimalStatistics decimalStatistics = 6;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6227,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DecimalStatistics decimalStatistics = 6;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6246,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DecimalStatistics decimalStatistics = 6;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6259,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DecimalStatistics decimalStatistics = 6;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6267,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DecimalStatistics decimalStatistics = 6;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6277,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DecimalStatistics decimalStatistics = 6;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6293,      // optional .org.apache.hadoop.hive.ql.io.orc.DateStatistics dateStatistics = 7;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6298,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DateStatistics dateStatistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6304,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DateStatistics dateStatistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6314,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DateStatistics dateStatistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6330,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DateStatistics dateStatistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6344,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DateStatistics dateStatistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6363,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DateStatistics dateStatistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6376,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DateStatistics dateStatistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6384,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DateStatistics dateStatistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6394,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DateStatistics dateStatistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6410,      // optional .org.apache.hadoop.hive.ql.io.orc.BinaryStatistics binaryStatistics = 8;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6415,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.BinaryStatistics binaryStatistics = 8;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6421,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.BinaryStatistics binaryStatistics = 8;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6431,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.BinaryStatistics binaryStatistics = 8;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6447,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.BinaryStatistics binaryStatistics = 8;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6461,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.BinaryStatistics binaryStatistics = 8;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6480,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.BinaryStatistics binaryStatistics = 8;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6493,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.BinaryStatistics binaryStatistics = 8;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6501,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.BinaryStatistics binaryStatistics = 8;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6511,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.BinaryStatistics binaryStatistics = 8;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6527,      // optional .org.apache.hadoop.hive.ql.io.orc.TimestampStatistics timestampStatistics = 9;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6532,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.TimestampStatistics timestampStatistics = 9;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6538,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.TimestampStatistics timestampStatistics = 9;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6548,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.TimestampStatistics timestampStatistics = 9;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6564,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.TimestampStatistics timestampStatistics = 9;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6578,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.TimestampStatistics timestampStatistics = 9;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6597,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.TimestampStatistics timestampStatistics = 9;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6610,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.TimestampStatistics timestampStatistics = 9;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6618,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.TimestampStatistics timestampStatistics = 9;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6628,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.TimestampStatistics timestampStatistics = 9;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6677,      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.ColumnStatistics)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6685,    // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.ColumnStatistics)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6705,    // optional .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 2;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6707,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6711,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6715,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6720,   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.RowIndexEntry}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6821,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndexEntry_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6826,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndexEntry_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6871,    // optional .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 2;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6875,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6881,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6887,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7025,     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.RowIndexEntry}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7032,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndexEntry_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7037,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndexEntry_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7080,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndexEntry_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7234,      // optional .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 2;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7239,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7245,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7255,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7271,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7285,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7304,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7317,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7325,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7335,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7351,      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.RowIndexEntry)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7359,    // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.RowIndexEntry)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7365,    // repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7367,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7372,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7376,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7380,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7385,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7391,   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.RowIndex}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7466,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndex_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7471,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndex_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7491,    // repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7495,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7501,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7508,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7514,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7520,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7637,     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.RowIndex}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7644,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndex_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7649,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndex_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7690,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndex_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7785,      // repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7799,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7809,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7819,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7829,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7846,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7860,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7876,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7893,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7907,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7921,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7935,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7948,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7961,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7968,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7978,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7989,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7996,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8004,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8025,      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.RowIndex)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8033,    // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.RowIndex)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8064,   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.BloomFilter}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8157,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_BloomFilter_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8162,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_BloomFilter_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8342,     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.BloomFilter}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8349,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_BloomFilter_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8354,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_BloomFilter_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8392,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_BloomFilter_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8575,      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.BloomFilter)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8583,    // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.BloomFilter)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8589,    // repeated .org.apache.hadoop.hive.ql.io.orc.BloomFilter bloomFilter = 1;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8591,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.BloomFilter bloomFilter = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8596,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.BloomFilter bloomFilter = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8600,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.BloomFilter bloomFilter = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8604,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.BloomFilter bloomFilter = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8609,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.BloomFilter bloomFilter = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8615,   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.BloomFilterIndex}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8690,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_BloomFilterIndex_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8695,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_BloomFilterIndex_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8715,    // repeated .org.apache.hadoop.hive.ql.io.orc.BloomFilter bloomFilter = 1;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8719,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.BloomFilter bloomFilter = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8725,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.BloomFilter bloomFilter = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8732,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.BloomFilter bloomFilter = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8738,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.BloomFilter bloomFilter = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8744,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.BloomFilter bloomFilter = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8861,     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.BloomFilterIndex}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8868,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_BloomFilterIndex_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8873,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_BloomFilterIndex_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8914,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_BloomFilterIndex_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9009,      // repeated .org.apache.hadoop.hive.ql.io.orc.BloomFilter bloomFilter = 1;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9023,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.BloomFilter bloomFilter = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9033,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.BloomFilter bloomFilter = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9043,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.BloomFilter bloomFilter = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9053,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.BloomFilter bloomFilter = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9070,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.BloomFilter bloomFilter = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9084,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.BloomFilter bloomFilter = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9100,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.BloomFilter bloomFilter = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9117,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.BloomFilter bloomFilter = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9131,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.BloomFilter bloomFilter = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9145,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.BloomFilter bloomFilter = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9159,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.BloomFilter bloomFilter = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9172,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.BloomFilter bloomFilter = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9185,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.BloomFilter bloomFilter = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9192,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.BloomFilter bloomFilter = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9202,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.BloomFilter bloomFilter = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9213,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.BloomFilter bloomFilter = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9220,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.BloomFilter bloomFilter = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9228,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.BloomFilter bloomFilter = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9249,      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.BloomFilterIndex)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9257,    // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.BloomFilterIndex)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9263,    // required .org.apache.hadoop.hive.ql.io.orc.Stream.Kind kind = 1;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9265,     * <code>required .org.apache.hadoop.hive.ql.io.orc.Stream.Kind kind = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9269,     * <code>required .org.apache.hadoop.hive.ql.io.orc.Stream.Kind kind = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9294,   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.Stream}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9379,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Stream_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9384,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Stream_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9405,     * Protobuf enum {@code org.apache.hadoop.hive.ql.io.orc.Stream.Kind}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9542,      // @@protoc_insertion_point(enum_scope:org.apache.hadoop.hive.ql.io.orc.Stream.Kind)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9546,    // required .org.apache.hadoop.hive.ql.io.orc.Stream.Kind kind = 1;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9550,     * <code>required .org.apache.hadoop.hive.ql.io.orc.Stream.Kind kind = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9556,     * <code>required .org.apache.hadoop.hive.ql.io.orc.Stream.Kind kind = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9604,      if (!hasKind()) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9605,        memoizedIsInitialized = 0;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9606,        return false;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9607,      }
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9724,     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.Stream}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9731,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Stream_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9736,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Stream_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9776,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Stream_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9837,        if (!hasKind()) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9838,
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9839,          return false;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9840,        }
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9863,      // required .org.apache.hadoop.hive.ql.io.orc.Stream.Kind kind = 1;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9866,       * <code>required .org.apache.hadoop.hive.ql.io.orc.Stream.Kind kind = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9872,       * <code>required .org.apache.hadoop.hive.ql.io.orc.Stream.Kind kind = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9878,       * <code>required .org.apache.hadoop.hive.ql.io.orc.Stream.Kind kind = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9890,       * <code>required .org.apache.hadoop.hive.ql.io.orc.Stream.Kind kind = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9965,      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.Stream)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9973,    // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.Stream)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9979,    // required .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding.Kind kind = 1;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9981,     * <code>required .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding.Kind kind = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9985,     * <code>required .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding.Kind kind = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10000,   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.ColumnEncoding}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10080,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnEncoding_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10085,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnEncoding_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10106,     * Protobuf enum {@code org.apache.hadoop.hive.ql.io.orc.ColumnEncoding.Kind}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10202,      // @@protoc_insertion_point(enum_scope:org.apache.hadoop.hive.ql.io.orc.ColumnEncoding.Kind)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10206,    // required .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding.Kind kind = 1;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10210,     * <code>required .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding.Kind kind = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10216,     * <code>required .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding.Kind kind = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10247,      if (!hasKind()) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10248,        memoizedIsInitialized = 0;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10249,        return false;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10250,      }
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10360,     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.ColumnEncoding}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10367,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnEncoding_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10372,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnEncoding_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10410,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnEncoding_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10464,        if (!hasKind()) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10465,
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10466,          return false;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10467,        }
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10490,      // required .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding.Kind kind = 1;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10493,       * <code>required .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding.Kind kind = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10499,       * <code>required .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding.Kind kind = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10505,       * <code>required .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding.Kind kind = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10517,       * <code>required .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding.Kind kind = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10559,      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.ColumnEncoding)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10567,    // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.ColumnEncoding)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10573,    // repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10575,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10580,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10584,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10588,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10593,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10598,    // repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10600,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10605,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10609,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10613,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10618,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10624,   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.StripeFooter}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10710,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeFooter_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10715,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeFooter_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10735,    // repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10739,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10745,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10752,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10758,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10764,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10771,    // repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10775,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10781,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10788,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10794,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10800,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10816,      for (int i = 0; i < getStreamsCount(); i++) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10817,        if (!getStreams(i).isInitialized()) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10818,          memoizedIsInitialized = 0;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10819,          return false;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10820,        }
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10821,      }
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10822,      for (int i = 0; i < getColumnsCount(); i++) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10823,        if (!getColumns(i).isInitialized()) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10824,          memoizedIsInitialized = 0;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10825,          return false;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10826,        }
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10827,      }
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10937,     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.StripeFooter}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10944,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeFooter_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10949,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeFooter_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10997,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeFooter_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11105,        for (int i = 0; i < getStreamsCount(); i++) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11106,          if (!getStreams(i).isInitialized()) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11107,
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11108,            return false;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11109,          }
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11110,        }
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11111,        for (int i = 0; i < getColumnsCount(); i++) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11112,          if (!getColumns(i).isInitialized()) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11113,
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11114,            return false;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11115,          }
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11116,        }
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11139,      // repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11153,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11163,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11173,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11183,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11200,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11214,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11230,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11247,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11261,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11275,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11289,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11302,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11315,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11322,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11332,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11343,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11350,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11358,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11379,      // repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11393,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11403,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11413,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11423,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11440,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11454,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11470,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11487,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11501,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11515,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11529,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11542,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11555,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11562,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11572,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11583,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11590,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11598,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11619,      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.StripeFooter)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11627,    // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.StripeFooter)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11633,    // required .org.apache.hadoop.hive.ql.io.orc.Type.Kind kind = 1;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11635,     * <code>required .org.apache.hadoop.hive.ql.io.orc.Type.Kind kind = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11639,     * <code>required .org.apache.hadoop.hive.ql.io.orc.Type.Kind kind = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11708,   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.Type}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11833,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Type_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11838,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Type_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11859,     * Protobuf enum {@code org.apache.hadoop.hive.ql.io.orc.Type.Kind}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12081,      // @@protoc_insertion_point(enum_scope:org.apache.hadoop.hive.ql.io.orc.Type.Kind)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12085,    // required .org.apache.hadoop.hive.ql.io.orc.Type.Kind kind = 1;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12089,     * <code>required .org.apache.hadoop.hive.ql.io.orc.Type.Kind kind = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12095,     * <code>required .org.apache.hadoop.hive.ql.io.orc.Type.Kind kind = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12216,      if (!hasKind()) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12217,        memoizedIsInitialized = 0;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12218,        return false;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12219,      }
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12376,     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.Type}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12383,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Type_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12388,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Type_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12434,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Type_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12533,        if (!hasKind()) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12534,
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12535,          return false;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12536,        }
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12559,      // required .org.apache.hadoop.hive.ql.io.orc.Type.Kind kind = 1;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12562,       * <code>required .org.apache.hadoop.hive.ql.io.orc.Type.Kind kind = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12568,       * <code>required .org.apache.hadoop.hive.ql.io.orc.Type.Kind kind = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12574,       * <code>required .org.apache.hadoop.hive.ql.io.orc.Type.Kind kind = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12586,       * <code>required .org.apache.hadoop.hive.ql.io.orc.Type.Kind kind = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12853,      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.Type)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12861,    // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.Type)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12918,   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.StripeInformation}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13007,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeInformation_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13012,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeInformation_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13255,     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.StripeInformation}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13262,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeInformation_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13267,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeInformation_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13311,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeInformation_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13573,      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.StripeInformation)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13581,    // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.StripeInformation)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13587,    // required string name = 1;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13589,     * <code>required string name = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13593,     * <code>required string name = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13597,     * <code>required string name = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13602,    // required bytes value = 2;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13604,     * <code>required bytes value = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13608,     * <code>required bytes value = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13613,   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.UserMetadataItem}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13687,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_UserMetadataItem_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13692,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_UserMetadataItem_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13713,    // required string name = 1;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13717,     * <code>required string name = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13723,     * <code>required string name = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13740,     * <code>required string name = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13756,    // required bytes value = 2;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13760,     * <code>required bytes value = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13766,     * <code>required bytes value = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13781,      if (!hasName()) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13782,        memoizedIsInitialized = 0;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13783,        return false;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13784,      }
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13785,      if (!hasValue()) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13786,        memoizedIsInitialized = 0;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13787,        return false;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13788,      }
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13898,     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.UserMetadataItem}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13905,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_UserMetadataItem_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13910,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_UserMetadataItem_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13948,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_UserMetadataItem_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14004,        if (!hasName()) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14005,
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14006,          return false;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14007,        }
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14008,        if (!hasValue()) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14009,
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14010,          return false;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14011,        }
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14034,      // required string name = 1;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14037,       * <code>required string name = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14043,       * <code>required string name = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14057,       * <code>required string name = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14073,       * <code>required string name = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14086,       * <code>required string name = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14095,       * <code>required string name = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14108,      // required bytes value = 2;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14111,       * <code>required bytes value = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14117,       * <code>required bytes value = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14123,       * <code>required bytes value = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14135,       * <code>required bytes value = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14144,      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.UserMetadataItem)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14152,    // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.UserMetadataItem)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14158,    // repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics colStats = 1;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14160,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics colStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14165,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics colStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14169,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics colStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14173,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics colStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14178,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics colStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14184,   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.StripeStatistics}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14259,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14264,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14284,    // repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics colStats = 1;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14288,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics colStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14294,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics colStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14301,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics colStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14307,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics colStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14313,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics colStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14430,     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.StripeStatistics}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14437,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14442,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14483,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14578,      // repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics colStats = 1;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14592,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics colStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14602,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics colStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14612,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics colStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14622,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics colStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14639,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics colStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14653,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics colStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14669,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics colStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14686,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics colStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14700,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics colStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14714,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics colStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14728,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics colStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14741,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics colStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14754,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics colStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14761,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics colStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14771,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics colStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14782,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics colStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14789,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics colStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14797,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics colStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14818,      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.StripeStatistics)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14826,    // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.StripeStatistics)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14832,    // repeated .org.apache.hadoop.hive.ql.io.orc.StripeStatistics stripeStats = 1;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14834,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeStatistics stripeStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14839,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeStatistics stripeStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14843,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeStatistics stripeStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14847,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeStatistics stripeStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14852,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeStatistics stripeStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14858,   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.Metadata}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14933,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Metadata_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14938,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Metadata_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14958,    // repeated .org.apache.hadoop.hive.ql.io.orc.StripeStatistics stripeStats = 1;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14962,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeStatistics stripeStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14968,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeStatistics stripeStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14975,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeStatistics stripeStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14981,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeStatistics stripeStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14987,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeStatistics stripeStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15104,     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.Metadata}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15111,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Metadata_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15116,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Metadata_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15157,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Metadata_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15252,      // repeated .org.apache.hadoop.hive.ql.io.orc.StripeStatistics stripeStats = 1;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15266,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeStatistics stripeStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15276,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeStatistics stripeStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15286,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeStatistics stripeStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15296,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeStatistics stripeStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15313,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeStatistics stripeStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15327,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeStatistics stripeStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15343,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeStatistics stripeStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15360,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeStatistics stripeStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15374,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeStatistics stripeStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15388,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeStatistics stripeStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15402,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeStatistics stripeStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15415,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeStatistics stripeStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15428,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeStatistics stripeStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15435,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeStatistics stripeStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15445,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeStatistics stripeStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15456,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeStatistics stripeStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15463,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeStatistics stripeStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15471,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeStatistics stripeStats = 1;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15492,      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.Metadata)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15500,    // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.Metadata)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15526,    // repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15528,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15533,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15537,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15541,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15546,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15551,    // repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15553,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15558,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15562,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15566,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15571,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15576,    // repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15578,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15583,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15587,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15591,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15596,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15611,    // repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15613,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15618,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15622,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15626,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15631,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15647,   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.Footer}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15775,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Footer_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15780,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Footer_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15833,    // repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15837,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15843,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15850,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15856,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15862,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15869,    // repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15873,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15879,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15886,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15892,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15898,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15905,    // repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15909,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15915,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15922,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15928,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15934,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15957,    // repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15961,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15967,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15974,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15980,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15986,     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16024,      for (int i = 0; i < getTypesCount(); i++) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16025,        if (!getTypes(i).isInitialized()) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16026,          memoizedIsInitialized = 0;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16027,          return false;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16028,        }
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16029,      }
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16030,      for (int i = 0; i < getMetadataCount(); i++) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16031,        if (!getMetadata(i).isInitialized()) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16032,          memoizedIsInitialized = 0;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16033,          return false;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16034,        }
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16035,      }
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16187,     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.Footer}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16194,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Footer_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16199,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Footer_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16269,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Footer_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16477,        for (int i = 0; i < getTypesCount(); i++) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16478,          if (!getTypes(i).isInitialized()) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16479,
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16480,            return false;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16481,          }
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16482,        }
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16483,        for (int i = 0; i < getMetadataCount(); i++) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16484,          if (!getMetadata(i).isInitialized()) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16485,
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16486,            return false;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16487,          }
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16488,        }
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16577,      // repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16591,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16601,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16611,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16621,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16638,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16652,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16668,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16685,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16699,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16713,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16727,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16740,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16753,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16760,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16770,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16781,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16788,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16796,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16817,      // repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16831,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16841,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16851,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16861,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16878,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16892,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16908,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16925,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16939,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16953,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16967,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16980,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16993,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17000,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17010,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17021,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17028,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17036,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17057,      // repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17071,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17081,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17091,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17101,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17118,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17132,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17148,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17165,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17179,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17193,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17207,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17220,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17233,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17240,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17250,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17261,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17268,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17276,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17330,      // repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17344,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17354,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17364,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17374,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17391,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17405,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17421,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17438,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17452,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17466,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17480,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17493,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17506,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17513,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17523,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17534,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17541,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17549,       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17603,      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.Footer)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17611,    // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.Footer)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17627,    // optional .org.apache.hadoop.hive.ql.io.orc.CompressionKind compression = 2;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17629,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.CompressionKind compression = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17633,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.CompressionKind compression = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17739,   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.PostScript}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17867,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_PostScript_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17872,      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_PostScript_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17909,    // optional .org.apache.hadoop.hive.ql.io.orc.CompressionKind compression = 2;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17913,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.CompressionKind compression = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17919,     * <code>optional .org.apache.hadoop.hive.ql.io.orc.CompressionKind compression = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18254,     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.PostScript}
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18265,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_PostScript_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18270,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_PostScript_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18318,        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_PostScript_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18472,      // optional .org.apache.hadoop.hive.ql.io.orc.CompressionKind compression = 2;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18475,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.CompressionKind compression = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18481,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.CompressionKind compression = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18487,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.CompressionKind compression = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18499,       * <code>optional .org.apache.hadoop.hive.ql.io.orc.CompressionKind compression = 2;</code>
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18837,      // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.PostScript)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18845,    // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.PostScript)
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18849,    internal_static_org_apache_hadoop_hive_ql_io_orc_IntegerStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18852,      internal_static_org_apache_hadoop_hive_ql_io_orc_IntegerStatistics_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18854,    internal_static_org_apache_hadoop_hive_ql_io_orc_DoubleStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18857,      internal_static_org_apache_hadoop_hive_ql_io_orc_DoubleStatistics_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18859,    internal_static_org_apache_hadoop_hive_ql_io_orc_StringStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18862,      internal_static_org_apache_hadoop_hive_ql_io_orc_StringStatistics_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18864,    internal_static_org_apache_hadoop_hive_ql_io_orc_BucketStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18867,      internal_static_org_apache_hadoop_hive_ql_io_orc_BucketStatistics_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18869,    internal_static_org_apache_hadoop_hive_ql_io_orc_DecimalStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18872,      internal_static_org_apache_hadoop_hive_ql_io_orc_DecimalStatistics_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18874,    internal_static_org_apache_hadoop_hive_ql_io_orc_DateStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18877,      internal_static_org_apache_hadoop_hive_ql_io_orc_DateStatistics_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18879,    internal_static_org_apache_hadoop_hive_ql_io_orc_TimestampStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18882,      internal_static_org_apache_hadoop_hive_ql_io_orc_TimestampStatistics_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18884,    internal_static_org_apache_hadoop_hive_ql_io_orc_BinaryStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18887,      internal_static_org_apache_hadoop_hive_ql_io_orc_BinaryStatistics_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18889,    internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18892,      internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnStatistics_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18894,    internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndexEntry_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18897,      internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndexEntry_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18899,    internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndex_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18902,      internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndex_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18904,    internal_static_org_apache_hadoop_hive_ql_io_orc_BloomFilter_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18907,      internal_static_org_apache_hadoop_hive_ql_io_orc_BloomFilter_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18909,    internal_static_org_apache_hadoop_hive_ql_io_orc_BloomFilterIndex_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18912,      internal_static_org_apache_hadoop_hive_ql_io_orc_BloomFilterIndex_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18914,    internal_static_org_apache_hadoop_hive_ql_io_orc_Stream_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18917,      internal_static_org_apache_hadoop_hive_ql_io_orc_Stream_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18919,    internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnEncoding_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18922,      internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnEncoding_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18924,    internal_static_org_apache_hadoop_hive_ql_io_orc_StripeFooter_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18927,      internal_static_org_apache_hadoop_hive_ql_io_orc_StripeFooter_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18929,    internal_static_org_apache_hadoop_hive_ql_io_orc_Type_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18932,      internal_static_org_apache_hadoop_hive_ql_io_orc_Type_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18934,    internal_static_org_apache_hadoop_hive_ql_io_orc_StripeInformation_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18937,      internal_static_org_apache_hadoop_hive_ql_io_orc_StripeInformation_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18939,    internal_static_org_apache_hadoop_hive_ql_io_orc_UserMetadataItem_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18942,      internal_static_org_apache_hadoop_hive_ql_io_orc_UserMetadataItem_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18944,    internal_static_org_apache_hadoop_hive_ql_io_orc_StripeStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18947,      internal_static_org_apache_hadoop_hive_ql_io_orc_StripeStatistics_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18949,    internal_static_org_apache_hadoop_hive_ql_io_orc_Metadata_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18952,      internal_static_org_apache_hadoop_hive_ql_io_orc_Metadata_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18954,    internal_static_org_apache_hadoop_hive_ql_io_orc_Footer_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18957,      internal_static_org_apache_hadoop_hive_ql_io_orc_Footer_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18959,    internal_static_org_apache_hadoop_hive_ql_io_orc_PostScript_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18962,      internal_static_org_apache_hadoop_hive_ql_io_orc_PostScript_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18972,"      ""\n\017orc_proto.proto\022 org.apache.hadoop.hiv"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18973,"      ""e.ql.io.orc\""B\n\021IntegerStatistics\022\017\n\007mini"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18974,"      ""mum\030\001 \001(\022\022\017\n\007maximum\030\002 \001(\022\022\013\n\003sum\030\003 \001(\022\"""" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18975,"      ""A\n\020DoubleStatistics\022\017\n\007minimum\030\001 \001(\001\022\017\n\007"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18976,"      ""maximum\030\002 \001(\001\022\013\n\003sum\030\003 \001(\001\""A\n\020StringStat"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18977,"      ""istics\022\017\n\007minimum\030\001 \001(\t\022\017\n\007maximum\030\002 \001(\t"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18978,"      ""\022\013\n\003sum\030\003 \001(\022\""%\n\020BucketStatistics\022\021\n\005cou"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18979,"      ""nt\030\001 \003(\004B\002\020\001\""B\n\021DecimalStatistics\022\017\n\007min"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18980,"      ""imum\030\001 \001(\t\022\017\n\007maximum\030\002 \001(\t\022\013\n\003sum\030\003 \001(\t"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18981,"      ""\""2\n\016DateStatistics\022\017\n\007minimum\030\001 \001(\021\022\017\n\007m"","
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18982,"      ""aximum\030\002 \001(\021\""7\n\023TimestampStatistics\022\017\n\007m"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18983,"      ""inimum\030\001 \001(\022\022\017\n\007maximum\030\002 \001(\022\""\037\n\020BinaryS"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18984,"      ""tatistics\022\013\n\003sum\030\001 \001(\022\""\255\005\n\020ColumnStatist"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18985,"      ""ics\022\026\n\016numberOfValues\030\001 \001(\004\022J\n\rintStatis"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18986,"      ""tics\030\002 \001(\01323.org.apache.hadoop.hive.ql.i"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18987,"      ""o.orc.IntegerStatistics\022L\n\020doubleStatist"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18988,"      ""ics\030\003 \001(\01322.org.apache.hadoop.hive.ql.io"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18989,"      "".orc.DoubleStatistics\022L\n\020stringStatistic"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18990,"      ""s\030\004 \001(\01322.org.apache.hadoop.hive.ql.io.o"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18991,"      ""rc.StringStatistics\022L\n\020bucketStatistics\030"","
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18992,"      ""\005 \001(\01322.org.apache.hadoop.hive.ql.io.orc"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18993,"      "".BucketStatistics\022N\n\021decimalStatistics\030\006"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18994,"      "" \001(\01323.org.apache.hadoop.hive.ql.io.orc."" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18995,"      ""DecimalStatistics\022H\n\016dateStatistics\030\007 \001("" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18996,"      ""\01320.org.apache.hadoop.hive.ql.io.orc.Dat"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18997,"      ""eStatistics\022L\n\020binaryStatistics\030\010 \001(\01322."" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18998,"      ""org.apache.hadoop.hive.ql.io.orc.BinaryS"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18999,"      ""tatistics\022R\n\023timestampStatistics\030\t \001(\01325"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19000,"      "".org.apache.hadoop.hive.ql.io.orc.Timest"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19001,"      ""ampStatistics\022\017\n\007hasNull\030\n \001(\010\""n\n\rRowInd"","
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19002,"      ""exEntry\022\025\n\tpositions\030\001 \003(\004B\002\020\001\022F\n\nstatis"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19003,"      ""tics\030\002 \001(\01322.org.apache.hadoop.hive.ql.i"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19004,"      ""o.orc.ColumnStatistics\""J\n\010RowIndex\022>\n\005en"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19005,"      ""try\030\001 \003(\0132/.org.apache.hadoop.hive.ql.io"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19006,"      "".orc.RowIndexEntry\""7\n\013BloomFilter\022\030\n\020num"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19007,"      ""HashFunctions\030\001 \001(\r\022\016\n\006bitset\030\002 \003(\006\""V\n\020B"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19008,"      ""loomFilterIndex\022B\n\013bloomFilter\030\001 \003(\0132-.o"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19009,"      ""rg.apache.hadoop.hive.ql.io.orc.BloomFil"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19010,"      ""ter\""\354\001\n\006Stream\022;\n\004kind\030\001 \002(\0162-.org.apach"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19011,"      ""e.hadoop.hive.ql.io.orc.Stream.Kind\022\016\n\006c"","
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19012,"      ""olumn\030\002 \001(\r\022\016\n\006length\030\003 \001(\004\""\204\001\n\004Kind\022\013\n\007"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19013,"      ""PRESENT\020\000\022\010\n\004DATA\020\001\022\n\n\006LENGTH\020\002\022\023\n\017DICTI"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19014,"      ""ONARY_DATA\020\003\022\024\n\020DICTIONARY_COUNT\020\004\022\r\n\tSE"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19015,"      ""CONDARY\020\005\022\r\n\tROW_INDEX\020\006\022\020\n\014BLOOM_FILTER"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19016,"      ""\020\007\""\263\001\n\016ColumnEncoding\022C\n\004kind\030\001 \002(\01625.or"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19017,"      ""g.apache.hadoop.hive.ql.io.orc.ColumnEnc"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19018,"      ""oding.Kind\022\026\n\016dictionarySize\030\002 \001(\r\""D\n\004Ki"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19019,"      ""nd\022\n\n\006DIRECT\020\000\022\016\n\nDICTIONARY\020\001\022\r\n\tDIRECT"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19020,"      ""_V2\020\002\022\021\n\rDICTIONARY_V2\020\003\""\214\001\n\014StripeFoote"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19021,"      ""r\0229\n\007streams\030\001 \003(\0132(.org.apache.hadoop.h"","
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19022,"      ""ive.ql.io.orc.Stream\022A\n\007columns\030\002 \003(\01320."" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19023,"      ""org.apache.hadoop.hive.ql.io.orc.ColumnE"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19024,"      ""ncoding\""\370\002\n\004Type\0229\n\004kind\030\001 \002(\0162+.org.apa"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19025,"      ""che.hadoop.hive.ql.io.orc.Type.Kind\022\024\n\010s"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19026,"      ""ubtypes\030\002 \003(\rB\002\020\001\022\022\n\nfieldNames\030\003 \003(\t\022\025\n"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19027,"      ""\rmaximumLength\030\004 \001(\r\022\021\n\tprecision\030\005 \001(\r\022"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19028,"      ""\r\n\005scale\030\006 \001(\r\""\321\001\n\004Kind\022\013\n\007BOOLEAN\020\000\022\010\n\004"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19029,"      ""BYTE\020\001\022\t\n\005SHORT\020\002\022\007\n\003INT\020\003\022\010\n\004LONG\020\004\022\t\n\005"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19030,"      ""FLOAT\020\005\022\n\n\006DOUBLE\020\006\022\n\n\006STRING\020\007\022\n\n\006BINAR"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19031,"      ""Y\020\010\022\r\n\tTIMESTAMP\020\t\022\010\n\004LIST\020\n\022\007\n\003MAP\020\013\022\n\n"","
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19032,"      ""\006STRUCT\020\014\022\t\n\005UNION\020\r\022\013\n\007DECIMAL\020\016\022\010\n\004DAT"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19033,"      ""E\020\017\022\013\n\007VARCHAR\020\020\022\010\n\004CHAR\020\021\""x\n\021StripeInfo"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19034,"      ""rmation\022\016\n\006offset\030\001 \001(\004\022\023\n\013indexLength\030\002"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19035,"      "" \001(\004\022\022\n\ndataLength\030\003 \001(\004\022\024\n\014footerLength"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19036,"      ""\030\004 \001(\004\022\024\n\014numberOfRows\030\005 \001(\004\""/\n\020UserMeta"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19037,"      ""dataItem\022\014\n\004name\030\001 \002(\t\022\r\n\005value\030\002 \002(\014\""X\n"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19038,"      ""\020StripeStatistics\022D\n\010colStats\030\001 \003(\01322.or"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19039,"      ""g.apache.hadoop.hive.ql.io.orc.ColumnSta"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19040,"      ""tistics\""S\n\010Metadata\022G\n\013stripeStats\030\001 \003(\013"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19041,"      ""22.org.apache.hadoop.hive.ql.io.orc.Stri"","
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19042,"      ""peStatistics\""\356\002\n\006Footer\022\024\n\014headerLength\030"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19043,"      ""\001 \001(\004\022\025\n\rcontentLength\030\002 \001(\004\022D\n\007stripes\030"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19044,"      ""\003 \003(\01323.org.apache.hadoop.hive.ql.io.orc"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19045,"      "".StripeInformation\0225\n\005types\030\004 \003(\0132&.org."" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19046,"      ""apache.hadoop.hive.ql.io.orc.Type\022D\n\010met"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19047,"      ""adata\030\005 \003(\01322.org.apache.hadoop.hive.ql."" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19048,"      ""io.orc.UserMetadataItem\022\024\n\014numberOfRows\030"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19049,"      ""\006 \001(\004\022F\n\nstatistics\030\007 \003(\01322.org.apache.h"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19050,"      ""adoop.hive.ql.io.orc.ColumnStatistics\022\026\n"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19051,"      ""\016rowIndexStride\030\010 \001(\r\""\334\001\n\nPostScript\022\024\n\014"","
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19052,"      ""footerLength\030\001 \001(\004\022F\n\013compression\030\002 \001(\0162"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19053,"      ""1.org.apache.hadoop.hive.ql.io.orc.Compr"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19054,"      ""essionKind\022\034\n\024compressionBlockSize\030\003 \001(\004"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19055,"      ""\022\023\n\007version\030\004 \003(\rB\002\020\001\022\026\n\016metadataLength\030"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19056,"      ""\005 \001(\004\022\025\n\rwriterVersion\030\006 \001(\r\022\016\n\005magic\030\300>"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19057,"      "" \001(\t*:\n\017CompressionKind\022\010\n\004NONE\020\000\022\010\n\004ZLI"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19058,"      ""B\020\001\022\n\n\006SNAPPY\020\002\022\007\n\003LZO\020\003"""
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19065,          internal_static_org_apache_hadoop_hive_ql_io_orc_IntegerStatistics_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19067,          internal_static_org_apache_hadoop_hive_ql_io_orc_IntegerStatistics_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19069,"              internal_static_org_apache_hadoop_hive_ql_io_orc_IntegerStatistics_descriptor,"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19071,          internal_static_org_apache_hadoop_hive_ql_io_orc_DoubleStatistics_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19073,          internal_static_org_apache_hadoop_hive_ql_io_orc_DoubleStatistics_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19075,"              internal_static_org_apache_hadoop_hive_ql_io_orc_DoubleStatistics_descriptor,"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19077,          internal_static_org_apache_hadoop_hive_ql_io_orc_StringStatistics_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19079,          internal_static_org_apache_hadoop_hive_ql_io_orc_StringStatistics_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19081,"              internal_static_org_apache_hadoop_hive_ql_io_orc_StringStatistics_descriptor,"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19083,          internal_static_org_apache_hadoop_hive_ql_io_orc_BucketStatistics_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19085,          internal_static_org_apache_hadoop_hive_ql_io_orc_BucketStatistics_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19087,"              internal_static_org_apache_hadoop_hive_ql_io_orc_BucketStatistics_descriptor,"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19089,          internal_static_org_apache_hadoop_hive_ql_io_orc_DecimalStatistics_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19091,          internal_static_org_apache_hadoop_hive_ql_io_orc_DecimalStatistics_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19093,"              internal_static_org_apache_hadoop_hive_ql_io_orc_DecimalStatistics_descriptor,"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19095,          internal_static_org_apache_hadoop_hive_ql_io_orc_DateStatistics_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19097,          internal_static_org_apache_hadoop_hive_ql_io_orc_DateStatistics_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19099,"              internal_static_org_apache_hadoop_hive_ql_io_orc_DateStatistics_descriptor,"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19101,          internal_static_org_apache_hadoop_hive_ql_io_orc_TimestampStatistics_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19103,          internal_static_org_apache_hadoop_hive_ql_io_orc_TimestampStatistics_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19105,"              internal_static_org_apache_hadoop_hive_ql_io_orc_TimestampStatistics_descriptor,"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19107,          internal_static_org_apache_hadoop_hive_ql_io_orc_BinaryStatistics_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19109,          internal_static_org_apache_hadoop_hive_ql_io_orc_BinaryStatistics_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19111,"              internal_static_org_apache_hadoop_hive_ql_io_orc_BinaryStatistics_descriptor,"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19113,          internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnStatistics_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19115,          internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnStatistics_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19117,"              internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnStatistics_descriptor,"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19119,          internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndexEntry_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19121,          internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndexEntry_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19123,"              internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndexEntry_descriptor,"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19125,          internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndex_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19127,          internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndex_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19129,"              internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndex_descriptor,"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19131,          internal_static_org_apache_hadoop_hive_ql_io_orc_BloomFilter_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19133,          internal_static_org_apache_hadoop_hive_ql_io_orc_BloomFilter_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19135,"              internal_static_org_apache_hadoop_hive_ql_io_orc_BloomFilter_descriptor,"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19137,          internal_static_org_apache_hadoop_hive_ql_io_orc_BloomFilterIndex_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19139,          internal_static_org_apache_hadoop_hive_ql_io_orc_BloomFilterIndex_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19141,"              internal_static_org_apache_hadoop_hive_ql_io_orc_BloomFilterIndex_descriptor,"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19143,          internal_static_org_apache_hadoop_hive_ql_io_orc_Stream_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19145,          internal_static_org_apache_hadoop_hive_ql_io_orc_Stream_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19147,"              internal_static_org_apache_hadoop_hive_ql_io_orc_Stream_descriptor,"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19149,          internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnEncoding_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19151,          internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnEncoding_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19153,"              internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnEncoding_descriptor,"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19155,          internal_static_org_apache_hadoop_hive_ql_io_orc_StripeFooter_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19157,          internal_static_org_apache_hadoop_hive_ql_io_orc_StripeFooter_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19159,"              internal_static_org_apache_hadoop_hive_ql_io_orc_StripeFooter_descriptor,"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19161,          internal_static_org_apache_hadoop_hive_ql_io_orc_Type_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19163,          internal_static_org_apache_hadoop_hive_ql_io_orc_Type_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19165,"              internal_static_org_apache_hadoop_hive_ql_io_orc_Type_descriptor,"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19167,          internal_static_org_apache_hadoop_hive_ql_io_orc_StripeInformation_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19169,          internal_static_org_apache_hadoop_hive_ql_io_orc_StripeInformation_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19171,"              internal_static_org_apache_hadoop_hive_ql_io_orc_StripeInformation_descriptor,"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19173,          internal_static_org_apache_hadoop_hive_ql_io_orc_UserMetadataItem_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19175,          internal_static_org_apache_hadoop_hive_ql_io_orc_UserMetadataItem_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19177,"              internal_static_org_apache_hadoop_hive_ql_io_orc_UserMetadataItem_descriptor,"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19179,          internal_static_org_apache_hadoop_hive_ql_io_orc_StripeStatistics_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19181,          internal_static_org_apache_hadoop_hive_ql_io_orc_StripeStatistics_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19183,"              internal_static_org_apache_hadoop_hive_ql_io_orc_StripeStatistics_descriptor,"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19185,          internal_static_org_apache_hadoop_hive_ql_io_orc_Metadata_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19187,          internal_static_org_apache_hadoop_hive_ql_io_orc_Metadata_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19189,"              internal_static_org_apache_hadoop_hive_ql_io_orc_Metadata_descriptor,"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19191,          internal_static_org_apache_hadoop_hive_ql_io_orc_Footer_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19193,          internal_static_org_apache_hadoop_hive_ql_io_orc_Footer_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19195,"              internal_static_org_apache_hadoop_hive_ql_io_orc_Footer_descriptor,"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19197,          internal_static_org_apache_hadoop_hive_ql_io_orc_PostScript_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19199,          internal_static_org_apache_hadoop_hive_ql_io_orc_PostScript_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19201,"              internal_static_org_apache_hadoop_hive_ql_io_orc_PostScript_descriptor,"
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,869,"      for (Map.Entry<String, ColumnInfo> lEntry : leftmap.entrySet()) {"
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,870,        String field = lEntry.getKey();
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,872,        ColumnInfo rInfo = rightmap.get(field);
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,873,        if (rInfo == null) {
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,874,"          throw new SemanticException(SemanticAnalyzer.generateErrorMessage(tabref,"
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,875,"              ""Schema of both sides of union should match. "" + rightalias"
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,876,"                  + "" does not have the field "" + field));"
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,877,        }
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,878,        if (lInfo == null) {
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,879,"          throw new SemanticException(SemanticAnalyzer.generateErrorMessage(tabref,"
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,880,"              ""Schema of both sides of union should match. "" + leftalias"
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,881,"                  + "" does not have the field "" + field));"
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,882,        }
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,883,        if (!lInfo.getInternalName().equals(rInfo.getInternalName())) {
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,884,          throw new CalciteSemanticException(SemanticAnalyzer.generateErrorMessage(
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,885,"              tabref,"
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,886,"              ""Schema of both sides of union should match: field "" + field + "":"""
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,887,"                  + "" appears on the left side of the UNION at column position: """
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,888,                  + SemanticAnalyzer.getPositionFromInternalName(lInfo.getInternalName())
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,889,"                  + "", and on the right side of the UNION at column position: """
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,890,                  + SemanticAnalyzer.getPositionFromInternalName(rInfo.getInternalName())
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,891,"                  + "". Column positions should match for a UNION""));"
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,892,        }
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,893,"        // try widening coversion, otherwise fail union"
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,897,"          throw new CalciteSemanticException(SemanticAnalyzer.generateErrorMessage(tabref,"
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,898,"              ""Schema of both sides of union should match: Column "" + field + "" is of type """
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,899,"                  + lInfo.getType().getTypeName() + "" on first table and type """
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,900,"                  + rInfo.getType().getTypeName() + "" on second table""));"
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,902,      }
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,903,
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,904,      // 3. construct Union Output RR using original left & right Input
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,905,      RowResolver unionoutRR = new RowResolver();
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,906,"      for (Map.Entry<String, ColumnInfo> lEntry : leftmap.entrySet()) {"
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,907,        String field = lEntry.getKey();
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,908,        ColumnInfo lInfo = lEntry.getValue();
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,909,        ColumnInfo rInfo = rightmap.get(field);
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,911,        unionColInfo.setTabAlias(unionalias);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,560,"      sb.append(""The abstract syntax tree is null"");"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8944,"    for (Map.Entry<String, ColumnInfo> lEntry : leftmap.entrySet()) {"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8945,      String field = lEntry.getKey();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8947,      ColumnInfo rInfo = rightmap.get(field);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8948,      if (rInfo == null) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8949,"        throw new SemanticException(generateErrorMessage(tabref,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8950,"            ""Schema of both sides of union should match. "" + rightalias"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8951,"                + "" does not have the field "" + field));"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8952,      }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8953,      if (lInfo == null) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8954,"        throw new SemanticException(generateErrorMessage(tabref,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8955,"            ""Schema of both sides of union should match. "" + leftalias"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8956,"                + "" does not have the field "" + field));"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8957,      }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8958,      if (!lInfo.getInternalName().equals(rInfo.getInternalName())) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8959,"        throw new SemanticException(generateErrorMessage(tabref,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8960,"            ""Schema of both sides of union should match: field "" + field + "":"""
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8961,"                + "" appears on the left side of the UNION at column position: "" +"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8962,                getPositionFromInternalName(lInfo.getInternalName())
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8963,"                + "", and on the right side of the UNION at column position: "" +"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8964,                getPositionFromInternalName(rInfo.getInternalName())
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8965,"                + "". Column positions should match for a UNION""));"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8966,      }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8967,"      // try widening coversion, otherwise fail union"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8977,    }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8978,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8979,    // construct the forward operator
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8980,    RowResolver unionoutRR = new RowResolver();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8981,"    for (Map.Entry<String, ColumnInfo> lEntry : leftmap.entrySet()) {"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8982,      String field = lEntry.getKey();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8983,      ColumnInfo lInfo = lEntry.getValue();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8984,      ColumnInfo rInfo = rightmap.get(field);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,9112,"    for (Map.Entry<String, ColumnInfo> unionEntry : unionoutRR.getFieldMap(unionalias).entrySet()) {"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,9113,      String field = unionEntry.getKey();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,9114,      ColumnInfo lInfo = origInputFieldMap.get(field);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,9115,"      ExprNodeDesc column = new ExprNodeColumnDesc(lInfo.getType(), lInfo.getInternalName(),"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,9116,"          lInfo.getTabAlias(), lInfo.getIsVirtualCol(), lInfo.isSkewedCol());"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,9117,      if (!lInfo.getType().equals(unionEntry.getValue().getType())) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,9120,"            column, (PrimitiveTypeInfo)unionEntry.getValue().getType());"
ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java,396,      if (onepath.toUri().relativize(fpath.toUri()).equals(fpath.toUri())) {
beeline/src/java/org/apache/hive/beeline/Commands.java,779,    line = line.trim();
beeline/src/java/org/apache/hive/beeline/Commands.java,780,"    if (line.endsWith("";"")) {"
beeline/src/java/org/apache/hive/beeline/Commands.java,781,"      line = line.substring(0, line.length() - 1);"
beeline/src/java/org/apache/hive/beeline/Commands.java,782,    }
beeline/src/java/org/apache/hive/beeline/Commands.java,783,
beeline/src/java/org/apache/hive/beeline/Commands.java,788,    String sql = line;
beeline/src/java/org/apache/hive/beeline/Commands.java,789,
beeline/src/java/org/apache/hive/beeline/Commands.java,790,    if (sql.startsWith(BeeLine.COMMAND_PREFIX)) {
beeline/src/java/org/apache/hive/beeline/Commands.java,791,      sql = sql.substring(1);
beeline/src/java/org/apache/hive/beeline/Commands.java,792,    }
beeline/src/java/org/apache/hive/beeline/Commands.java,793,
beeline/src/java/org/apache/hive/beeline/Commands.java,794,"    String prefix = call ? ""call"" : ""sql"";"
beeline/src/java/org/apache/hive/beeline/Commands.java,795,
beeline/src/java/org/apache/hive/beeline/Commands.java,796,    if (sql.startsWith(prefix)) {
beeline/src/java/org/apache/hive/beeline/Commands.java,797,      sql = sql.substring(prefix.length());
beeline/src/java/org/apache/hive/beeline/Commands.java,798,    }
beeline/src/java/org/apache/hive/beeline/Commands.java,799,
beeline/src/java/org/apache/hive/beeline/Commands.java,800,    // batch statements?
beeline/src/java/org/apache/hive/beeline/Commands.java,801,    if (beeLine.getBatch() != null) {
beeline/src/java/org/apache/hive/beeline/Commands.java,802,      beeLine.getBatch().add(sql);
beeline/src/java/org/apache/hive/beeline/Commands.java,803,      return true;
beeline/src/java/org/apache/hive/beeline/Commands.java,804,    }
beeline/src/java/org/apache/hive/beeline/Commands.java,806,    try {
beeline/src/java/org/apache/hive/beeline/Commands.java,807,      Statement stmnt = null;
beeline/src/java/org/apache/hive/beeline/Commands.java,808,      boolean hasResults;
beeline/src/java/org/apache/hive/beeline/Commands.java,809,      Thread logThread = null;
beeline/src/java/org/apache/hive/beeline/Commands.java,811,      try {
beeline/src/java/org/apache/hive/beeline/Commands.java,812,        long start = System.currentTimeMillis();
beeline/src/java/org/apache/hive/beeline/Commands.java,814,        if (call) {
beeline/src/java/org/apache/hive/beeline/Commands.java,815,          stmnt = beeLine.getDatabaseConnection().getConnection().prepareCall(sql);
beeline/src/java/org/apache/hive/beeline/Commands.java,816,          hasResults = ((CallableStatement) stmnt).execute();
beeline/src/java/org/apache/hive/beeline/Commands.java,817,        } else {
beeline/src/java/org/apache/hive/beeline/Commands.java,818,          stmnt = beeLine.createStatement();
beeline/src/java/org/apache/hive/beeline/Commands.java,819,          if (beeLine.getOpts().isSilent()) {
beeline/src/java/org/apache/hive/beeline/Commands.java,820,            hasResults = stmnt.execute(sql);
beeline/src/java/org/apache/hive/beeline/Commands.java,821,          } else {
beeline/src/java/org/apache/hive/beeline/Commands.java,822,            logThread = new Thread(createLogRunnable(stmnt));
beeline/src/java/org/apache/hive/beeline/Commands.java,823,            logThread.setDaemon(true);
beeline/src/java/org/apache/hive/beeline/Commands.java,824,            logThread.start();
beeline/src/java/org/apache/hive/beeline/Commands.java,825,            hasResults = stmnt.execute(sql);
beeline/src/java/org/apache/hive/beeline/Commands.java,826,            logThread.interrupt();
beeline/src/java/org/apache/hive/beeline/Commands.java,827,            logThread.join(DEFAULT_QUERY_PROGRESS_THREAD_TIMEOUT);
beeline/src/java/org/apache/hive/beeline/Commands.java,828,          }
beeline/src/java/org/apache/hive/beeline/Commands.java,831,        beeLine.showWarnings();
beeline/src/java/org/apache/hive/beeline/Commands.java,833,        if (hasResults) {
beeline/src/java/org/apache/hive/beeline/Commands.java,834,          do {
beeline/src/java/org/apache/hive/beeline/Commands.java,835,            ResultSet rs = stmnt.getResultSet();
beeline/src/java/org/apache/hive/beeline/Commands.java,836,            try {
beeline/src/java/org/apache/hive/beeline/Commands.java,837,              int count = beeLine.print(rs);
beeline/src/java/org/apache/hive/beeline/Commands.java,838,              long end = System.currentTimeMillis();
beeline/src/java/org/apache/hive/beeline/Commands.java,840,"              beeLine.info(beeLine.loc(""rows-selected"", count) + "" """
beeline/src/java/org/apache/hive/beeline/Commands.java,841,                  + beeLine.locElapsedTime(end - start));
beeline/src/java/org/apache/hive/beeline/Commands.java,842,            } finally {
beeline/src/java/org/apache/hive/beeline/Commands.java,843,              if (logThread != null) {
beeline/src/java/org/apache/hive/beeline/Commands.java,845,                showRemainingLogsIfAny(stmnt);
beeline/src/java/org/apache/hive/beeline/Commands.java,846,                logThread = null;
beeline/src/java/org/apache/hive/beeline/Commands.java,848,              rs.close();
beeline/src/java/org/apache/hive/beeline/Commands.java,850,          } while (BeeLine.getMoreResults(stmnt));
beeline/src/java/org/apache/hive/beeline/Commands.java,851,        } else {
beeline/src/java/org/apache/hive/beeline/Commands.java,852,          int count = stmnt.getUpdateCount();
beeline/src/java/org/apache/hive/beeline/Commands.java,853,          long end = System.currentTimeMillis();
beeline/src/java/org/apache/hive/beeline/Commands.java,854,"          beeLine.info(beeLine.loc(""rows-affected"", count)"
beeline/src/java/org/apache/hive/beeline/Commands.java,855,"              + "" "" + beeLine.locElapsedTime(end - start));"
beeline/src/java/org/apache/hive/beeline/Commands.java,856,        }
beeline/src/java/org/apache/hive/beeline/Commands.java,857,      } finally {
beeline/src/java/org/apache/hive/beeline/Commands.java,858,        if (logThread != null) {
beeline/src/java/org/apache/hive/beeline/Commands.java,859,          if (!logThread.isInterrupted()) {
beeline/src/java/org/apache/hive/beeline/Commands.java,860,            logThread.interrupt();
beeline/src/java/org/apache/hive/beeline/Commands.java,862,          logThread.join(DEFAULT_QUERY_PROGRESS_THREAD_TIMEOUT);
beeline/src/java/org/apache/hive/beeline/Commands.java,863,          showRemainingLogsIfAny(stmnt);
beeline/src/java/org/apache/hive/beeline/Commands.java,864,        }
beeline/src/java/org/apache/hive/beeline/Commands.java,865,        if (stmnt != null) {
beeline/src/java/org/apache/hive/beeline/Commands.java,866,          stmnt.close();
beeline/src/java/org/apache/hive/beeline/Commands.java,869,    } catch (Exception e) {
beeline/src/java/org/apache/hive/beeline/Commands.java,870,      return beeLine.error(e);
beeline/src/java/org/apache/hive/beeline/Commands.java,872,    beeLine.showWarnings();
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18924,"      ""2\020\002\022\021\n\rDICTIONARY_V2\020\003\""^\n\014StripeFooter\022\"""" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18926,"      ""lumns\030\002 \003(\0132\031.orc.proto.ColumnEncoding\""\341"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18927,"      ""\002\n\004Type\022\""\n\004kind\030\001 \001(\0162\024.orc.proto.Type.K"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18928,"      ""ind\022\024\n\010subtypes\030\002 \003(\rB\002\020\001\022\022\n\nfieldNames\030"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18929,"      ""\003 \003(\t\022\025\n\rmaximumLength\030\004 \001(\r\022\021\n\tprecisio"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18930,"      ""n\030\005 \001(\r\022\r\n\005scale\030\006 \001(\r\""\321\001\n\004Kind\022\013\n\007BOOLE"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18931,"      ""AN\020\000\022\010\n\004BYTE\020\001\022\t\n\005SHORT\020\002\022\007\n\003INT\020\003\022\010\n\004LO"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18932,"      ""NG\020\004\022\t\n\005FLOAT\020\005\022\n\n\006DOUBLE\020\006\022\n\n\006STRING\020\007\022"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18933,"      ""\n\n\006BINARY\020\010\022\r\n\tTIMESTAMP\020\t\022\010\n\004LIST\020\n\022\007\n\003"","
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18934,"      ""MAP\020\013\022\n\n\006STRUCT\020\014\022\t\n\005UNION\020\r\022\013\n\007DECIMAL\020"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18935,"      ""\016\022\010\n\004DATE\020\017\022\013\n\007VARCHAR\020\020\022\010\n\004CHAR\020\021\""x\n\021St"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18936,"      ""ripeInformation\022\016\n\006offset\030\001 \001(\004\022\023\n\013index"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18937,"      ""Length\030\002 \001(\004\022\022\n\ndataLength\030\003 \001(\004\022\024\n\014foot"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18938,"      ""erLength\030\004 \001(\004\022\024\n\014numberOfRows\030\005 \001(\004\""/\n\020"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18939,"      ""UserMetadataItem\022\014\n\004name\030\001 \001(\t\022\r\n\005value\030"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18940,"      ""\002 \001(\014\""A\n\020StripeStatistics\022-\n\010colStats\030\001 "" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18941,"      ""\003(\0132\033.orc.proto.ColumnStatistics\""<\n\010Meta"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18942,"      ""data\0220\n\013stripeStats\030\001 \003(\0132\033.orc.proto.St"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18943,"      ""ripeStatistics\""\222\002\n\006Footer\022\024\n\014headerLengt"","
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18944,"      ""h\030\001 \001(\004\022\025\n\rcontentLength\030\002 \001(\004\022-\n\007stripe"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18945,"      ""s\030\003 \003(\0132\034.orc.proto.StripeInformation\022\036\n"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18946,"      ""\005types\030\004 \003(\0132\017.orc.proto.Type\022-\n\010metadat"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18947,"      ""a\030\005 \003(\0132\033.orc.proto.UserMetadataItem\022\024\n\014"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18948,"      ""numberOfRows\030\006 \001(\004\022/\n\nstatistics\030\007 \003(\0132\033"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18949,"      "".orc.proto.ColumnStatistics\022\026\n\016rowIndexS"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18950,"      ""tride\030\010 \001(\r\""\305\001\n\nPostScript\022\024\n\014footerLeng"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18951,"      ""th\030\001 \001(\004\022/\n\013compression\030\002 \001(\0162\032.orc.prot"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18952,"      ""o.CompressionKind\022\034\n\024compressionBlockSiz"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18953,"      ""e\030\003 \001(\004\022\023\n\007version\030\004 \003(\rB\002\020\001\022\026\n\016metadata"","
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18954,"      ""Length\030\005 \001(\004\022\025\n\rwriterVersion\030\006 \001(\r\022\016\n\005m"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18955,"      ""agic\030\300> \001(\t*:\n\017CompressionKind\022\010\n\004NONE\020\000"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18956,"      ""\022\010\n\004ZLIB\020\001\022\n\n\006SNAPPY\020\002\022\007\n\003LZO\020\003B\""\n org.a"" +"
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,18957,"      ""pache.hadoop.hive.ql.io.orc"""
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,19059,"              new java.lang.String[] { ""Streams"", ""Columns"", });"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/FileDump.java,53,"  private static final String ROWINDEX_PREFIX = ""--rowindex="";"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/FileDump.java,82,"    else printMetaData(Arrays.asList(files), conf, rowIndexCols);"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/FileDump.java,93,                                    List<Integer> rowIndexCols) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/FileDump.java,128,"        System.out.println(""  Stripe: "" + stripe.toString());"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/FileDump.java,132,"          String kind = section.hasKind() ? section.getKind().name() : ""UNKNOWN"";"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,33,import java.util.LinkedList;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,58,import org.apache.hadoop.hive.ql.io.orc.RecordReaderUtils.ByteBufferAllocatorPool;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,81,import com.google.common.collect.Lists;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,82,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,189,                  ) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,294,                     List<OrcProto.ColumnEncoding> encoding
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,296,      checkEncoding(encoding.get(columnId));
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,393,                     List<OrcProto.ColumnEncoding> encodings
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,395,"      super.startStripe(streams, encodings);"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,463,                     List<OrcProto.ColumnEncoding> encodings
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,465,"      super.startStripe(streams, encodings);"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,547,                     List<OrcProto.ColumnEncoding> encodings
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,549,"      super.startStripe(streams, encodings);"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,552,"      reader = createIntegerReader(encodings.get(columnId).getKind(), streams.get(name), true,"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,553,          false);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,633,                     List<OrcProto.ColumnEncoding> encodings
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,635,"      super.startStripe(streams, encodings);"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,638,"      reader = createIntegerReader(encodings.get(columnId).getKind(), streams.get(name), true,"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,639,          false);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,720,                     List<OrcProto.ColumnEncoding> encodings
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,722,"      super.startStripe(streams, encodings);"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,725,"      reader = createIntegerReader(encodings.get(columnId).getKind(), streams.get(name), true,"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,726,          false);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,794,                     List<OrcProto.ColumnEncoding> encodings
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,796,"      super.startStripe(streams, encodings);"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,886,                     List<OrcProto.ColumnEncoding> encodings
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,888,"      super.startStripe(streams, encodings);"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,992,                     List<OrcProto.ColumnEncoding> encodings
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,994,"      super.startStripe(streams, encodings);"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,998,"      lengths = createIntegerReader(encodings.get(columnId).getKind(), streams.get(new"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,999,"          StreamName(columnId, OrcProto.Stream.Kind.LENGTH)), false, false);"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,1066,  public static class TimestampTreeReader extends TreeReader {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,1067,    protected IntegerReader data = null;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,1068,    protected IntegerReader nanos = null;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,1104,                     List<OrcProto.ColumnEncoding> encodings
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,1106,"      super.startStripe(streams, encodings);"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,1107,"      data = createIntegerReader(encodings.get(columnId).getKind(),"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,1110,"      nanos = createIntegerReader(encodings.get(columnId).getKind(),"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,1137,        Timestamp ts = new Timestamp(0);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,1138,        long millis = (data.next() + WriterImpl.BASE_TIMESTAMP) *
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,1139,            WriterImpl.MILLIS_PER_SECOND;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,1147,        ts.setTime(millis);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,1226,                     List<OrcProto.ColumnEncoding> encodings
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,1228,"      super.startStripe(streams, encodings);"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,1231,"      reader = createIntegerReader(encodings.get(columnId).getKind(), streams.get(name), true, false);"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,1320,                     List<OrcProto.ColumnEncoding> encodings
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,1322,"      super.startStripe(streams, encodings);"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,1325,"      scaleReader = createIntegerReader(encodings.get(columnId).getKind(), streams.get("
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,1326,"          new StreamName(columnId, OrcProto.Stream.Kind.SECONDARY)), true, false);"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,1452,                     List<OrcProto.ColumnEncoding> encodings
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,1456,      switch (encodings.get(columnId).getKind()) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,1467,              encodings.get(columnId).getKind());
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,1469,"      reader.startStripe(streams, encodings);"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,1602,                     List<OrcProto.ColumnEncoding> encodings
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,1604,"      super.startStripe(streams, encodings);"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,1608,"      lengths = createIntegerReader(encodings.get(columnId).getKind(),"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,1723,                     List<OrcProto.ColumnEncoding> encodings
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,1725,"      super.startStripe(streams, encodings);"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,1736,"      readDictionaryLengthStream(in, encodings.get(columnId));"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,1740,"      reader = createIntegerReader(encodings.get(columnId).getKind(),"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,2111,                     List<OrcProto.ColumnEncoding> encodings
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,2113,"      super.startStripe(streams, encodings);"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,2116,"          field.startStripe(streams, encodings);"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,2187,                     List<OrcProto.ColumnEncoding> encodings
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,2189,"      super.startStripe(streams, encodings);"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,2194,"          field.startStripe(streams, encodings);"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,2279,                     List<OrcProto.ColumnEncoding> encodings
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,2281,"      super.startStripe(streams, encodings);"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,2282,"      lengths = createIntegerReader(encodings.get(columnId).getKind(),"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,2286,"        elementReader.startStripe(streams, encodings);"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,2373,                     List<OrcProto.ColumnEncoding> encodings
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,2375,"      super.startStripe(streams, encodings);"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,2376,"      lengths = createIntegerReader(encodings.get(columnId).getKind(),"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,2380,"        keyReader.startStripe(streams, encodings);"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,2383,"        valueReader.startStripe(streams, encodings);"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,2979,"      reader.startStripe(streams, stripeFooter.getColumnsList());"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java,29,import java.util.Arrays;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java,1514,  static final long BASE_TIMESTAMP =
ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java,1515,"      Timestamp.valueOf(""2015-01-01 00:00:00"").getTime() / MILLIS_PER_SECOND;"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java,1553,        seconds.write((val.getTime() / MILLIS_PER_SECOND) - BASE_TIMESTAMP);
ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java,1641,"    assertEquals(607, split.getLength());"
ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java,1648,"    assertEquals(629, split.getLength());"
ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java,1656,"      assertEquals(241, combineSplit.getLength(bucket));"
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/MathExpr.java,78,  /* Convert an integer value in miliseconds since the epoch to a timestamp value
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/MathExpr.java,82,    return v * 1000000;
ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorTypeCasts.java,49,  // Number of microseconds in one second
ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorTypeCasts.java,50,  private static final long MICROS_PER_SECOND = 1000000;
ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorTypeCasts.java,51,
ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorTypeCasts.java,114,"    Assert.assertEquals(-2 * MICROS_PER_SECOND, resultV.vector[0]);"
ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorTypeCasts.java,115,"    Assert.assertEquals(2 * MICROS_PER_SECOND, resultV.vector[1]);"
serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java,515,  /**
serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java,516,   * Interprets a float as a unix timestamp and returns a Timestamp object
serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java,517,   * @param f
serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java,518,   * @return the equivalent Timestamp object
serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java,519,   */
serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java,520,  public static Timestamp floatToTimestamp(float f) {
serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java,521,    return doubleToTimestamp((double) f);
serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java,522,  }
serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java,523,
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1066,      result = new Timestamp(((BooleanObjectInspector) oi).get(o) ? 1 : 0);
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1069,      result = new Timestamp(((ByteObjectInspector) oi).get(o));
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1072,      result = new Timestamp(((ShortObjectInspector) oi).get(o));
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1075,      result = new Timestamp(((IntObjectInspector) oi).get(o));
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1078,      result = new Timestamp(((LongObjectInspector) oi).get(o));
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1081,      result = TimestampWritable.floatToTimestamp(((FloatObjectInspector) oi).get(o));
ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFLastDay.java,21,import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;
ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFLastDay.java,28,import junit.framework.TestCase;
ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFLastDay.java,29,
serde/src/java/org/apache/hadoop/hive/serde2/io/DateWritable.java,28,import org.apache.commons.logging.Log;
serde/src/java/org/apache/hadoop/hive/serde2/io/DateWritable.java,29,import org.apache.commons.logging.LogFactory;
serde/src/java/org/apache/hadoop/hive/serde2/io/DateWritable.java,30,import org.apache.hadoop.hive.serde2.ByteStream.Output;
serde/src/java/org/apache/hadoop/hive/serde2/io/DateWritable.java,47,  private static final Log LOG = LogFactory.getLog(DateWritable.class);
serde/src/java/org/apache/hadoop/hive/serde2/io/DateWritable.java,48,
serde/src/java/org/apache/hadoop/hive/serde2/io/DateWritable.java,139,    return (int)(millisUtc / MILLIS_PER_DAY);
serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoUtils.java,771,    if (typeA == typeB) {
serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoUtils.java,774,"    if (TypeInfoUtils.doPrimitiveCategoriesMatch(typeA,  typeB)) {"
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1156,
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/MathExpr.java,21,import java.io.IOException;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/MathExpr.java,22,import java.io.OutputStream;
serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java,530,   * Converts the time in seconds to a timestamp.
serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java,531,   * @param timeInSeconds time in seconds
serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java,534,  public static Timestamp longToTimestamp(long timeInSeconds) {
serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java,535,    return new Timestamp(timeInSeconds * 1000);
serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java,537,
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorConverter.java,286,          inputOI));
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1062,    switch (oi.getPrimitiveCategory()) {
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1067,      longValue = ((BooleanObjectInspector) oi).get(o) ? 1 : 0;
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1068,      result = TimestampWritable.longToTimestamp(longValue);
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1071,      longValue = ((ByteObjectInspector) oi).get(o);
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1072,      result = TimestampWritable.longToTimestamp(longValue);
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1075,      longValue = ((ShortObjectInspector) oi).get(o);
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1076,      result = TimestampWritable.longToTimestamp(longValue);
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1079,      longValue = ((IntObjectInspector) oi).get(o);
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1080,      result = TimestampWritable.longToTimestamp(longValue);
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1083,      longValue = ((LongObjectInspector) oi).get(o);
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1084,      result = TimestampWritable.longToTimestamp(longValue);
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1087,      result = TimestampWritable.doubleToTimestamp(((FloatObjectInspector) oi).get(o));
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1090,      result = TimestampWritable.doubleToTimestamp(((DoubleObjectInspector) oi).get(o));
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1093,      result = TimestampWritable.decimalToTimestamp(((HiveDecimalObjectInspector) oi)
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1097,      StringObjectInspector soi = (StringObjectInspector) oi;
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1103,"      result = getTimestampFromString(getString(o, oi));"
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1107,          ((DateObjectInspector) oi).getPrimitiveWritableObject(o).get().getTime());
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1110,      result = ((TimestampObjectInspector) oi).getPrimitiveWritableObject(o).getTimestamp();
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1114,          + oi.getTypeName());
serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/primitive/TestPrimitiveObjectInspectorUtils.java,75,
serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/primitive/TestPrimitiveObjectInspectorUtils.java,77,  public void testgetTimestamp() {
serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/primitive/TestPrimitiveObjectInspectorUtils.java,88,"    assertEquals(""1970-01-01 00:00:01.000"", gmtDateFormat.format(PrimitiveObjectInspectorUtils.getTimestamp(true, booleanOI)));"
serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/primitive/TestPrimitiveObjectInspectorUtils.java,93,"    assertEquals(""1970-01-01 00:00:01.000"", gmtDateFormat.format(PrimitiveObjectInspectorUtils.getTimestamp((byte)1, byteOI)));"
serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/primitive/TestPrimitiveObjectInspectorUtils.java,94,"    assertEquals(""1969-12-31 23:59:59.000"", gmtDateFormat.format(PrimitiveObjectInspectorUtils.getTimestamp((byte)-1, byteOI)));"
serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/primitive/TestPrimitiveObjectInspectorUtils.java,98,"    assertEquals(""1970-01-01 00:00:01.000"", gmtDateFormat.format(PrimitiveObjectInspectorUtils.getTimestamp((short)1, shortOI)));"
serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/primitive/TestPrimitiveObjectInspectorUtils.java,99,"    assertEquals(""1969-12-31 23:59:59.000"", gmtDateFormat.format(PrimitiveObjectInspectorUtils.getTimestamp((short)-1, shortOI)));"
serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/primitive/TestPrimitiveObjectInspectorUtils.java,103,"    assertEquals(""2015-02-07 15:01:22.000"", gmtDateFormat.format(PrimitiveObjectInspectorUtils.getTimestamp((int)1423321282, intOI)));"
serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/primitive/TestPrimitiveObjectInspectorUtils.java,104,"    assertEquals(""1969-12-31 23:59:59.000"", gmtDateFormat.format(PrimitiveObjectInspectorUtils.getTimestamp((int)-1, intOI)));"
serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/primitive/TestPrimitiveObjectInspectorUtils.java,108,"    assertEquals(""2015-02-07 15:01:22.000"", gmtDateFormat.format(PrimitiveObjectInspectorUtils.getTimestamp(1423321282L, longOI)));"
serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/primitive/TestPrimitiveObjectInspectorUtils.java,109,"    assertEquals(""1969-12-31 23:59:59.000"", gmtDateFormat.format(PrimitiveObjectInspectorUtils.getTimestamp(-1L, longOI)));"
ql/src/java/org/apache/hadoop/hive/ql/parse/QB.java,57,  private CreateTableDesc localDirectoryDesc = null ;
ql/src/java/org/apache/hadoop/hive/ql/parse/QB.java,288,  public CreateTableDesc getLLocalDirectoryDesc() {
ql/src/java/org/apache/hadoop/hive/ql/parse/QB.java,289,    return localDirectoryDesc;
ql/src/java/org/apache/hadoop/hive/ql/parse/QB.java,292,  public void setLocalDirectoryDesc(CreateTableDesc localDirectoryDesc) {
ql/src/java/org/apache/hadoop/hive/ql/parse/QB.java,293,    this.localDirectoryDesc = localDirectoryDesc;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1750,        case HiveParser.TOK_LOCAL_DIR:
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1794,"          qb.getMetaData().setDestForAlias(name, fname,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1795,              (ast.getToken().getType() == HiveParser.TOK_DIR));
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1797,          CreateTableDesc localDirectoryDesc = new CreateTableDesc();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1798,          boolean localDirectoryDescIsSet = false;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1804,                localDirectoryDesc.setOutputFormat(storageFormat.getOutputFormat());
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1805,                localDirectoryDesc.setSerName(storageFormat.getSerde());
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1806,                localDirectoryDescIsSet = true;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1812,                  localDirectoryDesc.setFieldDelim(rowFormatParams.fieldDelim);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1813,                  localDirectoryDesc.setLineDelim(rowFormatParams.lineDelim);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1814,                  localDirectoryDesc.setCollItemDelim(rowFormatParams.collItemDelim);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1815,                  localDirectoryDesc.setMapKeyDelim(rowFormatParams.mapKeyDelim);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1816,                  localDirectoryDesc.setFieldEscape(rowFormatParams.fieldEscape);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1817,                  localDirectoryDesc.setNullFormat(rowFormatParams.nullFormat);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1818,                  localDirectoryDescIsSet=true;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1823,                  localDirectoryDesc.setSerName(storageFormat.getSerde());
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1824,                  localDirectoryDescIsSet=true;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1829,          if (localDirectoryDescIsSet){
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1830,            qb.setLocalDirectoryDesc(localDirectoryDesc);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6503,"          table_desc = PlanUtils.getDefaultTableDesc(qb.getLLocalDirectoryDesc(), cols, colTypes);"
ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java,110,"  public static TableDesc getDefaultTableDesc(CreateTableDesc localDirectoryDesc,"
ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java,114,    if (localDirectoryDesc == null) {
ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java,121,      if (localDirectoryDesc.getFieldDelim() != null) {
ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java,123,"            serdeConstants.FIELD_DELIM, localDirectoryDesc.getFieldDelim());"
ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java,125,"            serdeConstants.SERIALIZATION_FORMAT, localDirectoryDesc.getFieldDelim());"
ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java,127,      if (localDirectoryDesc.getLineDelim() != null) {
ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java,129,"            serdeConstants.LINE_DELIM, localDirectoryDesc.getLineDelim());"
ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java,131,      if (localDirectoryDesc.getCollItemDelim() != null) {
ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java,133,"            serdeConstants.COLLECTION_DELIM, localDirectoryDesc.getCollItemDelim());"
ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java,135,      if (localDirectoryDesc.getMapKeyDelim() != null) {
ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java,137,"            serdeConstants.MAPKEY_DELIM, localDirectoryDesc.getMapKeyDelim());"
ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java,139,      if (localDirectoryDesc.getFieldEscape() !=null) {
ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java,141,"            serdeConstants.ESCAPE_CHAR, localDirectoryDesc.getFieldEscape());"
ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java,143,      if (localDirectoryDesc.getSerName() != null) {
ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java,145,"            serdeConstants.SERIALIZATION_LIB, localDirectoryDesc.getSerName());"
ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java,147,      if (localDirectoryDesc.getOutputFormat() != null){
ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java,148,        ret.setOutputFileFormatClass(JavaUtils.loadClass(localDirectoryDesc.getOutputFormat()));
ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java,150,      if (localDirectoryDesc.getNullFormat() != null) {
ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java,152,              localDirectoryDesc.getNullFormat());
ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java,154,      if (localDirectoryDesc.getTblProps() != null) {
ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java,155,        properties.putAll(localDirectoryDesc.getTblProps());
ql/src/java/org/apache/hadoop/hive/ql/parse/TaskCompiler.java,332,    rootTasks.add(cStatsTask);
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseInputFormatUtil.java,31,import java.io.IOException;
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseInputFormatUtil.java,32,import java.util.ArrayList;
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseInputFormatUtil.java,33,import java.util.HashMap;
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseInputFormatUtil.java,34,import java.util.List;
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseInputFormatUtil.java,35,import java.util.Map;
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseInputFormatUtil.java,36,
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseInputFormatUtil.java,98,    // The HBase table's row key maps to a Hive table column. In the corner case when only the
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseInputFormatUtil.java,99,"    // row key column is selected in Hive, the HBase Scan will be empty i.e. no column family/"
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseInputFormatUtil.java,100,    // column qualifier will have been added to the scan. We arbitrarily add at least one column
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseInputFormatUtil.java,101,    // to the HBase scan so that we can retrieve all of the row keys and return them as the Hive
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseInputFormatUtil.java,102,    // tables column projection.
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseInputFormatUtil.java,104,      for (ColumnMapping colMap: columnMappings) {
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseInputFormatUtil.java,105,        if (colMap.hbaseRowKey || colMap.hbaseTimestamp) {
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseInputFormatUtil.java,106,          continue;
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseInputFormatUtil.java,107,        }
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseInputFormatUtil.java,108,
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseInputFormatUtil.java,109,        if (colMap.qualifierName == null) {
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseInputFormatUtil.java,110,          scan.addFamily(colMap.familyNameBytes);
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseInputFormatUtil.java,111,        } else {
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseInputFormatUtil.java,112,"          scan.addColumn(colMap.familyNameBytes, colMap.qualifierNameBytes);"
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseInputFormatUtil.java,113,        }
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseInputFormatUtil.java,115,        if (!readAllColumns) {
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseInputFormatUtil.java,116,          break;
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,150,    // drop table. ignore error.
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,151,    try {
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,152,"      stmt.execute(""drop table "" + tableName);"
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,153,    } catch (Exception ex) {
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,154,      fail(ex.toString());
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,155,    }
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,163,"    stmt.execute(""load data local inpath '"""
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,164,"        + dataFilePath.toString() + ""' into table "" + tableName);"
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,165,
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,166,    // also initialize a paritioned table to test against.
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,167,
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,168,    // drop table. ignore error.
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,169,    try {
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,170,"      stmt.execute(""drop table "" + partitionedTableName);"
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,171,    } catch (Exception ex) {
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,172,      fail(ex.toString());
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,180,"    stmt.execute(""load data local inpath '"""
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,181,"        + dataFilePath.toString() + ""' into table "" + partitionedTableName"
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,182,"        + "" PARTITION ("" + partitionedColumnName + ""="""
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,183,"        + partitionedColumnValue + "")"");"
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,184,
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,185,    // drop table. ignore error.
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,186,    try {
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,187,"      stmt.execute(""drop table "" + dataTypeTableName);"
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,188,    } catch (Exception ex) {
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,189,      fail(ex.toString());
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,211,"    stmt.execute(""load data local inpath '"""
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,212,"        + dataTypeDataFilePath.toString() + ""' into table "" + dataTypeTableName"
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,213,"        + "" PARTITION (dt='20090619')"");"
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,214,
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,215,    // drop view. ignore error.
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,216,    try {
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,217,"      stmt.execute(""drop view "" + viewName);"
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,218,    } catch (Exception ex) {
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,219,      fail(ex.toString());
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,247,"    stmt.execute(""drop table "" + tableName);"
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,248,"    stmt.execute(""drop table "" + partitionedTableName);"
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,249,"    stmt.execute(""drop table "" + dataTypeTableName);"
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,1126,"    Map<String, Object[]> tests = new HashMap<String, Object[]>();"
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,1127,"    tests.put(""test%jdbc%"", new Object[]{""testhivejdbcdriver_table"""
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,1128,"        , ""testhivejdbcdriverpartitionedtable"""
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,1129,"        , ""testhivejdbcdriverview""});"
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,1130,"    tests.put(""%jdbcdriver\\_table"", new Object[]{""testhivejdbcdriver_table""});"
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,1131,"    tests.put(""testhivejdbcdriver\\_table"", new Object[]{""testhivejdbcdriver_table""});"
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,1132,"    tests.put(""test_ivejdbcdri_er\\_table"", new Object[]{""testhivejdbcdriver_table""});"
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,1133,"    tests.put(""test_ivejdbcdri_er_table"", new Object[]{""testhivejdbcdriver_table""});"
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,1134,"    tests.put(""test_ivejdbcdri_er%table"", new Object[]{"
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,1135,"        ""testhivejdbcdriver_table"", ""testhivejdbcdriverpartitionedtable"" });"
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,1136,"    tests.put(""%jdbc%"", new Object[]{ ""testhivejdbcdriver_table"""
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,1137,"        , ""testhivejdbcdriverpartitionedtable"""
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,1138,"        , ""testhivejdbcdriverview""});"
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,1139,"    tests.put("""", new Object[]{});"
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,1140,
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,1141,    for (String checkPattern: tests.keySet()) {
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,1142,"      ResultSet rs = con.getMetaData().getTables(""default"", null, checkPattern, null);"
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,1144,"      assertEquals(5, resMeta.getColumnCount());"
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,1154,"        assertEquals(""Get by index different from get by name."", rs.getString(3), resultTableName);"
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,1155,"        assertEquals(""Excpected a different table."", tests.get(checkPattern)[cnt], resultTableName);"
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,1167,"      assertEquals(""Received an incorrect number of tables."", tests.get(checkPattern).length, cnt);"
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,1169,
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,1170,    // only ask for the views.
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,1171,"    ResultSet rs = con.getMetaData().getTables(""default"", null, null"
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,1172,"        , new String[]{viewTypeName});"
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,1173,    int cnt=0;
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,1174,    while (rs.next()) {
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,1175,      cnt++;
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,1176,    }
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,1177,    rs.close();
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,1178,"    assertEquals(""Incorrect number of views found."", 1, cnt);"
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java,1273,"      ResultSet rs = con.getMetaData().getColumns(null, null, checkPattern[0],"
jdbc/src/java/org/apache/hive/jdbc/HiveDatabaseMetaData.java,649,    if (schemaPattern != null) {
jdbc/src/java/org/apache/hive/jdbc/HiveDatabaseMetaData.java,650,      getTableReq.setSchemaName(schemaPattern);
jdbc/src/java/org/apache/hive/jdbc/HiveDatabaseMetaData.java,651,    }
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40887,    private static class drop_table_with_environment_context_resultStandardSchemeFactory implements SchemeFactory {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40888,      public drop_table_with_environment_context_resultStandardScheme getScheme() {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40889,        return new drop_table_with_environment_context_resultStandardScheme();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40893,    private static class drop_table_with_environment_context_resultStandardScheme extends StandardScheme<drop_table_with_environment_context_result> {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40895,"      public void read(org.apache.thrift.protocol.TProtocol iprot, drop_table_with_environment_context_result struct) throws org.apache.thrift.TException {"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40905,            case 1: // O1
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40906,              if (schemeField.type == org.apache.thrift.protocol.TType.STRUCT) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40907,                struct.o1 = new NoSuchObjectException();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40908,                struct.o1.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40909,                struct.setO1IsSet(true);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40914,            case 2: // O3
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40916,                struct.o3 = new MetaException();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40917,                struct.o3.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40918,                struct.setO3IsSet(true);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40932,"      public void write(org.apache.thrift.protocol.TProtocol oprot, drop_table_with_environment_context_result struct) throws org.apache.thrift.TException {"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40941,        if (struct.o3 != null) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40942,          oprot.writeFieldBegin(O3_FIELD_DESC);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40943,          struct.o3.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40944,          oprot.writeFieldEnd();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40945,        }
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40952,    private static class drop_table_with_environment_context_resultTupleSchemeFactory implements SchemeFactory {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40953,      public drop_table_with_environment_context_resultTupleScheme getScheme() {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40954,        return new drop_table_with_environment_context_resultTupleScheme();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40958,    private static class drop_table_with_environment_context_resultTupleScheme extends TupleScheme<drop_table_with_environment_context_result> {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40961,"      public void write(org.apache.thrift.protocol.TProtocol prot, drop_table_with_environment_context_result struct) throws org.apache.thrift.TException {"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40964,        if (struct.isSetO1()) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40967,        if (struct.isSetO3()) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40974,        if (struct.isSetO3()) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40975,          struct.o3.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40976,        }
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40980,"      public void read(org.apache.thrift.protocol.TProtocol prot, drop_table_with_environment_context_result struct) throws org.apache.thrift.TException {"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40984,          struct.o1 = new NoSuchObjectException();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40985,          struct.o1.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40986,          struct.setO1IsSet(true);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40989,          struct.o3 = new MetaException();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40990,          struct.o3.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40991,          struct.setO3IsSet(true);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40998,"  public static class get_tables_args implements org.apache.thrift.TBase<get_tables_args, get_tables_args._Fields>, java.io.Serializable, Cloneable, Comparable<get_tables_args>   {"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,40999,"    private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct(""get_tables_args"");"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41001,"    private static final org.apache.thrift.protocol.TField DB_NAME_FIELD_DESC = new org.apache.thrift.protocol.TField(""db_name"", org.apache.thrift.protocol.TType.STRING, (short)1);"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41002,"    private static final org.apache.thrift.protocol.TField PATTERN_FIELD_DESC = new org.apache.thrift.protocol.TField(""pattern"", org.apache.thrift.protocol.TType.STRING, (short)2);"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41006,"      schemes.put(StandardScheme.class, new get_tables_argsStandardSchemeFactory());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41007,"      schemes.put(TupleScheme.class, new get_tables_argsTupleSchemeFactory());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41010,    private String db_name; // required
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41011,    private String pattern; // required
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41015,"      DB_NAME((short)1, ""db_name""),"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41016,"      PATTERN((short)2, ""pattern"");"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41031,          case 1: // DB_NAME
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41032,            return DB_NAME;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41033,          case 2: // PATTERN
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41034,            return PATTERN;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41078,"      tmpMap.put(_Fields.DB_NAME, new org.apache.thrift.meta_data.FieldMetaData(""db_name"", org.apache.thrift.TFieldRequirementType.DEFAULT,"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41080,"      tmpMap.put(_Fields.PATTERN, new org.apache.thrift.meta_data.FieldMetaData(""pattern"", org.apache.thrift.TFieldRequirementType.DEFAULT,"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41083,"      org.apache.thrift.meta_data.FieldMetaData.addStructMetaDataMap(get_tables_args.class, metaDataMap);"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41086,    public get_tables_args() {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41089,    public get_tables_args(
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41090,"      String db_name,"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41091,      String pattern)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41094,      this.db_name = db_name;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41095,      this.pattern = pattern;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41101,    public get_tables_args(get_tables_args other) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41102,      if (other.isSetDb_name()) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41103,        this.db_name = other.db_name;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41105,      if (other.isSetPattern()) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41106,        this.pattern = other.pattern;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41110,    public get_tables_args deepCopy() {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41111,      return new get_tables_args(this);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41116,      this.db_name = null;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41117,      this.pattern = null;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41120,    public String getDb_name() {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41121,      return this.db_name;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41124,    public void setDb_name(String db_name) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41125,      this.db_name = db_name;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41128,    public void unsetDb_name() {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41129,      this.db_name = null;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41132,    /** Returns true if field db_name is set (has been assigned a value) and false otherwise */
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41133,    public boolean isSetDb_name() {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41134,      return this.db_name != null;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41137,    public void setDb_nameIsSet(boolean value) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41139,        this.db_name = null;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41143,    public String getPattern() {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41144,      return this.pattern;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41147,    public void setPattern(String pattern) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41148,      this.pattern = pattern;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41151,    public void unsetPattern() {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41152,      this.pattern = null;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41155,    /** Returns true if field pattern is set (has been assigned a value) and false otherwise */
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41156,    public boolean isSetPattern() {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41157,      return this.pattern != null;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41160,    public void setPatternIsSet(boolean value) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41162,        this.pattern = null;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41168,      case DB_NAME:
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41170,          unsetDb_name();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41172,          setDb_name((String)value);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41176,      case PATTERN:
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41178,          unsetPattern();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41180,          setPattern((String)value);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41189,      case DB_NAME:
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41190,        return getDb_name();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41192,      case PATTERN:
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41193,        return getPattern();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41206,      case DB_NAME:
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41207,        return isSetDb_name();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41208,      case PATTERN:
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41209,        return isSetPattern();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41218,      if (that instanceof get_tables_args)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41219,        return this.equals((get_tables_args)that);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41223,    public boolean equals(get_tables_args that) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41227,      boolean this_present_db_name = true && this.isSetDb_name();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41228,      boolean that_present_db_name = true && that.isSetDb_name();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41229,      if (this_present_db_name || that_present_db_name) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41230,        if (!(this_present_db_name && that_present_db_name))
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41232,        if (!this.db_name.equals(that.db_name))
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41236,      boolean this_present_pattern = true && this.isSetPattern();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41237,      boolean that_present_pattern = true && that.isSetPattern();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41238,      if (this_present_pattern || that_present_pattern) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41239,        if (!(this_present_pattern && that_present_pattern))
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41241,        if (!this.pattern.equals(that.pattern))
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41252,      boolean present_db_name = true && (isSetDb_name());
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41253,      list.add(present_db_name);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41254,      if (present_db_name)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41255,        list.add(db_name);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41257,      boolean present_pattern = true && (isSetPattern());
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41258,      list.add(present_pattern);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41259,      if (present_pattern)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41260,        list.add(pattern);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41266,    public int compareTo(get_tables_args other) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41273,      lastComparison = Boolean.valueOf(isSetDb_name()).compareTo(other.isSetDb_name());
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41277,      if (isSetDb_name()) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41278,"        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.db_name, other.db_name);"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41283,      lastComparison = Boolean.valueOf(isSetPattern()).compareTo(other.isSetPattern());
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41287,      if (isSetPattern()) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41288,"        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.pattern, other.pattern);"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41310,"      StringBuilder sb = new StringBuilder(""get_tables_args("");"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41313,"      sb.append(""db_name:"");"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41314,      if (this.db_name == null) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41317,        sb.append(this.db_name);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41321,"      sb.append(""pattern:"");"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41322,      if (this.pattern == null) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41325,        sb.append(this.pattern);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41353,    private static class get_tables_argsStandardSchemeFactory implements SchemeFactory {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41354,      public get_tables_argsStandardScheme getScheme() {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41355,        return new get_tables_argsStandardScheme();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41359,    private static class get_tables_argsStandardScheme extends StandardScheme<get_tables_args> {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41361,"      public void read(org.apache.thrift.protocol.TProtocol iprot, get_tables_args struct) throws org.apache.thrift.TException {"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41371,            case 1: // DB_NAME
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41373,                struct.db_name = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41374,                struct.setDb_nameIsSet(true);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41379,            case 2: // PATTERN
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41381,                struct.pattern = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41382,                struct.setPatternIsSet(true);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41396,"      public void write(org.apache.thrift.protocol.TProtocol oprot, get_tables_args struct) throws org.apache.thrift.TException {"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41400,        if (struct.db_name != null) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41401,          oprot.writeFieldBegin(DB_NAME_FIELD_DESC);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41402,          oprot.writeString(struct.db_name);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41405,        if (struct.pattern != null) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41406,          oprot.writeFieldBegin(PATTERN_FIELD_DESC);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41407,          oprot.writeString(struct.pattern);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41416,    private static class get_tables_argsTupleSchemeFactory implements SchemeFactory {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41417,      public get_tables_argsTupleScheme getScheme() {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41418,        return new get_tables_argsTupleScheme();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41422,    private static class get_tables_argsTupleScheme extends TupleScheme<get_tables_args> {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41425,"      public void write(org.apache.thrift.protocol.TProtocol prot, get_tables_args struct) throws org.apache.thrift.TException {"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41428,        if (struct.isSetDb_name()) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41431,        if (struct.isSetPattern()) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41434,"        oprot.writeBitSet(optionals, 2);"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41435,        if (struct.isSetDb_name()) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41436,          oprot.writeString(struct.db_name);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41438,        if (struct.isSetPattern()) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41439,          oprot.writeString(struct.pattern);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41444,"      public void read(org.apache.thrift.protocol.TProtocol prot, get_tables_args struct) throws org.apache.thrift.TException {"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41446,        BitSet incoming = iprot.readBitSet(2);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41448,          struct.db_name = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41449,          struct.setDb_nameIsSet(true);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41452,          struct.pattern = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41453,          struct.setPatternIsSet(true);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41460,"  public static class get_tables_result implements org.apache.thrift.TBase<get_tables_result, get_tables_result._Fields>, java.io.Serializable, Cloneable, Comparable<get_tables_result>   {"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41461,"    private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct(""get_tables_result"");"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41468,"      schemes.put(StandardScheme.class, new get_tables_resultStandardSchemeFactory());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41469,"      schemes.put(TupleScheme.class, new get_tables_resultTupleSchemeFactory());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41472,    private List<String> success; // required
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41542,              new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.STRING))));
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41546,"      org.apache.thrift.meta_data.FieldMetaData.addStructMetaDataMap(get_tables_result.class, metaDataMap);"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41549,    public get_tables_result() {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41552,    public get_tables_result(
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41553,"      List<String> success,"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41564,    public get_tables_result(get_tables_result other) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41566,        List<String> __this__success = new ArrayList<String>(other.success);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41574,    public get_tables_result deepCopy() {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41575,      return new get_tables_result(this);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41588,    public java.util.Iterator<String> getSuccessIterator() {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41592,    public void addToSuccess(String elem) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41594,        this.success = new ArrayList<String>();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41599,    public List<String> getSuccess() {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41603,    public void setSuccess(List<String> success) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41651,          setSuccess((List<String>)value);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41697,      if (that instanceof get_tables_result)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41698,        return this.equals((get_tables_result)that);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41702,    public boolean equals(get_tables_result that) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41745,    public int compareTo(get_tables_result other) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41789,"      StringBuilder sb = new StringBuilder(""get_tables_result("");"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41832,    private static class get_tables_resultStandardSchemeFactory implements SchemeFactory {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41833,      public get_tables_resultStandardScheme getScheme() {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41834,        return new get_tables_resultStandardScheme();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41838,    private static class get_tables_resultStandardScheme extends StandardScheme<get_tables_result> {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41840,"      public void read(org.apache.thrift.protocol.TProtocol iprot, get_tables_result struct) throws org.apache.thrift.TException {"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41853,                  org.apache.thrift.protocol.TList _list650 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41854,                  struct.success = new ArrayList<String>(_list650.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41855,                  String _elem651;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41856,                  for (int _i652 = 0; _i652 < _list650.size; ++_i652)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41858,                    _elem651 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41859,                    struct.success.add(_elem651);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41886,"      public void write(org.apache.thrift.protocol.TProtocol oprot, get_tables_result struct) throws org.apache.thrift.TException {"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41893,"            oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, struct.success.size()));"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41894,            for (String _iter653 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41896,              oprot.writeString(_iter653);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41913,    private static class get_tables_resultTupleSchemeFactory implements SchemeFactory {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41914,      public get_tables_resultTupleScheme getScheme() {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41915,        return new get_tables_resultTupleScheme();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41919,    private static class get_tables_resultTupleScheme extends TupleScheme<get_tables_result> {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41922,"      public void write(org.apache.thrift.protocol.TProtocol prot, get_tables_result struct) throws org.apache.thrift.TException {"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41935,            for (String _iter654 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41937,              oprot.writeString(_iter654);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41947,"      public void read(org.apache.thrift.protocol.TProtocol prot, get_tables_result struct) throws org.apache.thrift.TException {"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41952,"            org.apache.thrift.protocol.TList _list655 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41953,            struct.success = new ArrayList<String>(_list655.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41954,            String _elem656;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41955,            for (int _i657 = 0; _i657 < _list655.size; ++_i657)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41957,              _elem656 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,41958,              struct.success.add(_elem656);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,42724,                  org.apache.thrift.protocol.TList _list658 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,42725,                  struct.success = new ArrayList<String>(_list658.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,42726,                  String _elem659;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,42727,                  for (int _i660 = 0; _i660 < _list658.size; ++_i660)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,42729,                    _elem659 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,42730,                    struct.success.add(_elem659);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,42765,            for (String _iter661 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,42767,              oprot.writeString(_iter661);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,42806,            for (String _iter662 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,42808,              oprot.writeString(_iter662);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,42823,"            org.apache.thrift.protocol.TList _list663 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,42824,            struct.success = new ArrayList<String>(_list663.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,42825,            String _elem664;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,42826,            for (int _i665 = 0; _i665 < _list663.size; ++_i665)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,42828,              _elem664 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,42829,              struct.success.add(_elem664);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,44282,                  org.apache.thrift.protocol.TList _list666 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,44283,                  struct.tbl_names = new ArrayList<String>(_list666.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,44284,                  String _elem667;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,44285,                  for (int _i668 = 0; _i668 < _list666.size; ++_i668)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,44287,                    _elem667 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,44288,                    struct.tbl_names.add(_elem667);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,44319,            for (String _iter669 : struct.tbl_names)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,44321,              oprot.writeString(_iter669);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,44358,            for (String _iter670 : struct.tbl_names)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,44360,              oprot.writeString(_iter670);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,44376,"            org.apache.thrift.protocol.TList _list671 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,44377,            struct.tbl_names = new ArrayList<String>(_list671.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,44378,            String _elem672;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,44379,            for (int _i673 = 0; _i673 < _list671.size; ++_i673)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,44381,              _elem672 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,44382,              struct.tbl_names.add(_elem672);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,44950,                  org.apache.thrift.protocol.TList _list674 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,44951,                  struct.success = new ArrayList<Table>(_list674.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,44952,                  Table _elem675;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,44953,                  for (int _i676 = 0; _i676 < _list674.size; ++_i676)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,44955,                    _elem675 = new Table();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,44956,                    _elem675.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,44957,                    struct.success.add(_elem675);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,45010,            for (Table _iter677 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,45012,              _iter677.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,45067,            for (Table _iter678 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,45069,              _iter678.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,45090,"            org.apache.thrift.protocol.TList _list679 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,45091,            struct.success = new ArrayList<Table>(_list679.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,45092,            Table _elem680;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,45093,            for (int _i681 = 0; _i681 < _list679.size; ++_i681)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,45095,              _elem680 = new Table();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,45096,              _elem680.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,45097,              struct.success.add(_elem680);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,46243,                  org.apache.thrift.protocol.TList _list682 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,46244,                  struct.success = new ArrayList<String>(_list682.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,46245,                  String _elem683;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,46246,                  for (int _i684 = 0; _i684 < _list682.size; ++_i684)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,46248,                    _elem683 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,46249,                    struct.success.add(_elem683);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,46302,            for (String _iter685 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,46304,              oprot.writeString(_iter685);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,46359,            for (String _iter686 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,46361,              oprot.writeString(_iter686);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,46382,"            org.apache.thrift.protocol.TList _list687 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,46383,            struct.success = new ArrayList<String>(_list687.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,46384,            String _elem688;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,46385,            for (int _i689 = 0; _i689 < _list687.size; ++_i689)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,46387,              _elem688 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,46388,              struct.success.add(_elem688);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,52247,                  org.apache.thrift.protocol.TList _list690 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,52248,                  struct.new_parts = new ArrayList<Partition>(_list690.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,52249,                  Partition _elem691;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,52250,                  for (int _i692 = 0; _i692 < _list690.size; ++_i692)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,52252,                    _elem691 = new Partition();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,52253,                    _elem691.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,52254,                    struct.new_parts.add(_elem691);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,52280,            for (Partition _iter693 : struct.new_parts)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,52282,              _iter693.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,52313,            for (Partition _iter694 : struct.new_parts)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,52315,              _iter694.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,52327,"            org.apache.thrift.protocol.TList _list695 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,52328,            struct.new_parts = new ArrayList<Partition>(_list695.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,52329,            Partition _elem696;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,52330,            for (int _i697 = 0; _i697 < _list695.size; ++_i697)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,52332,              _elem696 = new Partition();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,52333,              _elem696.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,52334,              struct.new_parts.add(_elem696);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,53335,                  org.apache.thrift.protocol.TList _list698 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,53336,                  struct.new_parts = new ArrayList<PartitionSpec>(_list698.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,53337,                  PartitionSpec _elem699;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,53338,                  for (int _i700 = 0; _i700 < _list698.size; ++_i700)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,53340,                    _elem699 = new PartitionSpec();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,53341,                    _elem699.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,53342,                    struct.new_parts.add(_elem699);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,53368,            for (PartitionSpec _iter701 : struct.new_parts)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,53370,              _iter701.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,53401,            for (PartitionSpec _iter702 : struct.new_parts)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,53403,              _iter702.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,53415,"            org.apache.thrift.protocol.TList _list703 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,53416,            struct.new_parts = new ArrayList<PartitionSpec>(_list703.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,53417,            PartitionSpec _elem704;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,53418,            for (int _i705 = 0; _i705 < _list703.size; ++_i705)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,53420,              _elem704 = new PartitionSpec();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,53421,              _elem704.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,53422,              struct.new_parts.add(_elem704);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,54598,                  org.apache.thrift.protocol.TList _list706 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,54599,                  struct.part_vals = new ArrayList<String>(_list706.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,54600,                  String _elem707;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,54601,                  for (int _i708 = 0; _i708 < _list706.size; ++_i708)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,54603,                    _elem707 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,54604,                    struct.part_vals.add(_elem707);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,54640,            for (String _iter709 : struct.part_vals)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,54642,              oprot.writeString(_iter709);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,54685,            for (String _iter710 : struct.part_vals)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,54687,              oprot.writeString(_iter710);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,54707,"            org.apache.thrift.protocol.TList _list711 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,54708,            struct.part_vals = new ArrayList<String>(_list711.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,54709,            String _elem712;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,54710,            for (int _i713 = 0; _i713 < _list711.size; ++_i713)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,54712,              _elem712 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,54713,              struct.part_vals.add(_elem712);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,57022,                  org.apache.thrift.protocol.TList _list714 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,57023,                  struct.part_vals = new ArrayList<String>(_list714.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,57024,                  String _elem715;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,57025,                  for (int _i716 = 0; _i716 < _list714.size; ++_i716)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,57027,                    _elem715 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,57028,                    struct.part_vals.add(_elem715);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,57073,            for (String _iter717 : struct.part_vals)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,57075,              oprot.writeString(_iter717);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,57126,            for (String _iter718 : struct.part_vals)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,57128,              oprot.writeString(_iter718);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,57151,"            org.apache.thrift.protocol.TList _list719 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,57152,            struct.part_vals = new ArrayList<String>(_list719.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,57153,            String _elem720;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,57154,            for (int _i721 = 0; _i721 < _list719.size; ++_i721)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,57156,              _elem720 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,57157,              struct.part_vals.add(_elem720);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,61027,                  org.apache.thrift.protocol.TList _list722 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,61028,                  struct.part_vals = new ArrayList<String>(_list722.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,61029,                  String _elem723;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,61030,                  for (int _i724 = 0; _i724 < _list722.size; ++_i724)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,61032,                    _elem723 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,61033,                    struct.part_vals.add(_elem723);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,61077,            for (String _iter725 : struct.part_vals)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,61079,              oprot.writeString(_iter725);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,61128,            for (String _iter726 : struct.part_vals)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,61130,              oprot.writeString(_iter726);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,61153,"            org.apache.thrift.protocol.TList _list727 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,61154,            struct.part_vals = new ArrayList<String>(_list727.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,61155,            String _elem728;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,61156,            for (int _i729 = 0; _i729 < _list727.size; ++_i729)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,61158,              _elem728 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,61159,              struct.part_vals.add(_elem728);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,62398,                  org.apache.thrift.protocol.TList _list730 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,62399,                  struct.part_vals = new ArrayList<String>(_list730.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,62400,                  String _elem731;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,62401,                  for (int _i732 = 0; _i732 < _list730.size; ++_i732)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,62403,                    _elem731 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,62404,                    struct.part_vals.add(_elem731);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,62457,            for (String _iter733 : struct.part_vals)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,62459,              oprot.writeString(_iter733);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,62516,            for (String _iter734 : struct.part_vals)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,62518,              oprot.writeString(_iter734);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,62544,"            org.apache.thrift.protocol.TList _list735 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,62545,            struct.part_vals = new ArrayList<String>(_list735.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,62546,            String _elem736;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,62547,            for (int _i737 = 0; _i737 < _list735.size; ++_i737)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,62549,              _elem736 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,62550,              struct.part_vals.add(_elem736);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,67152,                  org.apache.thrift.protocol.TList _list738 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,67153,                  struct.part_vals = new ArrayList<String>(_list738.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,67154,                  String _elem739;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,67155,                  for (int _i740 = 0; _i740 < _list738.size; ++_i740)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,67157,                    _elem739 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,67158,                    struct.part_vals.add(_elem739);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,67194,            for (String _iter741 : struct.part_vals)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,67196,              oprot.writeString(_iter741);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,67239,            for (String _iter742 : struct.part_vals)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,67241,              oprot.writeString(_iter742);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,67261,"            org.apache.thrift.protocol.TList _list743 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,67262,            struct.part_vals = new ArrayList<String>(_list743.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,67263,            String _elem744;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,67264,            for (int _i745 = 0; _i745 < _list743.size; ++_i745)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,67266,              _elem744 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,67267,              struct.part_vals.add(_elem744);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,68485,                  org.apache.thrift.protocol.TMap _map746 = iprot.readMapBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,68486,"                  struct.partitionSpecs = new HashMap<String,String>(2*_map746.size);"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,68487,                  String _key747;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,68488,                  String _val748;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,68489,                  for (int _i749 = 0; _i749 < _map746.size; ++_i749)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,68491,                    _key747 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,68492,                    _val748 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,68493,"                    struct.partitionSpecs.put(_key747, _val748);"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,68551,"            for (Map.Entry<String, String> _iter750 : struct.partitionSpecs.entrySet())"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,68553,              oprot.writeString(_iter750.getKey());
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,68554,              oprot.writeString(_iter750.getValue());
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,68617,"            for (Map.Entry<String, String> _iter751 : struct.partitionSpecs.entrySet())"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,68619,              oprot.writeString(_iter751.getKey());
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,68620,              oprot.writeString(_iter751.getValue());
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,68644,"            org.apache.thrift.protocol.TMap _map752 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,68645,"            struct.partitionSpecs = new HashMap<String,String>(2*_map752.size);"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,68646,            String _key753;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,68647,            String _val754;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,68648,            for (int _i755 = 0; _i755 < _map752.size; ++_i755)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,68650,              _key753 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,68651,              _val754 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,68652,"              struct.partitionSpecs.put(_key753, _val754);"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,70098,                  org.apache.thrift.protocol.TMap _map756 = iprot.readMapBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,70099,"                  struct.partitionSpecs = new HashMap<String,String>(2*_map756.size);"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,70100,                  String _key757;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,70101,                  String _val758;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,70102,                  for (int _i759 = 0; _i759 < _map756.size; ++_i759)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,70104,                    _key757 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,70105,                    _val758 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,70106,"                    struct.partitionSpecs.put(_key757, _val758);"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,70164,"            for (Map.Entry<String, String> _iter760 : struct.partitionSpecs.entrySet())"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,70166,              oprot.writeString(_iter760.getKey());
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,70167,              oprot.writeString(_iter760.getValue());
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,70230,"            for (Map.Entry<String, String> _iter761 : struct.partitionSpecs.entrySet())"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,70232,              oprot.writeString(_iter761.getKey());
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,70233,              oprot.writeString(_iter761.getValue());
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,70257,"            org.apache.thrift.protocol.TMap _map762 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,70258,"            struct.partitionSpecs = new HashMap<String,String>(2*_map762.size);"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,70259,            String _key763;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,70260,            String _val764;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,70261,            for (int _i765 = 0; _i765 < _map762.size; ++_i765)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,70263,              _key763 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,70264,              _val764 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,70265,"              struct.partitionSpecs.put(_key763, _val764);"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,70930,                  org.apache.thrift.protocol.TList _list766 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,70931,                  struct.success = new ArrayList<Partition>(_list766.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,70932,                  Partition _elem767;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,70933,                  for (int _i768 = 0; _i768 < _list766.size; ++_i768)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,70935,                    _elem767 = new Partition();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,70936,                    _elem767.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,70937,                    struct.success.add(_elem767);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,70999,            for (Partition _iter769 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71001,              _iter769.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71064,            for (Partition _iter770 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71066,              _iter770.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71090,"            org.apache.thrift.protocol.TList _list771 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71091,            struct.success = new ArrayList<Partition>(_list771.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71092,            Partition _elem772;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71093,            for (int _i773 = 0; _i773 < _list771.size; ++_i773)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71095,              _elem772 = new Partition();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71096,              _elem772.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71097,              struct.success.add(_elem772);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71796,                  org.apache.thrift.protocol.TList _list774 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71797,                  struct.part_vals = new ArrayList<String>(_list774.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71798,                  String _elem775;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71799,                  for (int _i776 = 0; _i776 < _list774.size; ++_i776)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71801,                    _elem775 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71802,                    struct.part_vals.add(_elem775);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71822,                  org.apache.thrift.protocol.TList _list777 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71823,                  struct.group_names = new ArrayList<String>(_list777.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71824,                  String _elem778;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71825,                  for (int _i779 = 0; _i779 < _list777.size; ++_i779)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71827,                    _elem778 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71828,                    struct.group_names.add(_elem778);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71864,            for (String _iter780 : struct.part_vals)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71866,              oprot.writeString(_iter780);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71881,            for (String _iter781 : struct.group_names)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71883,              oprot.writeString(_iter781);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71932,            for (String _iter782 : struct.part_vals)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71934,              oprot.writeString(_iter782);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71944,            for (String _iter783 : struct.group_names)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71946,              oprot.writeString(_iter783);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71966,"            org.apache.thrift.protocol.TList _list784 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71967,            struct.part_vals = new ArrayList<String>(_list784.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71968,            String _elem785;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71969,            for (int _i786 = 0; _i786 < _list784.size; ++_i786)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71971,              _elem785 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71972,              struct.part_vals.add(_elem785);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71983,"            org.apache.thrift.protocol.TList _list787 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71984,            struct.group_names = new ArrayList<String>(_list787.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71985,            String _elem788;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71986,            for (int _i789 = 0; _i789 < _list787.size; ++_i789)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71988,              _elem788 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,71989,              struct.group_names.add(_elem788);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,74758,                  org.apache.thrift.protocol.TList _list790 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,74759,                  struct.success = new ArrayList<Partition>(_list790.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,74760,                  Partition _elem791;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,74761,                  for (int _i792 = 0; _i792 < _list790.size; ++_i792)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,74763,                    _elem791 = new Partition();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,74764,                    _elem791.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,74765,                    struct.success.add(_elem791);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,74809,            for (Partition _iter793 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,74811,              _iter793.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,74858,            for (Partition _iter794 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,74860,              _iter794.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,74878,"            org.apache.thrift.protocol.TList _list795 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,74879,            struct.success = new ArrayList<Partition>(_list795.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,74880,            Partition _elem796;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,74881,            for (int _i797 = 0; _i797 < _list795.size; ++_i797)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,74883,              _elem796 = new Partition();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,74884,              _elem796.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,74885,              struct.success.add(_elem796);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,75575,                  org.apache.thrift.protocol.TList _list798 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,75576,                  struct.group_names = new ArrayList<String>(_list798.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,75577,                  String _elem799;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,75578,                  for (int _i800 = 0; _i800 < _list798.size; ++_i800)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,75580,                    _elem799 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,75581,                    struct.group_names.add(_elem799);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,75625,            for (String _iter801 : struct.group_names)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,75627,              oprot.writeString(_iter801);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,75682,            for (String _iter802 : struct.group_names)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,75684,              oprot.writeString(_iter802);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,75712,"            org.apache.thrift.protocol.TList _list803 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,75713,            struct.group_names = new ArrayList<String>(_list803.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,75714,            String _elem804;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,75715,            for (int _i805 = 0; _i805 < _list803.size; ++_i805)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,75717,              _elem804 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,75718,              struct.group_names.add(_elem804);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,76205,                  org.apache.thrift.protocol.TList _list806 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,76206,                  struct.success = new ArrayList<Partition>(_list806.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,76207,                  Partition _elem807;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,76208,                  for (int _i808 = 0; _i808 < _list806.size; ++_i808)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,76210,                    _elem807 = new Partition();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,76211,                    _elem807.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,76212,                    struct.success.add(_elem807);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,76256,            for (Partition _iter809 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,76258,              _iter809.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,76305,            for (Partition _iter810 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,76307,              _iter810.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,76325,"            org.apache.thrift.protocol.TList _list811 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,76326,            struct.success = new ArrayList<Partition>(_list811.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,76327,            Partition _elem812;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,76328,            for (int _i813 = 0; _i813 < _list811.size; ++_i813)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,76330,              _elem812 = new Partition();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,76331,              _elem812.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,76332,              struct.success.add(_elem812);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,77395,                  org.apache.thrift.protocol.TList _list814 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,77396,                  struct.success = new ArrayList<PartitionSpec>(_list814.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,77397,                  PartitionSpec _elem815;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,77398,                  for (int _i816 = 0; _i816 < _list814.size; ++_i816)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,77400,                    _elem815 = new PartitionSpec();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,77401,                    _elem815.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,77402,                    struct.success.add(_elem815);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,77446,            for (PartitionSpec _iter817 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,77448,              _iter817.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,77495,            for (PartitionSpec _iter818 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,77497,              _iter818.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,77515,"            org.apache.thrift.protocol.TList _list819 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,77516,            struct.success = new ArrayList<PartitionSpec>(_list819.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,77517,            PartitionSpec _elem820;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,77518,            for (int _i821 = 0; _i821 < _list819.size; ++_i821)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,77520,              _elem820 = new PartitionSpec();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,77521,              _elem820.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,77522,              struct.success.add(_elem820);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,78501,                  org.apache.thrift.protocol.TList _list822 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,78502,                  struct.success = new ArrayList<String>(_list822.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,78503,                  String _elem823;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,78504,                  for (int _i824 = 0; _i824 < _list822.size; ++_i824)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,78506,                    _elem823 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,78507,                    struct.success.add(_elem823);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,78542,            for (String _iter825 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,78544,              oprot.writeString(_iter825);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,78583,            for (String _iter826 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,78585,              oprot.writeString(_iter826);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,78600,"            org.apache.thrift.protocol.TList _list827 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,78601,            struct.success = new ArrayList<String>(_list827.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,78602,            String _elem828;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,78603,            for (int _i829 = 0; _i829 < _list827.size; ++_i829)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,78605,              _elem828 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,78606,              struct.success.add(_elem828);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,79194,                  org.apache.thrift.protocol.TList _list830 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,79195,                  struct.part_vals = new ArrayList<String>(_list830.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,79196,                  String _elem831;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,79197,                  for (int _i832 = 0; _i832 < _list830.size; ++_i832)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,79199,                    _elem831 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,79200,                    struct.part_vals.add(_elem831);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,79244,            for (String _iter833 : struct.part_vals)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,79246,              oprot.writeString(_iter833);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,79295,            for (String _iter834 : struct.part_vals)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,79297,              oprot.writeString(_iter834);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,79320,"            org.apache.thrift.protocol.TList _list835 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,79321,            struct.part_vals = new ArrayList<String>(_list835.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,79322,            String _elem836;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,79323,            for (int _i837 = 0; _i837 < _list835.size; ++_i837)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,79325,              _elem836 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,79326,              struct.part_vals.add(_elem836);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,79817,                  org.apache.thrift.protocol.TList _list838 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,79818,                  struct.success = new ArrayList<Partition>(_list838.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,79819,                  Partition _elem839;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,79820,                  for (int _i840 = 0; _i840 < _list838.size; ++_i840)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,79822,                    _elem839 = new Partition();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,79823,                    _elem839.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,79824,                    struct.success.add(_elem839);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,79868,            for (Partition _iter841 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,79870,              _iter841.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,79917,            for (Partition _iter842 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,79919,              _iter842.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,79937,"            org.apache.thrift.protocol.TList _list843 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,79938,            struct.success = new ArrayList<Partition>(_list843.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,79939,            Partition _elem844;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,79940,            for (int _i845 = 0; _i845 < _list843.size; ++_i845)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,79942,              _elem844 = new Partition();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,79943,              _elem844.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,79944,              struct.success.add(_elem844);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,80716,                  org.apache.thrift.protocol.TList _list846 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,80717,                  struct.part_vals = new ArrayList<String>(_list846.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,80718,                  String _elem847;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,80719,                  for (int _i848 = 0; _i848 < _list846.size; ++_i848)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,80721,                    _elem847 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,80722,                    struct.part_vals.add(_elem847);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,80750,                  org.apache.thrift.protocol.TList _list849 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,80751,                  struct.group_names = new ArrayList<String>(_list849.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,80752,                  String _elem850;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,80753,                  for (int _i851 = 0; _i851 < _list849.size; ++_i851)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,80755,                    _elem850 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,80756,                    struct.group_names.add(_elem850);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,80792,            for (String _iter852 : struct.part_vals)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,80794,              oprot.writeString(_iter852);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,80812,            for (String _iter853 : struct.group_names)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,80814,              oprot.writeString(_iter853);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,80866,            for (String _iter854 : struct.part_vals)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,80868,              oprot.writeString(_iter854);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,80881,            for (String _iter855 : struct.group_names)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,80883,              oprot.writeString(_iter855);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,80903,"            org.apache.thrift.protocol.TList _list856 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,80904,            struct.part_vals = new ArrayList<String>(_list856.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,80905,            String _elem857;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,80906,            for (int _i858 = 0; _i858 < _list856.size; ++_i858)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,80908,              _elem857 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,80909,              struct.part_vals.add(_elem857);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,80924,"            org.apache.thrift.protocol.TList _list859 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,80925,            struct.group_names = new ArrayList<String>(_list859.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,80926,            String _elem860;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,80927,            for (int _i861 = 0; _i861 < _list859.size; ++_i861)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,80929,              _elem860 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,80930,              struct.group_names.add(_elem860);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,81417,                  org.apache.thrift.protocol.TList _list862 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,81418,                  struct.success = new ArrayList<Partition>(_list862.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,81419,                  Partition _elem863;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,81420,                  for (int _i864 = 0; _i864 < _list862.size; ++_i864)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,81422,                    _elem863 = new Partition();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,81423,                    _elem863.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,81424,                    struct.success.add(_elem863);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,81468,            for (Partition _iter865 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,81470,              _iter865.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,81517,            for (Partition _iter866 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,81519,              _iter866.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,81537,"            org.apache.thrift.protocol.TList _list867 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,81538,            struct.success = new ArrayList<Partition>(_list867.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,81539,            Partition _elem868;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,81540,            for (int _i869 = 0; _i869 < _list867.size; ++_i869)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,81542,              _elem868 = new Partition();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,81543,              _elem868.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,81544,              struct.success.add(_elem868);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,82137,                  org.apache.thrift.protocol.TList _list870 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,82138,                  struct.part_vals = new ArrayList<String>(_list870.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,82139,                  String _elem871;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,82140,                  for (int _i872 = 0; _i872 < _list870.size; ++_i872)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,82142,                    _elem871 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,82143,                    struct.part_vals.add(_elem871);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,82187,            for (String _iter873 : struct.part_vals)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,82189,              oprot.writeString(_iter873);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,82238,            for (String _iter874 : struct.part_vals)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,82240,              oprot.writeString(_iter874);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,82263,"            org.apache.thrift.protocol.TList _list875 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,82264,            struct.part_vals = new ArrayList<String>(_list875.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,82265,            String _elem876;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,82266,            for (int _i877 = 0; _i877 < _list875.size; ++_i877)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,82268,              _elem876 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,82269,              struct.part_vals.add(_elem876);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,82757,                  org.apache.thrift.protocol.TList _list878 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,82758,                  struct.success = new ArrayList<String>(_list878.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,82759,                  String _elem879;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,82760,                  for (int _i880 = 0; _i880 < _list878.size; ++_i880)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,82762,                    _elem879 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,82763,                    struct.success.add(_elem879);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,82807,            for (String _iter881 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,82809,              oprot.writeString(_iter881);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,82856,            for (String _iter882 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,82858,              oprot.writeString(_iter882);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,82876,"            org.apache.thrift.protocol.TList _list883 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,82877,            struct.success = new ArrayList<String>(_list883.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,82878,            String _elem884;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,82879,            for (int _i885 = 0; _i885 < _list883.size; ++_i885)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,82881,              _elem884 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,82882,              struct.success.add(_elem884);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,84049,                  org.apache.thrift.protocol.TList _list886 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,84050,                  struct.success = new ArrayList<Partition>(_list886.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,84051,                  Partition _elem887;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,84052,                  for (int _i888 = 0; _i888 < _list886.size; ++_i888)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,84054,                    _elem887 = new Partition();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,84055,                    _elem887.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,84056,                    struct.success.add(_elem887);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,84100,            for (Partition _iter889 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,84102,              _iter889.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,84149,            for (Partition _iter890 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,84151,              _iter890.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,84169,"            org.apache.thrift.protocol.TList _list891 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,84170,            struct.success = new ArrayList<Partition>(_list891.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,84171,            Partition _elem892;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,84172,            for (int _i893 = 0; _i893 < _list891.size; ++_i893)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,84174,              _elem892 = new Partition();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,84175,              _elem892.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,84176,              struct.success.add(_elem892);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,85343,                  org.apache.thrift.protocol.TList _list894 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,85344,                  struct.success = new ArrayList<PartitionSpec>(_list894.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,85345,                  PartitionSpec _elem895;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,85346,                  for (int _i896 = 0; _i896 < _list894.size; ++_i896)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,85348,                    _elem895 = new PartitionSpec();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,85349,                    _elem895.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,85350,                    struct.success.add(_elem895);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,85394,            for (PartitionSpec _iter897 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,85396,              _iter897.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,85443,            for (PartitionSpec _iter898 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,85445,              _iter898.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,85463,"            org.apache.thrift.protocol.TList _list899 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,85464,            struct.success = new ArrayList<PartitionSpec>(_list899.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,85465,            PartitionSpec _elem900;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,85466,            for (int _i901 = 0; _i901 < _list899.size; ++_i901)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,85468,              _elem900 = new PartitionSpec();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,85469,              _elem900.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,85470,              struct.success.add(_elem900);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,86918,                  org.apache.thrift.protocol.TList _list902 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,86919,                  struct.names = new ArrayList<String>(_list902.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,86920,                  String _elem903;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,86921,                  for (int _i904 = 0; _i904 < _list902.size; ++_i904)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,86923,                    _elem903 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,86924,                    struct.names.add(_elem903);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,86960,            for (String _iter905 : struct.names)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,86962,              oprot.writeString(_iter905);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,87005,            for (String _iter906 : struct.names)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,87007,              oprot.writeString(_iter906);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,87027,"            org.apache.thrift.protocol.TList _list907 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,87028,            struct.names = new ArrayList<String>(_list907.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,87029,            String _elem908;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,87030,            for (int _i909 = 0; _i909 < _list907.size; ++_i909)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,87032,              _elem908 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,87033,              struct.names.add(_elem908);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,87520,                  org.apache.thrift.protocol.TList _list910 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,87521,                  struct.success = new ArrayList<Partition>(_list910.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,87522,                  Partition _elem911;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,87523,                  for (int _i912 = 0; _i912 < _list910.size; ++_i912)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,87525,                    _elem911 = new Partition();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,87526,                    _elem911.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,87527,                    struct.success.add(_elem911);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,87571,            for (Partition _iter913 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,87573,              _iter913.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,87620,            for (Partition _iter914 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,87622,              _iter914.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,87640,"            org.apache.thrift.protocol.TList _list915 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,87641,            struct.success = new ArrayList<Partition>(_list915.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,87642,            Partition _elem916;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,87643,            for (int _i917 = 0; _i917 < _list915.size; ++_i917)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,87645,              _elem916 = new Partition();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,87646,              _elem916.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,87647,              struct.success.add(_elem916);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,89197,                  org.apache.thrift.protocol.TList _list918 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,89198,                  struct.new_parts = new ArrayList<Partition>(_list918.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,89199,                  Partition _elem919;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,89200,                  for (int _i920 = 0; _i920 < _list918.size; ++_i920)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,89202,                    _elem919 = new Partition();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,89203,                    _elem919.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,89204,                    struct.new_parts.add(_elem919);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,89240,            for (Partition _iter921 : struct.new_parts)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,89242,              _iter921.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,89285,            for (Partition _iter922 : struct.new_parts)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,89287,              _iter922.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,89307,"            org.apache.thrift.protocol.TList _list923 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,89308,            struct.new_parts = new ArrayList<Partition>(_list923.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,89309,            Partition _elem924;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,89310,            for (int _i925 = 0; _i925 < _list923.size; ++_i925)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,89312,              _elem924 = new Partition();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,89313,              _elem924.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,89314,              struct.new_parts.add(_elem924);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,91510,                  org.apache.thrift.protocol.TList _list926 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,91511,                  struct.part_vals = new ArrayList<String>(_list926.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,91512,                  String _elem927;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,91513,                  for (int _i928 = 0; _i928 < _list926.size; ++_i928)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,91515,                    _elem927 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,91516,                    struct.part_vals.add(_elem927);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,91561,            for (String _iter929 : struct.part_vals)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,91563,              oprot.writeString(_iter929);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,91614,            for (String _iter930 : struct.part_vals)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,91616,              oprot.writeString(_iter930);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,91639,"            org.apache.thrift.protocol.TList _list931 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,91640,            struct.part_vals = new ArrayList<String>(_list931.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,91641,            String _elem932;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,91642,            for (int _i933 = 0; _i933 < _list931.size; ++_i933)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,91644,              _elem932 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,91645,              struct.part_vals.add(_elem932);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,92519,                  org.apache.thrift.protocol.TList _list934 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,92520,                  struct.part_vals = new ArrayList<String>(_list934.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,92521,                  String _elem935;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,92522,                  for (int _i936 = 0; _i936 < _list934.size; ++_i936)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,92524,                    _elem935 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,92525,                    struct.part_vals.add(_elem935);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,92559,            for (String _iter937 : struct.part_vals)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,92561,              oprot.writeString(_iter937);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,92598,            for (String _iter938 : struct.part_vals)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,92600,              oprot.writeString(_iter938);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,92615,"            org.apache.thrift.protocol.TList _list939 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,92616,            struct.part_vals = new ArrayList<String>(_list939.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,92617,            String _elem940;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,92618,            for (int _i941 = 0; _i941 < _list939.size; ++_i941)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,92620,              _elem940 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,92621,              struct.part_vals.add(_elem940);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,94776,                  org.apache.thrift.protocol.TList _list942 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,94777,                  struct.success = new ArrayList<String>(_list942.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,94778,                  String _elem943;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,94779,                  for (int _i944 = 0; _i944 < _list942.size; ++_i944)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,94781,                    _elem943 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,94782,                    struct.success.add(_elem943);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,94817,            for (String _iter945 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,94819,              oprot.writeString(_iter945);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,94858,            for (String _iter946 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,94860,              oprot.writeString(_iter946);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,94875,"            org.apache.thrift.protocol.TList _list947 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,94876,            struct.success = new ArrayList<String>(_list947.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,94877,            String _elem948;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,94878,            for (int _i949 = 0; _i949 < _list947.size; ++_i949)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,94880,              _elem948 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,94881,              struct.success.add(_elem948);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,95644,                  org.apache.thrift.protocol.TMap _map950 = iprot.readMapBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,95645,"                  struct.success = new HashMap<String,String>(2*_map950.size);"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,95646,                  String _key951;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,95647,                  String _val952;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,95648,                  for (int _i953 = 0; _i953 < _map950.size; ++_i953)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,95650,                    _key951 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,95651,                    _val952 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,95652,"                    struct.success.put(_key951, _val952);"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,95687,"            for (Map.Entry<String, String> _iter954 : struct.success.entrySet())"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,95689,              oprot.writeString(_iter954.getKey());
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,95690,              oprot.writeString(_iter954.getValue());
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,95729,"            for (Map.Entry<String, String> _iter955 : struct.success.entrySet())"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,95731,              oprot.writeString(_iter955.getKey());
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,95732,              oprot.writeString(_iter955.getValue());
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,95747,"            org.apache.thrift.protocol.TMap _map956 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,95748,"            struct.success = new HashMap<String,String>(2*_map956.size);"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,95749,            String _key957;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,95750,            String _val958;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,95751,            for (int _i959 = 0; _i959 < _map956.size; ++_i959)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,95753,              _key957 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,95754,              _val958 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,95755,"              struct.success.put(_key957, _val958);"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,96350,                  org.apache.thrift.protocol.TMap _map960 = iprot.readMapBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,96351,"                  struct.part_vals = new HashMap<String,String>(2*_map960.size);"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,96352,                  String _key961;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,96353,                  String _val962;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,96354,                  for (int _i963 = 0; _i963 < _map960.size; ++_i963)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,96356,                    _key961 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,96357,                    _val962 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,96358,"                    struct.part_vals.put(_key961, _val962);"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,96402,"            for (Map.Entry<String, String> _iter964 : struct.part_vals.entrySet())"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,96404,              oprot.writeString(_iter964.getKey());
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,96405,              oprot.writeString(_iter964.getValue());
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,96456,"            for (Map.Entry<String, String> _iter965 : struct.part_vals.entrySet())"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,96458,              oprot.writeString(_iter965.getKey());
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,96459,              oprot.writeString(_iter965.getValue());
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,96482,"            org.apache.thrift.protocol.TMap _map966 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,96483,"            struct.part_vals = new HashMap<String,String>(2*_map966.size);"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,96484,            String _key967;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,96485,            String _val968;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,96486,            for (int _i969 = 0; _i969 < _map966.size; ++_i969)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,96488,              _key967 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,96489,              _val968 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,96490,"              struct.part_vals.put(_key967, _val968);"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,97974,                  org.apache.thrift.protocol.TMap _map970 = iprot.readMapBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,97975,"                  struct.part_vals = new HashMap<String,String>(2*_map970.size);"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,97976,                  String _key971;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,97977,                  String _val972;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,97978,                  for (int _i973 = 0; _i973 < _map970.size; ++_i973)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,97980,                    _key971 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,97981,                    _val972 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,97982,"                    struct.part_vals.put(_key971, _val972);"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,98026,"            for (Map.Entry<String, String> _iter974 : struct.part_vals.entrySet())"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,98028,              oprot.writeString(_iter974.getKey());
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,98029,              oprot.writeString(_iter974.getValue());
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,98080,"            for (Map.Entry<String, String> _iter975 : struct.part_vals.entrySet())"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,98082,              oprot.writeString(_iter975.getKey());
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,98083,              oprot.writeString(_iter975.getValue());
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,98106,"            org.apache.thrift.protocol.TMap _map976 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,98107,"            struct.part_vals = new HashMap<String,String>(2*_map976.size);"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,98108,            String _key977;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,98109,            String _val978;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,98110,            for (int _i979 = 0; _i979 < _map976.size; ++_i979)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,98112,              _key977 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,98113,              _val978 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,98114,"              struct.part_vals.put(_key977, _val978);"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,104838,                  org.apache.thrift.protocol.TList _list980 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,104839,                  struct.success = new ArrayList<Index>(_list980.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,104840,                  Index _elem981;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,104841,                  for (int _i982 = 0; _i982 < _list980.size; ++_i982)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,104843,                    _elem981 = new Index();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,104844,                    _elem981.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,104845,                    struct.success.add(_elem981);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,104889,            for (Index _iter983 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,104891,              _iter983.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,104938,            for (Index _iter984 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,104940,              _iter984.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,104958,"            org.apache.thrift.protocol.TList _list985 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,104959,            struct.success = new ArrayList<Index>(_list985.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,104960,            Index _elem986;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,104961,            for (int _i987 = 0; _i987 < _list985.size; ++_i987)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,104963,              _elem986 = new Index();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,104964,              _elem986.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,104965,              struct.success.add(_elem986);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,105944,                  org.apache.thrift.protocol.TList _list988 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,105945,                  struct.success = new ArrayList<String>(_list988.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,105946,                  String _elem989;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,105947,                  for (int _i990 = 0; _i990 < _list988.size; ++_i990)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,105949,                    _elem989 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,105950,                    struct.success.add(_elem989);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,105985,            for (String _iter991 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,105987,              oprot.writeString(_iter991);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,106026,            for (String _iter992 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,106028,              oprot.writeString(_iter992);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,106043,"            org.apache.thrift.protocol.TList _list993 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,106044,            struct.success = new ArrayList<String>(_list993.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,106045,            String _elem994;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,106046,            for (int _i995 = 0; _i995 < _list993.size; ++_i995)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,106048,              _elem994 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,106049,              struct.success.add(_elem994);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,121784,                  org.apache.thrift.protocol.TList _list996 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,121785,                  struct.success = new ArrayList<String>(_list996.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,121786,                  String _elem997;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,121787,                  for (int _i998 = 0; _i998 < _list996.size; ++_i998)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,121789,                    _elem997 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,121790,                    struct.success.add(_elem997);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,121825,            for (String _iter999 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,121827,              oprot.writeString(_iter999);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,121866,            for (String _iter1000 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,121868,              oprot.writeString(_iter1000);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,121883,"            org.apache.thrift.protocol.TList _list1001 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,121884,            struct.success = new ArrayList<String>(_list1001.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,121885,            String _elem1002;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,121886,            for (int _i1003 = 0; _i1003 < _list1001.size; ++_i1003)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,121888,              _elem1002 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,121889,              struct.success.add(_elem1002);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,125944,                  org.apache.thrift.protocol.TList _list1004 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,125945,                  struct.success = new ArrayList<String>(_list1004.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,125946,                  String _elem1005;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,125947,                  for (int _i1006 = 0; _i1006 < _list1004.size; ++_i1006)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,125949,                    _elem1005 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,125950,                    struct.success.add(_elem1005);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,125985,            for (String _iter1007 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,125987,              oprot.writeString(_iter1007);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,126026,            for (String _iter1008 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,126028,              oprot.writeString(_iter1008);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,126043,"            org.apache.thrift.protocol.TList _list1009 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,126044,            struct.success = new ArrayList<String>(_list1009.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,126045,            String _elem1010;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,126046,            for (int _i1011 = 0; _i1011 < _list1009.size; ++_i1011)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,126048,              _elem1010 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,126049,              struct.success.add(_elem1010);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,129340,                  org.apache.thrift.protocol.TList _list1012 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,129341,                  struct.success = new ArrayList<Role>(_list1012.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,129342,                  Role _elem1013;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,129343,                  for (int _i1014 = 0; _i1014 < _list1012.size; ++_i1014)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,129345,                    _elem1013 = new Role();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,129346,                    _elem1013.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,129347,                    struct.success.add(_elem1013);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,129382,            for (Role _iter1015 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,129384,              _iter1015.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,129423,            for (Role _iter1016 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,129425,              _iter1016.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,129440,"            org.apache.thrift.protocol.TList _list1017 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,129441,            struct.success = new ArrayList<Role>(_list1017.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,129442,            Role _elem1018;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,129443,            for (int _i1019 = 0; _i1019 < _list1017.size; ++_i1019)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,129445,              _elem1018 = new Role();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,129446,              _elem1018.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,129447,              struct.success.add(_elem1018);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,132452,                  org.apache.thrift.protocol.TList _list1020 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,132453,                  struct.group_names = new ArrayList<String>(_list1020.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,132454,                  String _elem1021;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,132455,                  for (int _i1022 = 0; _i1022 < _list1020.size; ++_i1022)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,132457,                    _elem1021 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,132458,                    struct.group_names.add(_elem1021);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,132494,            for (String _iter1023 : struct.group_names)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,132496,              oprot.writeString(_iter1023);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,132539,            for (String _iter1024 : struct.group_names)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,132541,              oprot.writeString(_iter1024);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,132562,"            org.apache.thrift.protocol.TList _list1025 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,132563,            struct.group_names = new ArrayList<String>(_list1025.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,132564,            String _elem1026;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,132565,            for (int _i1027 = 0; _i1027 < _list1025.size; ++_i1027)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,132567,              _elem1026 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,132568,              struct.group_names.add(_elem1026);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,134026,                  org.apache.thrift.protocol.TList _list1028 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,134027,                  struct.success = new ArrayList<HiveObjectPrivilege>(_list1028.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,134028,                  HiveObjectPrivilege _elem1029;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,134029,                  for (int _i1030 = 0; _i1030 < _list1028.size; ++_i1030)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,134031,                    _elem1029 = new HiveObjectPrivilege();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,134032,                    _elem1029.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,134033,                    struct.success.add(_elem1029);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,134068,            for (HiveObjectPrivilege _iter1031 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,134070,              _iter1031.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,134109,            for (HiveObjectPrivilege _iter1032 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,134111,              _iter1032.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,134126,"            org.apache.thrift.protocol.TList _list1033 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,134127,            struct.success = new ArrayList<HiveObjectPrivilege>(_list1033.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,134128,            HiveObjectPrivilege _elem1034;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,134129,            for (int _i1035 = 0; _i1035 < _list1033.size; ++_i1035)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,134131,              _elem1034 = new HiveObjectPrivilege();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,134132,              _elem1034.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,134133,              struct.success.add(_elem1034);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,137035,                  org.apache.thrift.protocol.TList _list1036 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,137036,                  struct.group_names = new ArrayList<String>(_list1036.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,137037,                  String _elem1037;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,137038,                  for (int _i1038 = 0; _i1038 < _list1036.size; ++_i1038)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,137040,                    _elem1037 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,137041,                    struct.group_names.add(_elem1037);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,137072,            for (String _iter1039 : struct.group_names)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,137074,              oprot.writeString(_iter1039);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,137111,            for (String _iter1040 : struct.group_names)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,137113,              oprot.writeString(_iter1040);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,137129,"            org.apache.thrift.protocol.TList _list1041 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,137130,            struct.group_names = new ArrayList<String>(_list1041.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,137131,            String _elem1042;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,137132,            for (int _i1043 = 0; _i1043 < _list1041.size; ++_i1043)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,137134,              _elem1042 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,137135,              struct.group_names.add(_elem1042);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,137538,                  org.apache.thrift.protocol.TList _list1044 = iprot.readListBegin();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,137539,                  struct.success = new ArrayList<String>(_list1044.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,137540,                  String _elem1045;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,137541,                  for (int _i1046 = 0; _i1046 < _list1044.size; ++_i1046)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,137543,                    _elem1045 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,137544,                    struct.success.add(_elem1045);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,137579,            for (String _iter1047 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,137581,              oprot.writeString(_iter1047);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,137620,            for (String _iter1048 : struct.success)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,137622,              oprot.writeString(_iter1048);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,137637,"            org.apache.thrift.protocol.TList _list1049 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());"
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,137638,            struct.success = new ArrayList<String>(_list1049.size);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,137639,            String _elem1050;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,137640,            for (int _i1051 = 0; _i1051 < _list1049.size; ++_i1051)
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,137642,              _elem1050 = iprot.readString();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,137643,              struct.success.add(_elem1050);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,113,import java.lang.reflect.Field;
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,58,import org.antlr.runtime.CommonTokenStream;
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,59,import org.antlr.runtime.RecognitionException;
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,146,import org.apache.hadoop.hive.metastore.parser.ExpressionTree.ANTLRNoCaseStringStream;
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,148,import org.apache.hadoop.hive.metastore.parser.ExpressionTree.LeafNode;
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,149,import org.apache.hadoop.hive.metastore.parser.ExpressionTree.Operator;
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,150,import org.apache.hadoop.hive.metastore.parser.FilterLexer;
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,151,import org.apache.hadoop.hive.metastore.parser.FilterParser;
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,773,"    return getDatabases("".*"");"
metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseStore.java,74,import org.apache.hadoop.hive.ql.io.sarg.SearchArgument;
metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseStore.java,490,"      List<Table> tables = getHBase().scanTables(HiveStringUtils.normalizeIdentifier(dbName),"
metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseStore.java,491,          pattern==null?null:HiveStringUtils.normalizeIdentifier(likeToRegex(pattern)));
metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseStore.java,492,      List<String> tableNames = new ArrayList<String>(tables.size());
metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseStore.java,493,      for (Table table : tables) tableNames.add(table.getTableName());
metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseStore.java,1663,"          getHBase().getPartitionStatistics(dbName, tblName, partNames,  partVals, colNames);"
ql/src/java/org/apache/hadoop/hive/ql/metadata/SessionHiveMetaStoreClient.java,164,      if (matcher == null) {
ql/src/java/org/apache/hadoop/hive/ql/metadata/SessionHiveMetaStoreClient.java,165,        matcher = pattern.matcher(tableName);
ql/src/java/org/apache/hadoop/hive/ql/metadata/SessionHiveMetaStoreClient.java,166,      } else {
ql/src/java/org/apache/hadoop/hive/ql/metadata/SessionHiveMetaStoreClient.java,167,        matcher.reset(tableName);
ql/src/java/org/apache/hadoop/hive/ql/metadata/SessionHiveMetaStoreClient.java,168,      }
ql/src/java/org/apache/hadoop/hive/ql/metadata/SessionHiveMetaStoreClient.java,514,      return null;
ql/src/java/org/apache/hadoop/hive/ql/metadata/SessionHiveMetaStoreClient.java,516,    return ss.getTempTables().get(dbName);
service/src/java/org/apache/hive/service/cli/operation/GetTablesOperation.java,25,import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
service/src/java/org/apache/hive/service/cli/operation/GetTablesOperation.java,27,import org.apache.hadoop.hive.metastore.api.Table;
service/src/java/org/apache/hive/service/cli/operation/GetTablesOperation.java,28,import org.apache.hadoop.hive.ql.metadata.TableIterable;
service/src/java/org/apache/hive/service/cli/operation/GetTablesOperation.java,32,import org.apache.hadoop.hive.ql.session.SessionState;
service/src/java/org/apache/hive/service/cli/operation/GetTablesOperation.java,51,  private final List<String> tableTypes = new ArrayList<String>();
service/src/java/org/apache/hive/service/cli/operation/GetTablesOperation.java,61,"  .addStringColumn(""REMARKS"", ""Comments about the table."");"
service/src/java/org/apache/hive/service/cli/operation/GetTablesOperation.java,75,      this.tableTypes.addAll(tableTypes);
service/src/java/org/apache/hive/service/cli/operation/GetTablesOperation.java,94,      int maxBatchSize = SessionState.get().getConf().getIntVar(ConfVars.METASTORE_BATCH_RETRIEVE_MAX);
service/src/java/org/apache/hive/service/cli/operation/GetTablesOperation.java,96,      for (String dbName : metastoreClient.getDatabases(schemaPattern)) {
service/src/java/org/apache/hive/service/cli/operation/GetTablesOperation.java,97,"        List<String> tableNames = metastoreClient.getTables(dbName, tablePattern);"
service/src/java/org/apache/hive/service/cli/operation/GetTablesOperation.java,98,"        for (Table table : new TableIterable(metastoreClient, dbName, tableNames, maxBatchSize)) {"
service/src/java/org/apache/hive/service/cli/operation/GetTablesOperation.java,99,          Object[] rowData = new Object[] {
service/src/java/org/apache/hive/service/cli/operation/GetTablesOperation.java,101,"              table.getDbName(),"
service/src/java/org/apache/hive/service/cli/operation/GetTablesOperation.java,102,"              table.getTableName(),"
service/src/java/org/apache/hive/service/cli/operation/GetTablesOperation.java,103,"              tableTypeMapping.mapToClientType(table.getTableType()),"
service/src/java/org/apache/hive/service/cli/operation/GetTablesOperation.java,104,"              table.getParameters().get(""comment"")"
service/src/java/org/apache/hive/service/cli/operation/GetTablesOperation.java,105,              };
service/src/java/org/apache/hive/service/cli/operation/GetTablesOperation.java,106,          if (tableTypes.isEmpty() || tableTypes.contains(
service/src/java/org/apache/hive/service/cli/operation/GetTablesOperation.java,107,                tableTypeMapping.mapToClientType(table.getTableType()))) {
service/src/java/org/apache/hive/service/cli/operation/GetTablesOperation.java,108,            rowSet.addRow(rowData);
service/src/java/org/apache/hive/service/cli/operation/GetTablesOperation.java,109,          }
service/src/java/org/apache/hive/service/cli/operation/GetTablesOperation.java,110,        }
service/src/java/org/apache/hive/service/cli/operation/MetadataOperation.java,21,import java.util.ArrayList;
service/src/java/org/apache/hive/service/cli/operation/MetadataOperation.java,98,"  private String convertPattern(final String pattern, boolean datanucleusFormat) {"
service/src/java/org/apache/hive/service/cli/operation/MetadataOperation.java,105,    return pattern
service/src/java/org/apache/hive/service/cli/operation/MetadataOperation.java,106,"        .replaceAll(""([^\\\\])%"", ""$1"" + wStr).replaceAll(""\\\\%"", ""%"").replaceAll(""^%"", wStr)"
service/src/java/org/apache/hive/service/cli/operation/MetadataOperation.java,107,"        .replaceAll(""([^\\\\])_"", ""$1."").replaceAll(""\\\\_"", ""_"").replaceAll(""^_"", ""."");"
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,202,"      returnVal = ""'"" + partVal + ""'"";"
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,215,"      returnVal = partColType + "" '"" + partVal + ""'"";"
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,218,"      returnVal = ""'"" + partVal + ""'"";"
beeline/src/test/org/apache/hive/beeline/cli/TestHiveCli.java,279,"    System.setProperty(""datanucleus.fixedDatastore"", ""false"");"
beeline/src/test/org/apache/hive/beeline/cli/TestHiveCli.java,280,"    System.setProperty(""datanucleus.autoCreateSchema"", ""true"");"
beeline/src/test/org/apache/hive/beeline/cli/TestHiveCli.java,281,"    System.setProperty(""hive.metastore.schema.verification"", ""false"");"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,147,"      HiveConf.ConfVars.METASTORE_AUTO_CREATE_SCHEMA,"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,536,"    METASTORE_VALIDATE_TABLES(""datanucleus.validateTables"", false,"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,538,"    METASTORE_VALIDATE_COLUMNS(""datanucleus.validateColumns"", false,"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,540,"    METASTORE_VALIDATE_CONSTRAINTS(""datanucleus.validateConstraints"", false,"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,543,"    METASTORE_AUTO_CREATE_SCHEMA(""datanucleus.autoCreateSchema"", false,"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,545,"    METASTORE_FIXED_DATASTORE(""datanucleus.fixedDatastore"", true, ""Dictates whether to allow updates to schema or not.""),"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,546,"    METASTORE_SCHEMA_VERIFICATION(""hive.metastore.schema.verification"", true,"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,3233,"      setBoolVar(ConfVars.METASTORE_AUTO_CREATE_SCHEMA, false);"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,3234,"      setBoolVar(ConfVars.METASTORE_FIXED_DATASTORE, true);"
itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetastoreVersion.java,54,"    System.setProperty(HiveConf.ConfVars.METASTORE_AUTO_CREATE_SCHEMA.toString(), ""true"");"
itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetastoreVersion.java,55,"    System.setProperty(HiveConf.ConfVars.METASTORE_FIXED_DATASTORE.toString(), ""false"");"
itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetastoreVersion.java,84,    assertTrue(hiveConf.getBoolVar(HiveConf.ConfVars.METASTORE_AUTO_CREATE_SCHEMA));
itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetastoreVersion.java,85,    assertFalse(hiveConf.getBoolVar(HiveConf.ConfVars.METASTORE_FIXED_DATASTORE));
itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetastoreVersion.java,96,    assertFalse(hiveConf.getBoolVar(HiveConf.ConfVars.METASTORE_AUTO_CREATE_SCHEMA));
itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetastoreVersion.java,97,    assertTrue(hiveConf.getBoolVar(HiveConf.ConfVars.METASTORE_FIXED_DATASTORE));
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcWithMiniHS2.java,734,   * Tests that DataNucleus' NucleusContext.classLoaderResolverMap clears cached class objects (& hence
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcWithMiniHS2.java,735,   * doesn't leak classloaders) on closing any session
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcWithMiniHS2.java,798,"        classLoaderResolverMap = NucleusContext.class.getDeclaredField(""classLoaderResolverMap"");"
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,7839,        Field classLoaderResolverMap =
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,7840,"            NucleusContext.class.getDeclaredField(""classLoaderResolverMap"");"
shims/common/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge.java,182,"        String methodStr, String tokenStrForm, TTransport underlyingTransport,"
shims/common/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge.java,183,"        Map<String, String> saslProps) throws IOException {"
shims/common/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge.java,184,"      AuthMethod method = AuthMethod.valueOf(AuthMethod.class, methodStr);"
shims/common/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge.java,201,        String names[] = SaslRpcServer.splitKerberosName(serverPrincipal);
shims/common/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge.java,208,          saslTransport = new TSaslClientTransport(
shims/common/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge.java,209,"              method.getMechanismName(),"
shims/common/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge.java,210,"              null,"
shims/common/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge.java,211,"              names[0], names[1],"
shims/common/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge.java,212,"              saslProps, null,"
shims/common/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge.java,213,              underlyingTransport);
shims/common/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge.java,214,"          return new TUGIAssumingTransport(saslTransport, UserGroupInformation.getCurrentUser());"
shims/common/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge.java,215,        } catch (SaslException se) {
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,67,import org.apache.hadoop.hive.metastore.cache.CacheUtils;
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,68,import org.apache.hadoop.hive.metastore.cache.CachedStore;
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,87,import com.google.common.collect.Maps;
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,124,"  public MetaStoreDirectSql(PersistenceManager pm, Configuration conf) {"
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,226,"    String selfTestQuery = ""select \""DB_ID\"" from \""DBS\"""";"
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,277,"          + ""FROM \""DBS\"" where \""NAME\"" = ? "";"
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,301,"          + "" FROM \""DATABASE_PARAMS\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,359,"        String filter = ""\""PARTITIONS\"".\""PART_NAME\"" in ("" + makeParams(input.size()) + "")"";"
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,361,"            Collections.emptyList(), null);"
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,391,    result.filter = PartitionFilterGenerator.generateSqlFilter(
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,392,"        table, tree, result.params, result.joins, dbHasJoinCastBug, defaultPartName, dbType);"
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,406,"        null, Collections.emptyList(), Collections.emptyList(), max);"
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,417,"      String queryText = ""select \""TBL_TYPE\"" from \""TBLS\"""" +"
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,418,"          "" inner join \""DBS\"" on \""TBLS\"".\""DB_ID\"" = \""DBS\"".\""DB_ID\"" "" +"
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,419,"          "" where \""TBLS\"".\""TBL_NAME\"" = ? and \""DBS\"".\""NAME\"" = ?"";"
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,463,"        ""select \""PARTITIONS\"".\""PART_ID\"" from \""PARTITIONS\"""""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,464,"      + ""  inner join \""TBLS\"" on \""PARTITIONS\"".\""TBL_ID\"" = \""TBLS\"".\""TBL_ID\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,465,"      + ""    and \""TBLS\"".\""TBL_NAME\"" = ? """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,466,"      + ""  inner join \""DBS\"" on \""TBLS\"".\""DB_ID\"" = \""DBS\"".\""DB_ID\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,467,"      + ""     and \""DBS\"".\""NAME\"" = ? """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,516,"      ""select \""PARTITIONS\"".\""PART_ID\"", \""SDS\"".\""SD_ID\"", \""SDS\"".\""CD_ID\"","""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,517,"    + "" \""SERDES\"".\""SERDE_ID\"", \""PARTITIONS\"".\""CREATE_TIME\"","""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,518,"    + "" \""PARTITIONS\"".\""LAST_ACCESS_TIME\"", \""SDS\"".\""INPUT_FORMAT\"", \""SDS\"".\""IS_COMPRESSED\"","""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,519,"    + "" \""SDS\"".\""IS_STOREDASSUBDIRECTORIES\"", \""SDS\"".\""LOCATION\"", \""SDS\"".\""NUM_BUCKETS\"","""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,520,"    + "" \""SDS\"".\""OUTPUT_FORMAT\"", \""SERDES\"".\""NAME\"", \""SERDES\"".\""SLIB\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,521,"    + ""from \""PARTITIONS\"""""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,522,"    + ""  left outer join \""SDS\"" on \""PARTITIONS\"".\""SD_ID\"" = \""SDS\"".\""SD_ID\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,523,"    + ""  left outer join \""SERDES\"" on \""SDS\"".\""SERDE_ID\"" = \""SERDES\"".\""SERDE_ID\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,626,"    queryText = ""select \""PART_ID\"", \""PARAM_KEY\"", \""PARAM_VALUE\"" from \""PARTITION_PARAMS\"""""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,639,"    queryText = ""select \""PART_ID\"", \""PART_KEY_VAL\"" from \""PARTITION_KEY_VALS\"""""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,659,"    queryText = ""select \""SD_ID\"", \""PARAM_KEY\"", \""PARAM_VALUE\"" from \""SD_PARAMS\"""""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,672,"    queryText = ""select \""SD_ID\"", \""COLUMN_NAME\"", \""SORT_COLS\"".\""ORDER\"""""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,673,"        + "" from \""SORT_COLS\"""""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,683,"    queryText = ""select \""SD_ID\"", \""BUCKET_COL_NAME\"" from \""BUCKETING_COLS\"""""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,693,"    queryText = ""select \""SD_ID\"", \""SKEWED_COL_NAME\"" from \""SKEWED_COL_NAMES\"""""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,708,"            ""select \""SKEWED_VALUES\"".\""SD_ID_OID\"","""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,709,"          + ""  \""SKEWED_STRING_LIST_VALUES\"".\""STRING_LIST_ID\"","""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,710,"          + ""  \""SKEWED_STRING_LIST_VALUES\"".\""STRING_LIST_VALUE\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,711,"          + ""from \""SKEWED_VALUES\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,712,"          + ""  left outer join \""SKEWED_STRING_LIST_VALUES\"" on \""SKEWED_VALUES\""."""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,713,"          + ""\""STRING_LIST_ID_EID\"" = \""SKEWED_STRING_LIST_VALUES\"".\""STRING_LIST_ID\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,714,"          + ""where \""SKEWED_VALUES\"".\""SD_ID_OID\"" in ("" + sdIds + "") """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,715,"          + ""  and \""SKEWED_VALUES\"".\""STRING_LIST_ID_EID\"" is not null """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,716,"          + ""  and \""SKEWED_VALUES\"".\""INTEGER_IDX\"" >= 0 """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,717,"          + ""order by \""SKEWED_VALUES\"".\""SD_ID_OID\"" asc, \""SKEWED_VALUES\"".\""INTEGER_IDX\"" asc,"""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,718,"          + ""  \""SKEWED_STRING_LIST_VALUES\"".\""INTEGER_IDX\"" asc"";"
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,730,            t.getSkewedInfo().addToSkewedColValues(Collections.emptyList());
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,744,"            ""select \""SKEWED_COL_VALUE_LOC_MAP\"".\""SD_ID\"","""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,745,"          + "" \""SKEWED_STRING_LIST_VALUES\"".STRING_LIST_ID,"""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,746,"          + "" \""SKEWED_COL_VALUE_LOC_MAP\"".\""LOCATION\"","""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,747,"          + "" \""SKEWED_STRING_LIST_VALUES\"".\""STRING_LIST_VALUE\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,748,"          + ""from \""SKEWED_COL_VALUE_LOC_MAP\"""""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,749,"          + ""  left outer join \""SKEWED_STRING_LIST_VALUES\"" on \""SKEWED_COL_VALUE_LOC_MAP\""."""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,750,"          + ""\""STRING_LIST_ID_KID\"" = \""SKEWED_STRING_LIST_VALUES\"".\""STRING_LIST_ID\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,751,"          + ""where \""SKEWED_COL_VALUE_LOC_MAP\"".\""SD_ID\"" in ("" + sdIds + "")"""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,752,"          + ""  and \""SKEWED_COL_VALUE_LOC_MAP\"".\""STRING_LIST_ID_KID\"" is not null """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,753,"          + ""order by \""SKEWED_COL_VALUE_LOC_MAP\"".\""SD_ID\"" asc,"""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,754,"          + ""  \""SKEWED_STRING_LIST_VALUES\"".\""STRING_LIST_ID\"" asc,"""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,755,"          + ""  \""SKEWED_STRING_LIST_VALUES\"".\""INTEGER_IDX\"" asc"";"
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,791,"          + "" from \""COLUMNS_V2\"" where \""CD_ID\"" in ("" + colIds + "") and \""INTEGER_IDX\"" >= 0"""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,801,"    queryText = ""select \""SERDE_ID\"", \""PARAM_KEY\"", \""PARAM_VALUE\"" from \""SERDE_PARAMS\"""""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,823,"    String queryText = ""select count(\""PARTITIONS\"".\""PART_ID\"") from \""PARTITIONS\"""""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,824,"      + ""  inner join \""TBLS\"" on \""PARTITIONS\"".\""TBL_ID\"" = \""TBLS\"".\""TBL_ID\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,825,"      + ""    and \""TBLS\"".\""TBL_NAME\"" = ? """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,826,"      + ""  inner join \""DBS\"" on \""TBLS\"".\""DB_ID\"" = \""DBS\"".\""DB_ID\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,827,"      + ""     and \""DBS\"".\""NAME\"" = ? """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,976,"        boolean dbHasJoinCastBug, String defaultPartName, DatabaseProduct dbType) {"
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,995,        DatabaseProduct dbType) throws MetaException {
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1006,"          table, params, joins, dbHasJoinCastBug, defaultPartName, dbType);"
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1125,"        joins.set(partColIndex, ""inner join \""PARTITION_KEY_VALS\"" \""FILTER"" + partColIndex"
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1126,"            + ""\"" on \""FILTER""  + partColIndex + ""\"".\""PART_ID\"" = \""PARTITIONS\"".\""PART_ID\"""""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1157,"          tableValue += ("" and \""TBLS\"".\""TBL_NAME\"" = ? and \""DBS\"".\""NAME\"" = ? and """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1158,"              + ""\""FILTER"" + partColIndex + ""\"".\""PART_ID\"" = \""PARTITIONS\"".\""PART_ID\"" and """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1190,"    final String queryText0 = ""select "" + STATS_COLLIST + "" from \""TAB_COL_STATS\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1228,"      return new AggrStats(Collections.emptyList(), 0); // Nothing to aggregate"
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1293,"    final String queryText0  = ""select count(\""COLUMN_NAME\"") from \""PART_COL_STATS\"""""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1311,              ForwardQueryResult fqr = (ForwardQueryResult) qResult;
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1358,"            + "" from \""PART_COL_STATS\"" where \""DB_NAME\"" = ? and \""TABLE_NAME\"" = ?"""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1367,    qResult =
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1368,"        executeWithArray(query,"
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1369,"            prepareParams(dbName, tblName, Collections.emptyList(), Collections.emptyList()),"
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1370,            queryText);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1436,"        + ""sum(\""NUM_DISTINCTS\"")"" + "" from \""PART_COL_STATS\"""""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1444,    ForwardQueryResult fqr = null;
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1475,"          + "" from \""PART_COL_STATS\"""" + "" where \""DB_NAME\"" = ? and \""TABLE_NAME\"" = ? """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1540,"            + "" from \""PART_COL_STATS\"" where \""DB_NAME\"" = ? and \""TABLE_NAME\"" = ? """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1612,"                    + ""\"",\""PARTITION_NAME\"" from \""PART_COL_STATS\"""""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1618,"                    + ""\"",\""PARTITION_NAME\"" from \""PART_COL_STATS\"""""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1631,              fqr = (ForwardQueryResult) qResult;
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1649,"                  + "" from \""PART_COL_STATS\"""" + "" where \""DB_NAME\"" = ? and \""TABLE_NAME\"" = ?"""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1660,              fqr = (ForwardQueryResult) qResult;
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1724,"      + "" \""PART_COL_STATS\"" where \""DB_NAME\"" = ? and \""TABLE_NAME\"" = ? and \""COLUMN_NAME\"""""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1899,"      + ""\""DBS\"".\""NAME\"", \""TBLS\"".\""TBL_NAME\"", \""COLUMNS_V2\"".\""COLUMN_NAME\"", """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1900,"      + ""\""KEY_CONSTRAINTS\"".\""POSITION\"", \""KEY_CONSTRAINTS\"".\""UPDATE_RULE\"", \""KEY_CONSTRAINTS\"".\""DELETE_RULE\"", """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1901,"      + ""\""KEY_CONSTRAINTS\"".\""CONSTRAINT_NAME\"" , \""KEY_CONSTRAINTS2\"".\""CONSTRAINT_NAME\"", \""KEY_CONSTRAINTS\"".\""ENABLE_VALIDATE_RELY\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1902,"      + "" FROM \""TBLS\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1903,"      + "" INNER JOIN \""KEY_CONSTRAINTS\"" ON \""TBLS\"".\""TBL_ID\"" = \""KEY_CONSTRAINTS\"".\""CHILD_TBL_ID\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1904,"      + "" INNER JOIN \""KEY_CONSTRAINTS\"" \""KEY_CONSTRAINTS2\"" ON \""KEY_CONSTRAINTS2\"".\""PARENT_TBL_ID\""  = \""KEY_CONSTRAINTS\"".\""PARENT_TBL_ID\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1905,"      + "" AND \""KEY_CONSTRAINTS2\"".\""PARENT_CD_ID\""  = \""KEY_CONSTRAINTS\"".\""PARENT_CD_ID\"" AND """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1906,"      + "" \""KEY_CONSTRAINTS2\"".\""PARENT_INTEGER_IDX\""  = \""KEY_CONSTRAINTS\"".\""PARENT_INTEGER_IDX\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1907,"      + "" INNER JOIN \""DBS\"" ON \""TBLS\"".\""DB_ID\"" = \""DBS\"".\""DB_ID\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1908,"      + "" INNER JOIN \""TBLS\"" \""T2\"" ON  \""KEY_CONSTRAINTS\"".\""PARENT_TBL_ID\"" = \""T2\"".\""TBL_ID\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1909,"      + "" INNER JOIN \""DBS\"" \""D2\"" ON \""T2\"".\""DB_ID\"" = \""D2\"".\""DB_ID\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1910,"      + "" INNER JOIN \""COLUMNS_V2\""  ON \""COLUMNS_V2\"".\""CD_ID\"" = \""KEY_CONSTRAINTS\"".\""CHILD_CD_ID\"" AND """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1911,"      + "" \""COLUMNS_V2\"".\""INTEGER_IDX\"" = \""KEY_CONSTRAINTS\"".\""CHILD_INTEGER_IDX\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1912,"      + "" INNER JOIN \""COLUMNS_V2\"" \""C2\"" ON \""C2\"".\""CD_ID\"" = \""KEY_CONSTRAINTS\"".\""PARENT_CD_ID\"" AND """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1913,"      + "" \""C2\"".\""INTEGER_IDX\"" = \""KEY_CONSTRAINTS\"".\""PARENT_INTEGER_IDX\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1914,"      + "" WHERE \""KEY_CONSTRAINTS\"".\""CONSTRAINT_TYPE\"" = """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1918,"      + (foreign_db_name == null ? """" : "" \""DBS\"".\""NAME\"" = ? AND"")"
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1919,"      + (foreign_tbl_name == null ? """" : "" \""TBLS\"".\""TBL_NAME\"" = ? AND"")"
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1976,"      ""SELECT \""DBS\"".\""NAME\"", \""TBLS\"".\""TBL_NAME\"", \""COLUMNS_V2\"".\""COLUMN_NAME\"","""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1977,"      + ""\""KEY_CONSTRAINTS\"".\""POSITION\"", """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1978,"      + ""\""KEY_CONSTRAINTS\"".\""CONSTRAINT_NAME\"", \""KEY_CONSTRAINTS\"".\""ENABLE_VALIDATE_RELY\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1979,"      + "" FROM  \""TBLS\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1980,"      + "" INNER  JOIN \""KEY_CONSTRAINTS\"" ON \""TBLS\"".\""TBL_ID\"" = \""KEY_CONSTRAINTS\"".\""PARENT_TBL_ID\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1981,"      + "" INNER JOIN \""DBS\"" ON \""TBLS\"".\""DB_ID\"" = \""DBS\"".\""DB_ID\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1982,"      + "" INNER JOIN \""COLUMNS_V2\"" ON \""COLUMNS_V2\"".\""CD_ID\"" = \""KEY_CONSTRAINTS\"".\""PARENT_CD_ID\"" AND """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1983,"      + "" \""COLUMNS_V2\"".\""INTEGER_IDX\"" = \""KEY_CONSTRAINTS\"".\""PARENT_INTEGER_IDX\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1984,"      + "" WHERE \""KEY_CONSTRAINTS\"".\""CONSTRAINT_TYPE\"" = ""+ MConstraint.PRIMARY_KEY_CONSTRAINT + "" AND """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1985,"      + (db_name == null ? """" : ""\""DBS\"".\""NAME\"" = ? AND"")"
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1986,"      + (tbl_name == null ? """" : "" \""TBLS\"".\""TBL_NAME\"" = ? "") ;"
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,2028,"      ""SELECT \""DBS\"".\""NAME\"", \""TBLS\"".\""TBL_NAME\"", \""COLUMNS_V2\"".\""COLUMN_NAME\"","""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,2029,"      + ""\""KEY_CONSTRAINTS\"".\""POSITION\"", """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,2030,"      + ""\""KEY_CONSTRAINTS\"".\""CONSTRAINT_NAME\"", \""KEY_CONSTRAINTS\"".\""ENABLE_VALIDATE_RELY\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,2031,"      + "" FROM  \""TBLS\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,2032,"      + "" INNER  JOIN \""KEY_CONSTRAINTS\"" ON \""TBLS\"".\""TBL_ID\"" = \""KEY_CONSTRAINTS\"".\""PARENT_TBL_ID\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,2033,"      + "" INNER JOIN \""DBS\"" ON \""TBLS\"".\""DB_ID\"" = \""DBS\"".\""DB_ID\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,2034,"      + "" INNER JOIN \""COLUMNS_V2\"" ON \""COLUMNS_V2\"".\""CD_ID\"" = \""KEY_CONSTRAINTS\"".\""PARENT_CD_ID\"" AND """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,2035,"      + "" \""COLUMNS_V2\"".\""INTEGER_IDX\"" = \""KEY_CONSTRAINTS\"".\""PARENT_INTEGER_IDX\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,2036,"      + "" WHERE \""KEY_CONSTRAINTS\"".\""CONSTRAINT_TYPE\"" = ""+ MConstraint.UNIQUE_CONSTRAINT + "" AND """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,2037,"      + (db_name == null ? """" : ""\""DBS\"".\""NAME\"" = ? AND"")"
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,2038,"      + (tbl_name == null ? """" : "" \""TBLS\"".\""TBL_NAME\"" = ? "") ;"
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,2080,"      ""SELECT \""DBS\"".\""NAME\"", \""TBLS\"".\""TBL_NAME\"", \""COLUMNS_V2\"".\""COLUMN_NAME\"","""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,2081,"      + ""\""KEY_CONSTRAINTS\"".\""CONSTRAINT_NAME\"", \""KEY_CONSTRAINTS\"".\""ENABLE_VALIDATE_RELY\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,2082,"      + "" FROM  \""TBLS\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,2083,"      + "" INNER  JOIN \""KEY_CONSTRAINTS\"" ON \""TBLS\"".\""TBL_ID\"" = \""KEY_CONSTRAINTS\"".\""PARENT_TBL_ID\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,2084,"      + "" INNER JOIN \""DBS\"" ON \""TBLS\"".\""DB_ID\"" = \""DBS\"".\""DB_ID\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,2085,"      + "" INNER JOIN \""COLUMNS_V2\"" ON \""COLUMNS_V2\"".\""CD_ID\"" = \""KEY_CONSTRAINTS\"".\""PARENT_CD_ID\"" AND """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,2086,"      + "" \""COLUMNS_V2\"".\""INTEGER_IDX\"" = \""KEY_CONSTRAINTS\"".\""PARENT_INTEGER_IDX\"" """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,2087,"      + "" WHERE \""KEY_CONSTRAINTS\"".\""CONSTRAINT_TYPE\"" = ""+ MConstraint.NOT_NULL_CONSTRAINT + "" AND """
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,2088,"      + (db_name == null ? """" : ""\""DBS\"".\""NAME\"" = ? AND"")"
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,2089,"      + (tbl_name == null ? """" : "" \""TBLS\"".\""TBL_NAME\"" = ? "") ;"
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,418,"        directSql = new MetaStoreDirectSql(pm, hiveConf);"
