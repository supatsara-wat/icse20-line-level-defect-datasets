File,LineNumber,src
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,344,  private int getNumBitVectorsForNDVEstimation(HiveConf conf) {
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,348,    if (percentageError <= 2.4) {
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,354,    } else if (percentageError <= 6.8) {
ql/src/java/org/apache/hadoop/hive/ql/exec/AbstractMapJoinOperator.java,89,"    JoinUtil.populateJoinKeyValue(joinKeys, conf.getKeys(),order,NOTSKIPBIGTABLE);"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,250,"        order,NOTSKIPBIGTABLE);"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,254,"    JoinUtil.populateJoinKeyValue(joinFilters, conf.getFilters(),order,NOTSKIPBIGTABLE);"
ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java,74,  protected transient int posBigTableTag = -1; // one of the tables that is not in memory
ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java,189,    posBigTableTag = conf.getPosBigTable();
ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java,193,    posBigTableAlias = order[posBigTableTag];
ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java,194,
ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java,205,"    JoinUtil.populateJoinKeyValue(joinKeys, conf.getKeys(), order, posBigTableAlias);"
ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java,213,"    JoinUtil.populateJoinKeyValue(joinValues, conf.getExprs(), order, posBigTableAlias);"
ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java,221,"    JoinUtil.populateJoinKeyValue(joinFilters, conf.getFilters(), order, posBigTableAlias);"
ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java,262,      if (pos == posBigTableTag) {
ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java,316,      alias = order[tag];
ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java,317,      // alias = (byte)tag;
ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java,328,"      HashMapWrapper<AbstractMapJoinKey, MapJoinObjectValue> hashTable = mapJoinTables"
ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java,329,          .get((byte) tag);
ql/src/java/org/apache/hadoop/hive/ql/exec/JoinUtil.java,110,
ql/src/java/org/apache/hadoop/hive/ql/exec/JoinUtil.java,112,
ql/src/java/org/apache/hadoop/hive/ql/exec/JoinUtil.java,113,"    Iterator<Map.Entry<Byte, List<ExprNodeDesc>>> entryIter = inputMap"
ql/src/java/org/apache/hadoop/hive/ql/exec/JoinUtil.java,114,        .entrySet().iterator();
ql/src/java/org/apache/hadoop/hive/ql/exec/JoinUtil.java,115,    while (entryIter.hasNext()) {
ql/src/java/org/apache/hadoop/hive/ql/exec/JoinUtil.java,116,"      Map.Entry<Byte, List<ExprNodeDesc>> e = entryIter.next();"
ql/src/java/org/apache/hadoop/hive/ql/exec/JoinUtil.java,117,      Byte key = order[e.getKey()];
ql/src/java/org/apache/hadoop/hive/ql/exec/JoinUtil.java,118,
ql/src/java/org/apache/hadoop/hive/ql/exec/JoinUtil.java,120,
ql/src/java/org/apache/hadoop/hive/ql/exec/JoinUtil.java,121,      List<ExprNodeDesc> expr = e.getValue();
ql/src/java/org/apache/hadoop/hive/ql/exec/JoinUtil.java,122,      int sz = expr.size();
ql/src/java/org/apache/hadoop/hive/ql/exec/JoinUtil.java,123,      total += sz;
ql/src/java/org/apache/hadoop/hive/ql/exec/JoinUtil.java,124,
ql/src/java/org/apache/hadoop/hive/ql/exec/JoinUtil.java,125,      for (int j = 0; j < sz; j++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/JoinUtil.java,126,        if(key == (byte) posBigTableAlias){
ql/src/java/org/apache/hadoop/hive/ql/exec/JoinUtil.java,128,        }else{
ql/src/java/org/apache/hadoop/hive/ql/exec/JoinUtil.java,129,          valueFields.add(ExprNodeEvaluatorFactory.get(expr.get(j)));
ql/src/java/org/apache/hadoop/hive/ql/exec/JoinUtil.java,132,
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,228,      // get alias
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,229,      alias = order[tag];
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,230,      // alias = (byte)tag;
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,245,      storage.get((byte) tag).add(value);
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,248,        if (pos.intValue() != tag) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java,305,    int pos = 0;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java,325,      Byte tag = (byte) rsconf.getTag();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java,327,"      keyExprMap.put(tag, keys);"
ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java,331,"      columnTransfer.put(tag, map);"
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/LocalMapJoinProcFactory.java,135,      Byte[] order = mapJoinOp.getConf().getTagOrder();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/LocalMapJoinProcFactory.java,136,      int bigTableAlias = (int) order[bigTable];
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/LocalMapJoinProcFactory.java,146,        if (i == bigTableAlias) {
ql/src/java/org/apache/hadoop/hive/ql/exec/AbstractMapJoinOperator.java,57,  protected transient int posBigTable = -1; // one of the tables that is not in memory
ql/src/java/org/apache/hadoop/hive/ql/exec/AbstractMapJoinOperator.java,96,    posBigTable = conf.getPosBigTable();
ql/src/java/org/apache/hadoop/hive/ql/exec/AbstractMapJoinOperator.java,101,"        rowContainerStandardObjectInspectors.get((byte) posBigTable),"
ql/src/java/org/apache/hadoop/hive/ql/exec/AbstractMapJoinOperator.java,102,"        order[posBigTable], joinCacheSize,spillTableDesc, conf,"
ql/src/java/org/apache/hadoop/hive/ql/exec/AbstractMapJoinOperator.java,104,"    storage.put((byte) posBigTable, bigPosRC);"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,249,    Byte[] reorder = getExecContext() == null ? order : null;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,251,"        reorder,NOTSKIPBIGTABLE);"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,255,"    JoinUtil.populateJoinKeyValue(joinFilters, conf.getFilters(),reorder,NOTSKIPBIGTABLE);"
ql/src/java/org/apache/hadoop/hive/ql/exec/ConditionalTask.java,87,"        console.printInfo(HadoopJobExecHelper.getJobEndMsg("""" + Utilities.randGen.nextInt())"
ql/src/java/org/apache/hadoop/hive/ql/exec/ConditionalTask.java,88,"            + "", job is filtered out (removed at runtime)."");"
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,32,import org.apache.hadoop.hive.conf.HiveConf;
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,68,  private int bigTableAlias;
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,88,    bigTableAlias = order[posBigTable];
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,125,    // index for values is just alias
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,126,    for (int tag = 0; tag < order.length; tag++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,127,      int alias = (int) order[tag];
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,128,
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,129,      if (alias == this.bigTableAlias) {
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,132,
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,133,
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,136,        valueTableDesc = conf.getValueTblDescs().get(tag);
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,138,        valueTableDesc = conf.getValueFilteredTblDescs().get(tag);
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,144,"      MapJoinMetaData.put(Integer.valueOf(alias), new HashTableSinkObjectCtx(ObjectInspectorUtils"
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,245,      for (Byte pos : order) {
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,246,        if (pos.intValue() != alias) {
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,256,"              storage.put(pos, dummyObjVectors[pos.intValue()]);"
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,271,      for (Byte pos : order) {
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,272,        if (pos.intValue() != tag) {
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,99,    for (Byte alias: order) {
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,100,      if (alias > maxAlias) {
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,101,        maxAlias = alias;
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,115,    byte storePos = (byte) 0;
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,116,    for (Byte alias : order) {
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,118,"          rowContainerStandardObjectInspectors.get(storePos),"
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,119,"          alias, bucketSize,spillTableDesc, conf, !hasFilter(storePos),"
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,121,      nextGroupStorage[storePos] = rc;
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,123,"          rowContainerStandardObjectInspectors.get((byte)storePos),"
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,124,"          alias,bucketSize,spillTableDesc, conf, !hasFilter(storePos),"
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,126,      candidateStorage[alias] = candidateRC;
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,127,      storePos++;
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,131,    for (Byte alias : order) {
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,132,      if(alias != (byte) posBigTable) {
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,133,        fetchDone[alias] = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,135,      foundNextKeyGroup[alias] = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,236,      for (Byte t : order) {
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,237,        if(t != (byte)posBigTable) {
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,238,          fetchNextGroup(t);
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,271,      assert tag == (byte)posBigTable;
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,277,          && !smallestPos.contains((byte)this.posBigTable));
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,316,      for (byte t : order) {
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,317,        if (this.foundNextKeyGroup[t]
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,318,            && this.nextKeyWritables[t] != null) {
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,319,          promoteNextGroupToCandidate(t);
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,324,      for (byte r : order) {
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,325,        if (this.candidateStorage[r].size() > 0) {
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,335,    for (Byte tag : order) {
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,336,      if(tag == (byte) posBigTable) {
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,339,      allFetchDone = allFetchDone && fetchDone[tag];
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,401,    if(t == (byte)posBigTable) {
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,466,    for (byte i : order) {
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,467,      ArrayList<Object> key = keyWritables[i];
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,473,        result[i] = -1;
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,476,"      result[i] = compareKeys(key, smallestOne);"
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,477,      if (result[i] < 0) {
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,571,      for (Byte t : order) {
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,572,        if(t != (byte)posBigTable) {
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,573,          fetchNextGroup(t);
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,582,    for (Byte alias : order) {
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,583,      if(alias != (byte) posBigTable) {
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,584,        fetchDone[alias] = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,586,      foundNextKeyGroup[alias] = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java,88,  private transient boolean hasVC;
ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java,356,          this.hasVC = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java,620,      if (this.hasVC) {
ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java,652,      if (this.hasVC) {
ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java,663,        if (this.hasVC) {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1606,      String tbl = parts.get(0).getTableName();
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1607,"      logInfo(""add_partitions : db="" + db + "" tbl="" + tbl);"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1618,        success = true;
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1619,        ms.commitTransaction();
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1776,        for (MetaStoreEventListener listener : listeners) {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1777,          AddPartitionEvent addPartitionEvent =
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1778,"              new AddPartitionEvent(tbl, part, success, this);"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1779,          addPartitionEvent.setEnvironmentContext(envContext);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1780,          listener.onAddPartition(addPartitionEvent);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1781,        }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,24,import java.util.Iterator;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,55,  /**
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,56,   * IntermediateObject.
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,57,   *
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,58,   */
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,59,  public static class IntermediateObject {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,60,    ArrayList<Object>[] objs;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,61,    int curSize;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,62,
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,63,"    public IntermediateObject(ArrayList<Object>[] objs, int curSize) {"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,64,      this.objs = objs;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,65,      this.curSize = curSize;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,66,    }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,67,
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,68,    public ArrayList<Object>[] getObjs() {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,69,      return objs;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,70,    }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,71,
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,72,    public int getCurSize() {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,73,      return curSize;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,74,    }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,75,
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,76,    public void pushObj(ArrayList<Object> newObj) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,77,      objs[curSize++] = newObj;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,78,    }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,79,
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,80,    public void popObj() {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,81,      curSize--;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,82,    }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,83,
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,84,    public Object topObj() {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,85,      return objs[curSize - 1];
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,86,    }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,87,  }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,88,
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,100,  protected transient int[][] filterMap;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,126,"  protected transient Object[] dummyObj; // for outer joins, contains the"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,127,  // potential nulls for the concerned
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,128,  // aliases
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,129,  protected transient RowContainer<ArrayList<Object>>[] dummyObjVectors; // empty
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,130,  // rows
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,131,  // for
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,132,  // each
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,133,  // table
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,267,    filterMap = conf.getFilterMap();
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,285,    dummyObj = new Object[numAliases];
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,317,      values.add((ArrayList<Object>) dummyObj[pos]);
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,341,"    LOG.info(""JOIN """
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,342,        + outputObjInspector.getTypeName()
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,343,"        + "" totalsz = "" + totalSz);"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,344,
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,348,
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,349,
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,350,transient boolean newGroupStarted = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,370,
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,371,  transient Object[] forwardCache;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,372,
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,373,"  private void createForwardJoinObject(IntermediateObject intObj,"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,374,      boolean[] nullsArr) throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,375,    int p = 0;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,377,      Byte alias = order[i];
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,378,      int sz = joinValues[alias].size();
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,379,      if (nullsArr[i]) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,380,        for (int j = 0; j < sz; j++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,381,          forwardCache[p++] = null;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,382,        }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,383,      } else {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,384,        ArrayList<Object> obj = intObj.getObjs()[i];
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,385,        for (int j = 0; j < sz; j++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,386,          forwardCache[p++] = obj.get(j);
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,390,
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,391,"    forward(forwardCache, outputObjInspector);"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,392,    countAfterReport = 0;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,393,  }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,394,
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,395,"  private void copyOldArray(boolean[] src, boolean[] dest) {"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,396,    for (int i = 0; i < src.length; i++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,397,      dest[i] = src[i];
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,401,  private ArrayList<boolean[]> joinObjectsInnerJoin(
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,402,"      ArrayList<boolean[]> resNulls, ArrayList<boolean[]> inputNulls,"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,403,"      ArrayList<Object> newObj, IntermediateObject intObj, int left,"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,404,      boolean newObjNull) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,405,    if (newObjNull) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,406,      return resNulls;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,407,    }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,408,    Iterator<boolean[]> nullsIter = inputNulls.iterator();
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,409,    while (nullsIter.hasNext()) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,410,      boolean[] oldNulls = nullsIter.next();
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,411,      boolean oldObjNull = oldNulls[left];
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,412,      if (!oldObjNull) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,413,        boolean[] newNulls = new boolean[intObj.getCurSize()];
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,414,"        copyOldArray(oldNulls, newNulls);"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,415,        newNulls[oldNulls.length] = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,416,        resNulls.add(newNulls);
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,418,    }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,419,    return resNulls;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,420,  }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,422,  /**
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,423,   * Implement semi join operator.
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,424,   */
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,425,  private ArrayList<boolean[]> joinObjectsLeftSemiJoin(
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,426,"      ArrayList<boolean[]> resNulls, ArrayList<boolean[]> inputNulls,"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,427,"      ArrayList<Object> newObj, IntermediateObject intObj, int left,"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,428,      boolean newObjNull) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,429,    if (newObjNull) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,430,      return resNulls;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,432,    Iterator<boolean[]> nullsIter = inputNulls.iterator();
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,433,    while (nullsIter.hasNext()) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,434,      boolean[] oldNulls = nullsIter.next();
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,435,      boolean oldObjNull = oldNulls[left];
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,436,      if (!oldObjNull) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,437,        boolean[] newNulls = new boolean[intObj.getCurSize()];
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,438,"        copyOldArray(oldNulls, newNulls);"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,439,        newNulls[oldNulls.length] = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,440,        resNulls.add(newNulls);
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,441,      }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,442,    }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,443,    return resNulls;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,444,  }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,445,
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,446,  private ArrayList<boolean[]> joinObjectsLeftOuterJoin(
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,447,"      ArrayList<boolean[]> resNulls, ArrayList<boolean[]> inputNulls,"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,448,"      ArrayList<Object> newObj, IntermediateObject intObj, int left, int right,"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,449,      boolean newObjNull) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,450,    // newObj is null if is already null or
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,451,    // if the row corresponding to the left alias does not pass through filter
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,452,"    newObjNull |= isLeftFiltered(left, right, intObj.getObjs()[left]);"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,453,
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,454,    Iterator<boolean[]> nullsIter = inputNulls.iterator();
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,455,    while (nullsIter.hasNext()) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,456,      boolean[] oldNulls = nullsIter.next();
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,457,      boolean oldObjNull = oldNulls[left];
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,458,      boolean[] newNulls = new boolean[intObj.getCurSize()];
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,459,"      copyOldArray(oldNulls, newNulls);"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,460,      if (oldObjNull) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,461,        newNulls[oldNulls.length] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,462,      } else {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,463,        newNulls[oldNulls.length] = newObjNull;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,464,      }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,465,      resNulls.add(newNulls);
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,466,    }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,467,    return resNulls;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,470,  private ArrayList<boolean[]> joinObjectsRightOuterJoin(
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,471,"      ArrayList<boolean[]> resNulls, ArrayList<boolean[]> inputNulls,"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,472,"      ArrayList<Object> newObj, IntermediateObject intObj, int left, int right,"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,473,"      boolean newObjNull, boolean firstRow) {"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,474,    if (newObjNull) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,475,      return resNulls;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,476,    }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,477,
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,478,    if (inputNulls.isEmpty() && firstRow) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,479,      boolean[] newNulls = new boolean[intObj.getCurSize()];
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,480,      for (int i = 0; i < intObj.getCurSize() - 1; i++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,481,        newNulls[i] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,482,      }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,483,      newNulls[intObj.getCurSize() - 1] = newObjNull;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,484,      resNulls.add(newNulls);
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,485,      return resNulls;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,486,    }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,488,    boolean allOldObjsNull = firstRow;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,490,    Iterator<boolean[]> nullsIter = inputNulls.iterator();
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,491,    while (nullsIter.hasNext()) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,492,      boolean[] oldNulls = nullsIter.next();
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,493,      if (!oldNulls[left]) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,494,        allOldObjsNull = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,495,        break;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,496,      }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,497,    }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,498,
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,499,"    // if the row does not pass through filter, all old Objects are null"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,500,"    if (isRightFiltered(left, right, newObj)) {"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,501,      allOldObjsNull = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,502,    }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,503,    nullsIter = inputNulls.iterator();
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,504,    while (nullsIter.hasNext()) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,505,      boolean[] oldNulls = nullsIter.next();
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,506,      boolean oldObjNull = oldNulls[left] || allOldObjsNull;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,507,
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,508,      if (!oldObjNull) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,509,        boolean[] newNulls = new boolean[intObj.getCurSize()];
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,510,"        copyOldArray(oldNulls, newNulls);"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,511,        newNulls[oldNulls.length] = newObjNull;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,512,        resNulls.add(newNulls);
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,513,      } else if (allOldObjsNull) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,514,        boolean[] newNulls = new boolean[intObj.getCurSize()];
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,515,        for (int i = 0; i < intObj.getCurSize() - 1; i++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,516,          newNulls[i] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,517,        }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,518,        newNulls[oldNulls.length] = newObjNull;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,519,        resNulls.add(newNulls);
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,520,        return resNulls;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,521,      }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,522,    }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,523,    return resNulls;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,524,  }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,525,
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,526,  private ArrayList<boolean[]> joinObjectsFullOuterJoin(
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,527,"      ArrayList<boolean[]> resNulls, ArrayList<boolean[]> inputNulls,"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,528,"      ArrayList<Object> newObj, IntermediateObject intObj, int left, int right,"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,529,"      boolean newObjNull, boolean firstRow) {"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,530,    if (newObjNull) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,531,      Iterator<boolean[]> nullsIter = inputNulls.iterator();
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,532,      while (nullsIter.hasNext()) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,533,        boolean[] oldNulls = nullsIter.next();
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,534,        boolean[] newNulls = new boolean[intObj.getCurSize()];
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,535,"        copyOldArray(oldNulls, newNulls);"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,536,        newNulls[oldNulls.length] = newObjNull;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,537,        resNulls.add(newNulls);
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,538,      }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,539,      return resNulls;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,540,    }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,542,    if (inputNulls.isEmpty() && firstRow) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,543,      boolean[] newNulls = new boolean[intObj.getCurSize()];
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,544,      for (int i = 0; i < intObj.getCurSize() - 1; i++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,545,        newNulls[i] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,546,      }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,547,      newNulls[intObj.getCurSize() - 1] = newObjNull;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,548,      resNulls.add(newNulls);
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,549,      return resNulls;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,550,    }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,552,    boolean allOldObjsNull = firstRow;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,554,    Iterator<boolean[]> nullsIter = inputNulls.iterator();
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,555,    while (nullsIter.hasNext()) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,556,      boolean[] oldNulls = nullsIter.next();
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,557,      if (!oldNulls[left]) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,558,        allOldObjsNull = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,559,        break;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,560,      }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,561,    }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,563,"    // if the row does not pass through filter, all old Objects are null"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,564,"    if (isRightFiltered(left, right, newObj)) {"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,565,      allOldObjsNull = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,566,    }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,567,    boolean rhsPreserved = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,568,
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,569,    nullsIter = inputNulls.iterator();
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,570,    while (nullsIter.hasNext()) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,571,      boolean[] oldNulls = nullsIter.next();
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,572,      // old obj is null even if the row corresponding to the left alias
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,573,      // does not pass through filter
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,574,      boolean oldObjNull = oldNulls[left] || allOldObjsNull
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,575,"          || isLeftFiltered(left, right, intObj.getObjs()[left]);"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,576,      if (!oldObjNull) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,577,        boolean[] newNulls = new boolean[intObj.getCurSize()];
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,578,"        copyOldArray(oldNulls, newNulls);"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,579,        newNulls[oldNulls.length] = newObjNull;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,580,        resNulls.add(newNulls);
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,581,      } else if (oldObjNull) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,582,        boolean[] newNulls = new boolean[intObj.getCurSize()];
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,583,"        copyOldArray(oldNulls, newNulls);"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,584,        newNulls[oldNulls.length] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,585,        resNulls.add(newNulls);
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,586,
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,587,        if (allOldObjsNull && !rhsPreserved) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,588,          newNulls = new boolean[intObj.getCurSize()];
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,589,          for (int i = 0; i < oldNulls.length; i++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,590,            newNulls[i] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,592,          newNulls[oldNulls.length] = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,593,          resNulls.add(newNulls);
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,594,          rhsPreserved = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,598,    return resNulls;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,601,  /*
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,602,   * The new input is added to the list of existing inputs. Each entry in the
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,603,   * array of inputNulls denotes the entries in the intermediate object to be
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,604,"   * used. The intermediate object is augmented with the new object, and list of"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,605,   * nulls is changed appropriately. The list will contain all non-nulls for a
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,606,   * inner join. The outer joins are processed appropriately.
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,607,   */
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,608,"  private ArrayList<boolean[]> joinObjects(ArrayList<boolean[]> inputNulls,"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,609,"      ArrayList<Object> newObj, IntermediateObject intObj, int joinPos,"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,610,      boolean firstRow) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,611,    ArrayList<boolean[]> resNulls = new ArrayList<boolean[]>();
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,612,    boolean newObjNull = newObj == dummyObj[joinPos] ? true : false;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,613,    if (joinPos == 0) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,614,      if (newObjNull) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,615,        return null;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,616,      }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,617,      boolean[] nulls = new boolean[1];
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,618,      nulls[0] = newObjNull;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,619,      resNulls.add(nulls);
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,620,      return resNulls;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,623,    int left = condn[joinPos - 1].getLeft();
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,624,    int right = condn[joinPos - 1].getRight();
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,625,    int type = condn[joinPos - 1].getType();
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,626,
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,627,    // process all nulls for RIGHT and FULL OUTER JOINS
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,628,    if (((type == JoinDesc.RIGHT_OUTER_JOIN) || (type == JoinDesc.FULL_OUTER_JOIN))
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,629,        && !newObjNull && (inputNulls == null) && firstRow) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,630,      boolean[] newNulls = new boolean[intObj.getCurSize()];
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,631,      for (int i = 0; i < newNulls.length - 1; i++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,632,        newNulls[i] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,633,      }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,634,      newNulls[newNulls.length - 1] = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,635,      resNulls.add(newNulls);
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,636,      return resNulls;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,638,
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,639,    if (inputNulls == null) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,640,      return null;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,642,
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,643,    if (type == JoinDesc.INNER_JOIN) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,644,"      return joinObjectsInnerJoin(resNulls, inputNulls, newObj, intObj, left,"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,645,          newObjNull);
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,646,    } else if (type == JoinDesc.LEFT_OUTER_JOIN) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,647,"      return joinObjectsLeftOuterJoin(resNulls, inputNulls, newObj, intObj,"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,648,"          left, right, newObjNull);"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,649,    } else if (type == JoinDesc.RIGHT_OUTER_JOIN) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,650,"      return joinObjectsRightOuterJoin(resNulls, inputNulls, newObj, intObj,"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,651,"          left, right, newObjNull, firstRow);"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,652,    } else if (type == JoinDesc.LEFT_SEMI_JOIN) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,653,"      return joinObjectsLeftSemiJoin(resNulls, inputNulls, newObj, intObj,"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,654,"          left, newObjNull);"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,656,
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,657,    assert (type == JoinDesc.FULL_OUTER_JOIN);
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,658,"    return joinObjectsFullOuterJoin(resNulls, inputNulls, newObj, intObj, left, right,"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,659,"        newObjNull, firstRow);"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,662,  /*
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,663,"   * genObject is a recursive function. For the inputs, a array of bitvectors is"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,664,   * maintained (inputNulls) where each entry denotes whether the element is to
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,665,   * be used or not (whether it is null or not). The size of the bitvector is
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,666,   * same as the number of inputs under consideration currently. When all inputs
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,667,"   * are accounted for, the output is forwarded appropriately."
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,668,   */
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,669,"  private void genObject(ArrayList<boolean[]> inputNulls, int aliasNum,"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,670,"      IntermediateObject intObj, boolean firstRow) throws HiveException {"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,671,    boolean childFirstRow = firstRow;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,672,    boolean skipping = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,673,
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,674,    if (aliasNum < numAliases) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,676,      // search for match in the rhs table
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,677,      AbstractRowContainer<ArrayList<Object>> aliasRes = storage[order[aliasNum]];
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,679,      for (ArrayList<Object> newObj = aliasRes.first(); newObj != null; newObj = aliasRes
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,680,          .next()) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,682,        // check for skipping in case of left semi join
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,683,        if (aliasNum > 0
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,684,            && condn[aliasNum - 1].getType() == JoinDesc.LEFT_SEMI_JOIN
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,685,            && newObj != dummyObj[aliasNum]) { // successful match
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,686,          skipping = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,687,        }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,689,        intObj.pushObj(newObj);
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,691,        // execute the actual join algorithm
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,692,"        ArrayList<boolean[]> newNulls = joinObjects(inputNulls, newObj, intObj,"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,693,"            aliasNum, childFirstRow);"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,695,        // recursively call the join the other rhs tables
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,696,"        genObject(newNulls, aliasNum + 1, intObj, firstRow);"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,698,        intObj.popObj();
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,699,        firstRow = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,701,"        // if left-semi-join found a match, skipping the rest of the rows in the"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,702,        // rhs table of the semijoin
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,703,        if (skipping) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,704,          break;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,705,        }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,706,      }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,707,    } else {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,708,      if (inputNulls == null) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,709,        return;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,710,      }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,711,      Iterator<boolean[]> nullsIter = inputNulls.iterator();
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,712,      while (nullsIter.hasNext()) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,713,        boolean[] nullsVec = nullsIter.next();
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,714,"        createForwardJoinObject(intObj, nullsVec);"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,715,      }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,716,    }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,783,          alw.add((ArrayList<Object>) dummyObj[i]);
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,821,            alw.add((ArrayList<Object>) dummyObj[i]);
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,851,"        genObject(null, 0, new IntermediateObject(new ArrayList[numAliases], 0),"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,852,            true);
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,856,  }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,857,
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,858,  // returns filter result of left object by filters associated with right alias
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,859,"  private boolean isLeftFiltered(int left, int right, List<Object> leftObj) {"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,860,    if (joinValues[order[left]].size() < leftObj.size()) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,861,      ShortWritable filter = (ShortWritable) leftObj.get(leftObj.size() - 1);
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,862,"      return JoinUtil.isFiltered(filter.get(), right);"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,863,    }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,864,    return false;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,865,  }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,866,
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,867,  // returns filter result of right object by filters associated with left alias
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,868,"  private boolean isRightFiltered(int left, int right, List<Object> rightObj) {"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,869,    if (joinValues[order[right]].size() < rightObj.size()) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,870,      ShortWritable filter = (ShortWritable) rightObj.get(rightObj.size() - 1);
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,871,"      return JoinUtil.isFiltered(filter.get(), left);"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,872,    }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,873,    return false;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,874,  }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,875,
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,876,  // returns object has any filtered tag
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,877,"  private boolean hasAnyFiltered(int alias, List<Object> row) {"
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,878,    return row == dummyObj[alias] ||
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,879,        hasFilter(alias) &&
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,880,        JoinUtil.hasAnyFiltered(((ShortWritable) row.get(row.size() - 1)).get());
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,881,  }
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,882,
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,883,  protected final boolean hasFilter(int alias) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,884,    return filterMap != null && filterMap[alias] != null;
ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java,84,  protected transient int[][] filterMap;
ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java,130,        Configuration conf) {
ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java,196,    filterMap = conf.getFilterMap();
ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java,231,        if (filterMap != null && filterMap[alias] != null) {
ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java,301,"            ObjectInspectorCopyOption.WRITABLE), keySerializer, keyTableDesc, hconf));"
ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java,323,"          joinValuesObjectInspectors[alias], joinFilters[alias], joinFilterObjectInspectors"
ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java,324,"              [alias], filterMap == null ? null : filterMap[alias]);"
ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java,325,
ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java,385,"    MapJoinMetaData.put(Integer.valueOf(metadataValueTag[tag]), new HashTableSinkObjectCtx("
ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java,386,"        standardOI, valueSerDe, valueTableDesc, hconf));"
ql/src/java/org/apache/hadoop/hive/ql/exec/JoinOperator.java,83,
ql/src/java/org/apache/hadoop/hive/ql/exec/JoinOperator.java,84,"      ArrayList<Object> nr = JoinUtil.computeValues(row, joinValues[alias],"
ql/src/java/org/apache/hadoop/hive/ql/exec/JoinOperator.java,85,"          joinValuesObjectInspectors[alias], joinFilters[alias],"
ql/src/java/org/apache/hadoop/hive/ql/exec/JoinOperator.java,86,"          joinFilterObjectInspectors[alias],"
ql/src/java/org/apache/hadoop/hive/ql/exec/JoinOperator.java,87,          filterMap == null ? null : filterMap[alias]);
ql/src/java/org/apache/hadoop/hive/ql/exec/JoinOperator.java,88,
ql/src/java/org/apache/hadoop/hive/ql/exec/JoinUtil.java,225,"      List<ExprNodeEvaluator> valueFields, List<ObjectInspector> valueFieldsOI,"
ql/src/java/org/apache/hadoop/hive/ql/exec/JoinUtil.java,226,"      List<ExprNodeEvaluator> filters, List<ObjectInspector> filtersOI,"
ql/src/java/org/apache/hadoop/hive/ql/exec/JoinUtil.java,227,      int[] filterMap) throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/exec/JoinUtil.java,230,    ArrayList<Object> nr = new ArrayList<Object>(valueFields.size());
ql/src/java/org/apache/hadoop/hive/ql/exec/JoinUtil.java,236,    if (filterMap != null) {
ql/src/java/org/apache/hadoop/hive/ql/exec/JoinUtil.java,237,      // add whether the row is filtered or not.
ql/src/java/org/apache/hadoop/hive/ql/exec/JoinUtil.java,238,"      nr.add(new ShortWritable(isFiltered(row, filters, filtersOI, filterMap)));"
ql/src/java/org/apache/hadoop/hive/ql/exec/JoinUtil.java,239,    }
ql/src/java/org/apache/hadoop/hive/ql/exec/JoinUtil.java,240,
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,121,"            ObjectInspectorCopyOption.WRITABLE), keySerializer, keyTableDesc, hconf));"
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,138,"          .getStandardObjectInspector(valueSerDe.getObjectInspector(),"
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,139,"              ObjectInspectorCopyOption.WRITABLE), valueSerDe, valueTableDesc, hconf));"
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,231,"      ArrayList<Object> value = JoinUtil.computeValues(row, joinValues[alias],"
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,232,"          joinValuesObjectInspectors[alias], joinFilters[alias], joinFilterObjectInspectors"
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,233,"              [alias], filterMap == null ? null : filterMap[alias]);"
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,234,
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,259,"    ArrayList<Object> value = JoinUtil.computeValues(row, joinValues[alias],"
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,260,"        joinValuesObjectInspectors[alias], joinFilters[alias],"
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,261,"        joinFilterObjectInspectors[alias],"
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,262,        filterMap == null ? null : filterMap[alias]);
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinObjectValue.java,43,
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinObjectValue.java,89,
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinObjectValue.java,105,              res.add(memObj.toArray());
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyTimestamp.java,69,    Timestamp t;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyTimestamp.java,71,      t = null;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyTimestamp.java,74,      t = Timestamp.valueOf(s);
ql/src/java/org/apache/hadoop/hive/ql/udf/UDFHour.java,90,    result.set(calendar.get(Calendar.HOUR));
ql/src/java/org/apache/hadoop/hive/ql/udf/UDFRound.java,51,          RoundingMode.HALF_UP).doubleValue());
ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsTask.java,297,    // Construct a column statistics object from the result
ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsTask.java,298,"    ColumnStatistics colStats = constructColumnStatsFromPackedRow(io.oi, io.o);"
ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsTask.java,300,    // Persist the column statistics object to the metastore
ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsTask.java,301,    try {
ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsTask.java,302,      db.updatePartitionColumnStatistics(colStats);
ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsTask.java,303,    } catch (Exception e) {
ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsTask.java,304,      e.printStackTrace();
ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsTask.java,320,    // Construct a column statistics object from the result
ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsTask.java,321,"    ColumnStatistics colStats = constructColumnStatsFromPackedRow(io.oi, io.o);"
ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsTask.java,323,    // Persist the column statistics object to the metastore
ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsTask.java,324,    try {
ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsTask.java,325,      db.updateTableColumnStatistics(colStats);
ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsTask.java,326,    } catch (Exception e) {
ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsTask.java,327,      e.printStackTrace();
ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsTask.java,348,      InspectableObject io = ftOp.getNextRow();
ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsTask.java,349,      if (io == null) {
ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsTask.java,350,        throw new CommandNeedRetryException();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,232,      if (p == null) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,233,        myagg.countNulls++;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,235,      else {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,236,        try {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,237,"          boolean v = PrimitiveObjectInspectorUtils.getBoolean(p, inputOI);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,238,          if (v == false) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,239,            myagg.countFalses++;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,240,          } else if (v == true){
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,241,            myagg.countTrues++;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,242,          }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,243,        } catch (NumberFormatException e) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,244,          if (!warned) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,245,            warned = true;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,246,"            LOG.warn(getClass().getSimpleName() + "" """
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,247,                + StringUtils.stringifyException(e));
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,248,            LOG.warn(getClass().getSimpleName()
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,249,"                + "" ignoring similar exceptions."");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,477,"        int numVectors = PrimitiveObjectInspectorUtils.getInt(parameters[1], numVectorsOI);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,575,      long numDV = myagg.numDV.estimateNumDistinctValues();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,775,"        int numVectors = PrimitiveObjectInspectorUtils.getInt(parameters[1], numVectorsOI);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,781,      //Update null counter if a null value is seen
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,782,      if (p == null) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,783,        myagg.countNulls++;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,784,      }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,785,      else {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,786,        try {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,787,"          double v = PrimitiveObjectInspectorUtils.getDouble(p, inputOI);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,789,          //Update min counter if new value is less than min seen so far
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,790,          if (v < myagg.min) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,791,            myagg.min = v;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,792,          }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,793,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,794,          //Update max counter if new value is greater than max seen so far
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,795,          if (v > myagg.max) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,796,            myagg.max = v;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,797,          }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,798,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,799,          // Add value to NumDistinctValue Estimator
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,800,          myagg.numDV.addToEstimator(v);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,801,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,802,        } catch (NumberFormatException e) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,803,          if (!warned) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,804,            warned = true;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,805,"            LOG.warn(getClass().getSimpleName() + "" """
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,806,                + StringUtils.stringifyException(e));
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,807,            LOG.warn(getClass().getSimpleName()
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,808,"                + "" ignoring similar exceptions."");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,873,      long numDV = myagg.numDV.estimateNumDistinctValues();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1087,"        int numVectors = PrimitiveObjectInspectorUtils.getInt(parameters[1], numVectorsOI);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1093,      // Update null counter if a null value is seen
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1094,      if (p == null) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1095,        myagg.countNulls++;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1096,      }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1097,      else {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1098,        try {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1099,"          String v = PrimitiveObjectInspectorUtils.getString(p, inputOI);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1101,          // Update max length if new length is greater than the ones seen so far
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1102,          int len = v.length();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1103,          if (len > myagg.maxLength) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1104,            myagg.maxLength = len;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1105,          }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1106,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1107,          // Update sum length with the new length
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1108,          myagg.sumLength += len;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1109,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1110,          // Increment count of values seen so far
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1111,          myagg.count++;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1112,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1113,          // Add string value to NumDistinctValue Estimator
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1114,          myagg.numDV.addToEstimator(v);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1115,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1116,        } catch (NumberFormatException e) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1117,          if (!warned) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1118,            warned = true;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1119,"            LOG.warn(getClass().getSimpleName() + "" """
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1120,                + StringUtils.stringifyException(e));
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1121,            LOG.warn(getClass().getSimpleName()
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1122,"                + "" ignoring similar exceptions."");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1189,      long numDV = myagg.numDV.estimateNumDistinctValues();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1190,      double avgLength = (double)(myagg.sumLength/(1.0 * (myagg.count + myagg.countNulls)));
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1351,      // Update null counter if a null value is seen
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1352,      if (p == null) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1353,        myagg.countNulls++;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1355,      else {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1356,        try {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1357,"          BytesWritable v = PrimitiveObjectInspectorUtils.getBinary(p, inputOI);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1358,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1359,          // Update max length if new length is greater than the ones seen so far
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1360,          int len = v.getLength();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1361,          if (len > myagg.maxLength) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1362,            myagg.maxLength = len;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1363,          }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1365,          // Update sum length with the new length
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1366,          myagg.sumLength += len;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1367,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1368,          // Increment count of values seen so far
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1369,          myagg.count++;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1370,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1371,        } catch (NumberFormatException e) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1372,          if (!warned) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1373,            warned = true;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1374,"            LOG.warn(getClass().getSimpleName() + "" """
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1375,                + StringUtils.stringifyException(e));
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1376,            LOG.warn(getClass().getSimpleName()
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1377,"                + "" ignoring similar exceptions."");"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,1443,      double avgLength = (double)(myagg.sumLength/(1.0 * (myagg.count + myagg.countNulls)));
ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java,239,"  JOINNODE_OUTERJOIN_MORETHAN_32(10142, ""Single join node containing outer join(s) "" +"
ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java,240,"      ""cannot have more than 32 aliases""),"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6536,"  private void mergeJoins(QB qb, QBJoinTree parent, QBJoinTree node,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6537,"      QBJoinTree target, int pos) {"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6622,    if (qb.getQbJoinTree() == node) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6623,      qb.setQbJoinTree(node.getJoinSrc());
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6624,    } else {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6625,      parent.setJoinSrc(node.getJoinSrc());
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6626,    }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6627,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6712,"  private boolean mergeJoinNodes(QB qb, QBJoinTree parent, QBJoinTree node,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6713,      QBJoinTree target) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6714,    if (target == null) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6715,      return false;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6717,    if (!node.getNoOuterJoin() || !target.getNoOuterJoin()) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6718,      // todo 8 way could be not enough number
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6719,      if (node.getLeftAliases().length + node.getRightAliases().length + 1 >= 32) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6720,        LOG.info(ErrorMsg.JOINNODE_OUTERJOIN_MORETHAN_32);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6721,        return false;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6724,"    int res = findMergePos(node, target);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6725,    if (res != -1) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6726,"      mergeJoins(qb, parent, node, target, res);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6727,      return true;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6729,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6730,"    return mergeJoinNodes(qb, parent, node, target.getJoinSrc());"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6733,  private void mergeJoinTree(QB qb) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6734,    QBJoinTree root = qb.getQbJoinTree();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6735,    QBJoinTree parent = null;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6736,    while (root != null) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6737,"      boolean merged = mergeJoinNodes(qb, parent, root, root.getJoinSrc());"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6738,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6739,      if (parent == null) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6740,        if (merged) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6741,          root = qb.getQbJoinTree();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6742,        } else {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6743,          parent = root;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6744,          root = root.getJoinSrc();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6745,        }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6746,      } else {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6747,        if (merged) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6748,          root = root.getJoinSrc();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6749,        } else {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6750,          parent = parent.getJoinSrc();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6751,          root = parent.getJoinSrc();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6752,        }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,7311,"      curr = genFileSinkPlan(dest, qb, curr);"
serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerDe.java,64,      // force output files to have a .avro extension
serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerDe.java,65,"      configuration.set(""hive.output.file.extension"", "".avro"");"
hbase-handler/src/java/org/apache/hadoop/hive/hbase/LazyHBaseRow.java,136,      fieldsInited[fieldID] = true;
hwi/src/test/org/apache/hadoop/hive/hwi/TestHWISessionManager.java,124,"    // ""describe [table_name]"" result format"
hwi/src/test/org/apache/hadoop/hive/hwi/TestHWISessionManager.java,125,    // first line should be format name:
hwi/src/test/org/apache/hadoop/hive/hwi/TestHWISessionManager.java,126,"    // ""# col_name             data_type               comment"""
hwi/src/test/org/apache/hadoop/hive/hwi/TestHWISessionManager.java,127,    // second line is empty
hwi/src/test/org/apache/hadoop/hive/hwi/TestHWISessionManager.java,128,    // the following lines contain the values
hwi/src/test/org/apache/hadoop/hive/hwi/TestHWISessionManager.java,129,    String resLine = searchBlockRes.get(0).get(2);
hwi/src/test/org/apache/hadoop/hive/hwi/TestHWISessionManager.java,132,    String resLine2 = searchBlockRes.get(0).get(3);
jdbc/src/test/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java,769,
jdbc/src/test/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java,770,"    // ""describe [table_name]"" result format"
jdbc/src/test/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java,771,    // first line should be format name:
jdbc/src/test/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java,772,"    // ""# col_name             data_type               comment"""
jdbc/src/test/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java,773,    // second line is empty
jdbc/src/test/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java,774,    // the following lines contain the values
jdbc/src/test/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java,775,    res.next();
jdbc/src/test/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java,776,"    assertEquals(true, res.getString(1).contains(""col_name""));"
jdbc/src/test/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java,777,"    assertEquals(true, res.getString(2).contains(""data_type""));"
jdbc/src/test/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java,778,"    assertEquals(true, res.getString(3).contains(""comment""));"
jdbc/src/test/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java,779,    res.next();
jdbc/src/test/org/apache/hive/jdbc/TestJdbcDriver2.java,820,    res.next(); // skip header 1
jdbc/src/test/org/apache/hive/jdbc/TestJdbcDriver2.java,821,    res.next(); // skip header 2
jdbc/src/test/org/apache/hive/jdbc/TestJdbcDriver2.java,822,
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,115,import org.apache.hadoop.hive.ql.plan.AlterTableAlterPartDesc;
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2326,      outStream.writeBytes(MetaDataFormatUtils.getAllColumnsInformation(cols));
ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/MetaDataFormatUtils.java,66,  public static String getAllColumnsInformation(List<FieldSchema> cols) {
ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/MetaDataFormatUtils.java,68,    formatColumnsHeader(columnInformation);
ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/MetaDataFormatUtils.java,73,"  public static String getAllColumnsInformation(List<FieldSchema> cols, List<FieldSchema> partCols) {"
ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/MetaDataFormatUtils.java,75,    formatColumnsHeader(columnInformation);
ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/TextMetaDataFormatter.java,22,import java.io.OutputStream;
ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/TextMetaDataFormatter.java,146,"                  MetaDataFormatUtils.getAllColumnsInformation(cols, partCols)"
ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/TextMetaDataFormatter.java,149,            outStream.writeBytes(MetaDataFormatUtils.getAllColumnsInformation(cols));
ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/TextMetaDataFormatter.java,458,            if (comment != null)
ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/TextMetaDataFormatter.java,459,                outStream.writeBytes(comment);
ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/TextMetaDataFormatter.java,461,            if (location != null)
ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/TextMetaDataFormatter.java,462,                outStream.writeBytes(location);
beeline/src/java/org/apache/hive/beeline/BeeLine.java,380,"    beeLine.begin(args, inputStream);"
beeline/src/java/org/apache/hive/beeline/BeeLine.java,382,    // exit the system: useful for Hypersonic and other
beeline/src/java/org/apache/hive/beeline/BeeLine.java,383,    // badly-behaving systems
beeline/src/java/org/apache/hive/beeline/BeeLine.java,385,      System.exit(0);
beeline/src/java/org/apache/hive/beeline/BeeLine.java,611,"  public void begin(String[] args, InputStream inputStream) throws IOException {"
beeline/src/java/org/apache/hive/beeline/BeeLine.java,621,      return;
ql/src/java/org/apache/hadoop/hive/ql/ppd/OpProcFactory.java,505,      Set<String> aliases = owi.getRowResolver(nd).getTableNames();
ql/src/java/org/apache/hadoop/hive/ql/ppd/OpProcFactory.java,507,"      if (aliases.size() == 1 && aliases.contains("""")) {"
ql/src/java/org/apache/hadoop/hive/ql/ppd/OpProcFactory.java,508,        // Reduce sink of group by operator
ql/src/java/org/apache/hadoop/hive/ql/ppd/OpProcFactory.java,509,        ignoreAliases = true;
serde/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/serde/test/InnerStruct.java,229,    return 0;
serde/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/serde/test/ThriftTestObj.java,365,    return 0;
serde/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/serde2/thrift/test/Complex.java,604,    return 0;
serde/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/serde2/thrift/test/Complex.java,862,                  String _val12; // optional
serde/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/serde2/thrift/test/Complex.java,1085,            String _val33; // optional
serde/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/serde2/thrift/test/IntString.java,345,    return 0;
serde/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/serde2/thrift/test/MegaStruct.java,1569,    return 0;
serde/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/serde2/thrift/test/MegaStruct.java,2092,                  String _val3; // optional
serde/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/serde2/thrift/test/MegaStruct.java,2112,                  MyEnum _val7; // optional
serde/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/serde2/thrift/test/MegaStruct.java,2132,                  String _val11; // optional
serde/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/serde2/thrift/test/MegaStruct.java,2152,                  MiniStruct _val15; // optional
serde/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/serde2/thrift/test/MegaStruct.java,2173,                  List<String> _val19; // optional
serde/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/serde2/thrift/test/MegaStruct.java,2203,                  List<MiniStruct> _val26; // optional
serde/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/serde2/thrift/test/MegaStruct.java,2855,            String _val79; // optional
serde/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/serde2/thrift/test/MegaStruct.java,2870,            MyEnum _val83; // optional
serde/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/serde2/thrift/test/MegaStruct.java,2885,            String _val87; // optional
serde/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/serde2/thrift/test/MegaStruct.java,2900,            MiniStruct _val91; // optional
serde/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/serde2/thrift/test/MegaStruct.java,2916,            List<String> _val95; // optional
serde/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/serde2/thrift/test/MegaStruct.java,2940,            List<MiniStruct> _val102; // optional
serde/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/serde2/thrift/test/MiniStruct.java,289,    return 0;
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,167,      return operation.getHandle();
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,179,      return operation.getHandle();
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,191,      return operation.getHandle();
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,204,      return operation.getHandle();
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,218,      return operation.getHandle();
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,230,      return operation.getHandle();
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,243,    return operation.getHandle();
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,256,      return operation.getHandle();
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,303,      sessionManager.getOperationManager().closeOperation(opHandle);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,344,}
beeline/src/java/org/apache/hive/beeline/Commands.java,774,      } catch (Exception e) {
beeline/src/java/org/apache/hive/beeline/Commands.java,775,        beeLine.error(e);
beeline/src/java/org/apache/hive/beeline/Commands.java,776,        throw e;
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,908,        Configuration conf = new Configuration();
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,1167,      if (conf.get(var.varname) != null) {
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,1168,"        l4j.debug(""Overriding Hadoop conf property "" + var.varname + ""='"" + conf.get(var.varname)"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,1169,"                  + ""' with Hive default value '"" + var.defaultVal +""'"");"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,1170,      }
ql/src/java/org/apache/hadoop/hive/ql/plan/HiveOperation.java,24,
ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java,85,"        path = new Path(System.getProperty(""user.dir""), path).toUri().toString();"
ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java,234,"      rTask = TaskFactory.get(new CopyWork(fromURI.toString(), copyURIStr),"
ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java,235,          conf);
ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java,264,"    LoadTableDesc loadTableWork = new LoadTableDesc(fromURI.toString(),"
ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java,265,"        loadTmpPath, Utilities.getTableDesc(ts.tableHandle), partSpec, isOverWrite);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,31,  private final int bitVectorSize = 32;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,56,    aValue = new Random(79798);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,57,    bValue = new Random(34115);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,79,        a[i] = a[i] + (1 << (bitVectorSize -1));
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,83,        b[i] = b[i] + (1 << (bitVectorSize -1));
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,200,    int mod = 1 << (bitVectorSize - 1) - 1;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,201,    long tempHash = a[hashNum] * v + b[hashNum];
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,209,      hash = hash + mod + 1;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,292,  /* We use two estimators - one due to Flajolet-Martin and a modification due to
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,293,   * Alon-Matias-Szegedy. FM uses the location of the least significant zero as an estimate of
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,294,   * log2(phi*ndvs).
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,295,   * AMS uses the location of the most significant one as an estimate of the log2(ndvs).
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,296,   * We average the two estimators with suitable modifications to obtain an estimate of ndvs.
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,300,    int sumMostSigOne = 0;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,302,    double avgMostSigOne;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,308,      int mostSigOne = bitVectorSize;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,309,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,310,      for (int j=0; j< bitVectorSize; j++) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,311,        if (bitVector[i].get(j)) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,312,          mostSigOne = j;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,313,        }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,314,      }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,315,      sumMostSigOne += mostSigOne;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,320,    avgMostSigOne = (double)(sumMostSigOne/(numBitVectors * 1.0));
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,321,"    numDistinctValues = Math.pow(2.0, (avgMostSigOne + avgLeastSigZero)/2.0);"
ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java,242,"  INVALID_JDO_FILTER_EXPRESSION(10043, ""Invalid expression for JDO filter""),"
ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java,324,"  TRUNCATE_COLUMN_INDEXED_TABLE(10227, ""Can not truncate columns from table with indexes""),"
ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java,325,"  TRUNCATE_COLUMN_NOT_RC(10228, ""Only RCFileFormat supports column truncation.""),"
ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java,326,"  TRUNCATE_COLUMN_ARCHIVED(10229, ""Column truncation cannot be performed on archived partitions.""),"
ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java,327,"  TRUNCATE_BUCKETED_COLUMN(10230,"
ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java,328,"      ""A column on which a partition/table is bucketed cannot be truncated.""),"
ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java,329,"  TRUNCATE_LIST_BUCKETED_COLUMN(10231,"
ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java,330,"      ""A column on which a partition/table is list bucketed cannot be truncated.""),"
jdbc/src/java/org/apache/hive/jdbc/HiveResultSetMetaData.java,85,    if (columnTypes == null) {
jdbc/src/java/org/apache/hive/jdbc/HiveResultSetMetaData.java,86,      throw new SQLException(
jdbc/src/java/org/apache/hive/jdbc/HiveResultSetMetaData.java,87,"          ""Could not determine column type name for ResultSet"");"
jdbc/src/java/org/apache/hive/jdbc/HiveResultSetMetaData.java,88,    }
jdbc/src/java/org/apache/hive/jdbc/HiveResultSetMetaData.java,89,
jdbc/src/java/org/apache/hive/jdbc/HiveResultSetMetaData.java,90,    if (column < 1 || column > columnTypes.size()) {
jdbc/src/java/org/apache/hive/jdbc/HiveResultSetMetaData.java,91,"      throw new SQLException(""Invalid column value: "" + column);"
jdbc/src/java/org/apache/hive/jdbc/HiveResultSetMetaData.java,92,    }
jdbc/src/java/org/apache/hive/jdbc/HiveResultSetMetaData.java,158,"    throw new SQLException(""Method not supported"");"
hbase-handler/src/test/org/apache/hadoop/hive/hbase/TestLazyHBaseObject.java,61,  public void testLazyHBaseCellMap1() {
hbase-handler/src/test/org/apache/hadoop/hive/hbase/TestLazyHBaseObject.java,122,  public void testLazyHBaseCellMap2() {
hbase-handler/src/test/org/apache/hadoop/hive/hbase/TestLazyHBaseObject.java,184,  public void testLazyHBaseCellMap3() {
hbase-handler/src/test/org/apache/hadoop/hive/hbase/TestLazyHBaseObject.java,454,  public void testLazyHBaseRow1() {
hbase-handler/src/test/org/apache/hadoop/hive/hbase/TestLazyHBaseObject.java,577,  public void testLazyHBaseRow2() {
hbase-handler/src/test/org/apache/hadoop/hive/hbase/TestLazyHBaseObject.java,699,  public void testLazyHBaseRow3() {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyFactory.java,52,import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyFactory.java,209,      byte escapeChar) {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyFactory.java,223,"          separator[separatorIndex], separator[separatorIndex + 1],"
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyFactory.java,229,"          nullSequence, escaped, escapeChar), separator[separatorIndex],"
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyFactory.java,244,"          fieldNames, fieldObjectInspectors, separator[separatorIndex],"
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyFactory.java,255,"          separator[separatorIndex], nullSequence, escaped, escapeChar);"
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyFactory.java,274,      byte escapeChar) {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyFactory.java,295,"      Text nullSequence, boolean escaped, byte escapeChar) {"
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazySimpleSerDe.java,212,"    // Read the separators: We use 8 levels of separators by default, but we"
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazySimpleSerDe.java,213,    // should change this when we allow users to specify more than 10 levels
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazySimpleSerDe.java,214,    // of separators through DDL.
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazySimpleSerDe.java,222,    for (int i = 3; i < serdeParams.separators.length; i++) {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazySimpleSerDe.java,223,      serdeParams.separators[i] = (byte) (i + 1);
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazySimpleSerDe.java,417,      throws IOException {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazySimpleSerDe.java,433,      separator = (char) separators[level];
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazySimpleSerDe.java,450,      separator = (char) separators[level];
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazySimpleSerDe.java,451,      char keyValueSeparator = (char) separators[level + 1];
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazySimpleSerDe.java,475,      separator = (char) separators[level];
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazySimpleSerDe.java,493,      separator = (char) separators[level];
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,732,      for (int i = 0; i < storageDescriptor.getCols().size(); i++) {
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,733,        FieldSchema col = storageDescriptor.getCols().get(i);
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,317,"    METASTORE_CONNECTION_POOLING_TYPE(""datanucleus.connectionPoolingType"", ""DBCP""),"
ql/src/java/org/apache/hadoop/hive/ql/exec/JoinOperator.java,90,      int sz = storage[alias].size();
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/AbstractRowContainer.java,41,  public abstract int size();
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinObjectValue.java,143,      out.writeInt(v.size());
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinRowContainer.java,68,  public int size() {
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/RowContainer.java,89,  private int size; // total # of elements in the RowContainer
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/RowContainer.java,286,  ArrayList<Object> row = new ArrayList<Object>(2);
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/RowContainer.java,363,  public int size() {
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,97,
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,104,"    Map.Entry<Integer, Translation> existingEntry;"
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,105,    existingEntry = translations.floorEntry(tokenStartIndex);
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,106,    boolean prefix = false;
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,107,    if (existingEntry != null) {
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,108,      if (existingEntry.getKey().equals(tokenStartIndex)) {
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,109,        if (existingEntry.getValue().tokenStopIndex == tokenStopIndex) {
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,110,          if (existingEntry.getValue().replacementText.equals(replacementText)) {
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,111,            // exact match for existing mapping: somebody is doing something
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,112,"            // redundant, but we'll let it pass"
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,113,            return;
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,114,          }
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,115,        } else if (tokenStopIndex > existingEntry.getValue().tokenStopIndex) {
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,116,"          // is existing mapping a prefix for new mapping? if so, that's also"
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,117,"          // redundant, but in this case we need to expand it"
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,118,          prefix = replacementText.startsWith(
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,119,            existingEntry.getValue().replacementText);
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,120,          assert(prefix);
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,121,        } else {
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,122,          // new mapping is a prefix for existing mapping:  ignore it
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,123,          prefix = existingEntry.getValue().replacementText.startsWith(
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,124,            replacementText);
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,125,          assert(prefix);
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,126,          return;
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,127,        }
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,128,      }
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,129,      if (!prefix) {
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,130,        assert (existingEntry.getValue().tokenStopIndex < tokenStartIndex);
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,131,      }
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,132,    }
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,133,    if (!prefix) {
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,134,      existingEntry = translations.ceilingEntry(tokenStartIndex);
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,135,      if (existingEntry != null) {
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,136,        assert (existingEntry.getKey() > tokenStopIndex);
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,137,      }
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,138,    }
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,140,    // Is existing entry a suffix of the newer entry and a subset of it?
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,141,    existingEntry = translations.floorEntry(tokenStopIndex);
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,142,    if (existingEntry != null) {
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,143,      if (existingEntry.getKey().equals(tokenStopIndex)) {
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,144,        if (tokenStartIndex < existingEntry.getKey() &&
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,145,            tokenStopIndex == existingEntry.getKey()) {
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,146,"          // Seems newer entry is a super-set of existing entry, remove existing entry"
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,147,          assert (replacementText.endsWith(existingEntry.getValue().replacementText));
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,148,          translations.remove(tokenStopIndex);
ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java,149,        }
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,327,"    METASTORE_IDENTIFIER_FACTORY(""datanucleus.identifierFactory"", ""datanucleus""),"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,356,"        ""org.datanucleus.jdo.JDOPersistenceManagerFactory""),"
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,129,import org.apache.hadoop.hive.ql.plan.TableDesc;
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,552,"    e.setPersistenceDelegate(org.datanucleus.sco.backed.Map.class, new MapDelegate());"
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,553,"    e.setPersistenceDelegate(org.datanucleus.sco.backed.List.class, new ListDelegate());"
ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/TextMetaDataFormatter.java,124,            outStream.writeBytes(
ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/TextMetaDataFormatter.java,125,              isPretty ?
ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/TextMetaDataFormatter.java,126,                  MetaDataPrettyFormatUtils.getAllColumnsInformation(
ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/TextMetaDataFormatter.java,127,"                      cols, partCols, prettyOutputNumCols)"
ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/TextMetaDataFormatter.java,129,"                  MetaDataFormatUtils.getAllColumnsInformation(cols, partCols, isFormatted)"
ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/TextMetaDataFormatter.java,130,              );
ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/TextMetaDataFormatter.java,132,            outStream.writeBytes(
ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/TextMetaDataFormatter.java,133,"                MetaDataFormatUtils.getAllColumnsInformation(cols, isFormatted));"
ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/TextMetaDataFormatter.java,139,                outStream.writeBytes(MetaDataFormatUtils.getPartitionInformation(part));
ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/TextMetaDataFormatter.java,141,                outStream.writeBytes(MetaDataFormatUtils.getTableInformation(tbl));
ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/TextMetaDataFormatter.java,153,                outStream.writeBytes(part.getTPartition().toString());
ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/TextMetaDataFormatter.java,161,                outStream.writeBytes(tbl.getTTable().toString());
ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/TextMetaDataFormatter.java,447,              outStream.writeBytes(comment);
serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/TestObjectInspectorConverters.java,22,
cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java,614,    int ret = run(args);
cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java,618,  public static int run(String[] args) throws Exception {
cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java,693,"  private static int executeDriver(CliSessionState ss, HiveConf conf, OptionsProcessor oproc)"
cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java,737,        int fileProcessStatus = cli.processFile(ss.fileName);
cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java,738,        return fileProcessStatus;
cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java,745,    ConsoleReader reader = new ConsoleReader();
cli/src/java/org/apache/hadoop/hive/cli/RCFileCat.java,260,    FileOutputStream fdout =
cli/src/java/org/apache/hadoop/hive/cli/RCFileCat.java,261,        new FileOutputStream(FileDescriptor.out);
cli/src/java/org/apache/hadoop/hive/cli/RCFileCat.java,263,"        new BufferedOutputStream(fdout, STDOUT_BUFFER_SIZE);"
cli/src/java/org/apache/hadoop/hive/cli/RCFileCat.java,273,    System.exit(1);
cli/src/test/org/apache/hadoop/hive/cli/TestCliDriverMethods.java,20,import junit.framework.TestCase;
cli/src/test/org/apache/hadoop/hive/cli/TestCliDriverMethods.java,21,import org.apache.hadoop.conf.Configuration;
cli/src/test/org/apache/hadoop/hive/cli/TestCliDriverMethods.java,22,import org.apache.hadoop.hive.metastore.api.FieldSchema;
cli/src/test/org/apache/hadoop/hive/cli/TestCliDriverMethods.java,23,import org.apache.hadoop.hive.metastore.api.Schema;
cli/src/test/org/apache/hadoop/hive/cli/TestCliDriverMethods.java,24,import org.apache.hadoop.hive.ql.CommandNeedRetryException;
cli/src/test/org/apache/hadoop/hive/cli/TestCliDriverMethods.java,25,import org.apache.hadoop.hive.ql.Driver;
cli/src/test/org/apache/hadoop/hive/cli/TestCliDriverMethods.java,26,import org.apache.hadoop.hive.ql.processors.CommandProcessorResponse;
cli/src/test/org/apache/hadoop/hive/cli/TestCliDriverMethods.java,27,
cli/src/test/org/apache/hadoop/hive/cli/TestCliDriverMethods.java,28,import java.io.PrintStream;
cli/src/test/org/apache/hadoop/hive/cli/TestCliDriverMethods.java,29,import java.util.ArrayList;
cli/src/test/org/apache/hadoop/hive/cli/TestCliDriverMethods.java,30,import java.util.List;
cli/src/test/org/apache/hadoop/hive/cli/TestCliDriverMethods.java,32,import static org.apache.hadoop.hive.conf.HiveConf.ConfVars;
cli/src/test/org/apache/hadoop/hive/cli/TestCliDriverMethods.java,61,  public void testThatCliDriverPrintsNoHeaderForCommandsWithNoSchema() throws CommandNeedRetryException {
cli/src/test/org/apache/hadoop/hive/cli/TestCliDriverMethods.java,72,   * @param mockSchema Schema to throw against test
cli/src/test/org/apache/hadoop/hive/cli/TestCliDriverMethods.java,74,   * @throws CommandNeedRetryException won't actually be thrown
cli/src/test/org/apache/hadoop/hive/cli/TestCliDriverMethods.java,81,"    when(conf.getBoolean(eq(ConfVars.HIVE_CLI_PRINT_HEADER.varname), anyBoolean())).thenReturn(true);"
jdbc/src/test/org/apache/hive/jdbc/TestJdbcDriver2.java,701,"          assertEquals(""Expected a tabletype view but got something else."", ""VIRTUAL_VIEW"", tableType);"
jdbc/src/test/org/apache/hive/jdbc/TestJdbcDriver2.java,711,"            , new String[]{""VIRTUAL_VIEW""});"
jdbc/src/test/org/apache/hive/jdbc/TestJdbcDriver2.java,745,    ResultSet rs = (ResultSet)con.getMetaData().getTableTypes();
jdbc/src/test/org/apache/hive/jdbc/TestJdbcDriver2.java,747,    Set<String> tabletypes = new HashSet();
jdbc/src/test/org/apache/hive/jdbc/TestJdbcDriver2.java,748,"    tabletypes.add(""MANAGED_TABLE"");"
jdbc/src/test/org/apache/hive/jdbc/TestJdbcDriver2.java,749,"    tabletypes.add(""EXTERNAL_TABLE"");"
jdbc/src/test/org/apache/hive/jdbc/TestJdbcDriver2.java,750,"    tabletypes.add(""VIRTUAL_VIEW"");"
jdbc/src/test/org/apache/hive/jdbc/TestJdbcDriver2.java,751,"    tabletypes.add(""INDEX_TABLE"");"
service/src/java/org/apache/hive/service/cli/operation/GetTableTypesOperation.java,54,"        rowSet.addRow(RESULT_SET_SCHEMA, new String[] {type.toString()});"
service/src/java/org/apache/hive/service/cli/operation/GetTablesOperation.java,83,"              table.getTableType(),"
service/src/java/org/apache/hive/service/cli/operation/GetTablesOperation.java,86,          if (tableTypes.isEmpty() || tableTypes.contains(table.getTableType())) {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,508,"      errorMessage += "" "" + e.getMessage();"
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,276,        String jtConf = ShimLoader.getHadoopShims().getJobLauncherRpcAddress(conf);
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,278,"        if (jtConf.equals(""local"")) {"
ql/src/java/org/apache/hadoop/hive/ql/exec/SecureCmdDoAs.java,41,  public String addArg(String cmdline) throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/exec/SecureCmdDoAs.java,42,    StringBuilder sb = new StringBuilder();
ql/src/java/org/apache/hadoop/hive/ql/exec/SecureCmdDoAs.java,43,    sb.append(cmdline);
ql/src/java/org/apache/hadoop/hive/ql/exec/SecureCmdDoAs.java,44,"    sb.append("" -hadooptoken "");"
ql/src/java/org/apache/hadoop/hive/ql/exec/SecureCmdDoAs.java,45,    sb.append(tokenPath.toUri().getPath());
ql/src/java/org/apache/hadoop/hive/ql/exec/SecureCmdDoAs.java,46,    return sb.toString();
ql/src/java/org/apache/hadoop/hive/ql/exec/SecureCmdDoAs.java,47,  }
ql/src/java/org/apache/hadoop/hive/ql/exec/SecureCmdDoAs.java,48,
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,618,    String hadoopAuthToken = null;
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,631,"        } else if (args[i].equals(""-hadooptoken"")) {"
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,632,          //set with HS2 in secure mode with doAs
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,633,          hadoopAuthToken = args[++i];
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapredLocalTask.java,239,        cmdLine = secureDoAs.addArg(cmdLine);
ql/src/java/org/apache/hadoop/hive/ql/Context.java,209,            FsPermission fsPermission = new FsPermission(scratchDirPermission);
ql/src/java/org/apache/hadoop/hive/ql/plan/CreateTableDesc.java,34,import org.apache.hadoop.hive.ql.exec.FunctionRegistry;
ql/src/java/org/apache/hadoop/hive/ql/plan/CreateTableDesc.java,56,  ArrayList<FieldSchema> cols;
ql/src/java/org/apache/hadoop/hive/ql/plan/CreateTableDesc.java,57,  ArrayList<FieldSchema> partCols;
ql/src/java/org/apache/hadoop/hive/ql/plan/CreateTableDesc.java,58,  ArrayList<String> bucketCols;
ql/src/java/org/apache/hadoop/hive/ql/plan/CreateTableDesc.java,59,  ArrayList<Order> sortCols;
ql/src/java/org/apache/hadoop/hive/ql/plan/CreateTableDesc.java,133,    this.skewedColNames = new ArrayList<String>(skewedColNames);
ql/src/java/org/apache/hadoop/hive/ql/plan/CreateTableDesc.java,134,    this.skewedColValues = new ArrayList<List<String>>(skewedColValues);
ql/src/java/org/apache/hadoop/hive/ql/plan/CreateTableDesc.java,169,  public ArrayList<FieldSchema> getCols() {
ql/src/java/org/apache/hadoop/hive/ql/plan/CreateTableDesc.java,177,  public ArrayList<FieldSchema> getPartCols() {
ql/src/java/org/apache/hadoop/hive/ql/plan/CreateTableDesc.java,186,  public ArrayList<String> getBucketCols() {
ql/src/java/org/apache/hadoop/hive/ql/plan/CreateTableDesc.java,306,  public ArrayList<Order> getSortCols() {
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,94,      driver = new Driver(getParentSession().getHiveConf());
shims/src/common/java/org/apache/hadoop/hive/thrift/TUGIContainingTransport.java,85,"      transMap.putIfAbsent(trans, new TUGIContainingTransport(trans));"
shims/src/common/java/org/apache/hadoop/hive/thrift/TUGIContainingTransport.java,86,      return transMap.get(trans);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFSplit.java,75,    for (String str : s.toString().split(regex.toString())) {
ql/src/java/org/apache/hadoop/hive/ql/parse/ParseDriver.java,204,    return (ASTNode) r.getTree();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCorrelation.java,22,import org.apache.commons.logging.Log;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCorrelation.java,23,import org.apache.commons.logging.LogFactory;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCorrelation.java,77,  static final Log LOG = LogFactory.getLog(GenericUDAFCorrelation.class.getName());
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCorrelation.java,78,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCorrelation.java,292,        double xavgOld = myagg.xavg;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCorrelation.java,293,        double yavgOld = myagg.yavg;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCorrelation.java,295,        myagg.xavg += (vx - xavgOld) / myagg.count;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCorrelation.java,296,        myagg.yavg += (vy - yavgOld) / myagg.count;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCorrelation.java,298,            myagg.covar += (vx - xavgOld) * (vy - myagg.yavg);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCorrelation.java,299,            myagg.xvar += (vx - xavgOld) * (vx - myagg.xavg);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCorrelation.java,300,            myagg.yvar += (vy - yavgOld) * (vy - myagg.yavg);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCorrelation.java,355,          myagg.xvar += xvarB + (xavgA - xavgB) * (xavgA - xavgB) * myagg.count;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCorrelation.java,356,          myagg.yvar += yvarB + (yavgA - yavgB) * (yavgA - yavgB) * myagg.count;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/GenMRSkewJoinProcessor.java,341,    cndTsk
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/GenMRSkewJoinProcessor.java,342,        .setResolverCtx(new ConditionalResolverSkewJoin.ConditionalResolverSkewJoinCtx(
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/GenMRSkewJoinProcessor.java,343,        bigKeysDirToTaskMap));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/GenMRSkewJoinProcessor.java,344,    List<Task<? extends Serializable>> oldChildTasks = currTask.getChildTasks();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/GenMRSkewJoinProcessor.java,348,    if (oldChildTasks != null) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/GenMRSkewJoinProcessor.java,349,      for (Task<? extends Serializable> tsk : cndTsk.getListTasks()) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/GenMRSkewJoinProcessor.java,350,        for (Task<? extends Serializable> oldChild : oldChildTasks) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/GenMRSkewJoinProcessor.java,351,          tsk.addDependentTask(oldChild);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/GenMRSkewJoinProcessor.java,352,        }
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/GenMRSkewJoinProcessor.java,353,      }
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/GenMRSkewJoinProcessor.java,354,    }
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverSkewJoin.java,53,"    HashMap<String, Task<? extends Serializable>> dirToTaskMap;"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverSkewJoin.java,62,"        HashMap<String, Task<? extends Serializable>> dirToTaskMap) {"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java,325,      if (retryDelaySeconds > 0) {
itests/hive-unit/src/test/java/org/apache/hive/beeline/TestBeeLineWithArgs.java,35,import org.apache.hive.beeline.BeeLine;
itests/hive-unit/src/test/java/org/apache/hive/beeline/TestBeeLineWithArgs.java,42,import static org.junit.Assert.*;
itests/hive-unit/src/test/java/org/apache/hive/beeline/TestBeeLineWithArgs.java,43,
ql/src/java/org/apache/hadoop/hive/ql/processors/SetProcessor.java,184,  private CommandProcessorResponse getVariable(String varname){
ql/src/java/org/apache/hadoop/hive/ql/processors/SetProcessor.java,188,      return new CommandProcessorResponse(0);
ql/src/java/org/apache/hadoop/hive/ql/processors/SetProcessor.java,195,        return new CommandProcessorResponse(0);
ql/src/java/org/apache/hadoop/hive/ql/processors/SetProcessor.java,204,        return new CommandProcessorResponse(0);
ql/src/java/org/apache/hadoop/hive/ql/processors/SetProcessor.java,213,        return new CommandProcessorResponse(0);
ql/src/java/org/apache/hadoop/hive/ql/processors/SetProcessor.java,222,        return new CommandProcessorResponse(0);
ql/src/java/org/apache/hadoop/hive/ql/processors/SetProcessor.java,229,"      return new CommandProcessorResponse(0, null, null, getSchema());"
ql/src/java/org/apache/hadoop/hive/ql/processors/SetProcessor.java,235,    Schema sch = getSchema();
ql/src/java/org/apache/hadoop/hive/ql/processors/SetProcessor.java,240,"      return new CommandProcessorResponse(0, null, null, sch);"
ql/src/java/org/apache/hadoop/hive/ql/processors/SetProcessor.java,245,"      return new CommandProcessorResponse(0, null, null, sch);"
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,98,import org.apache.hadoop.hive.common.ObjectPair;
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,136,import org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner;
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,137,import org.apache.hadoop.hive.ql.parse.PrunedPartitionList;
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,141,import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,187,import org.apache.commons.codec.binary.Base64;
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,188,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,444,"      // If sizes of atleast n-1 tables in a n-way join is known, and their sum is smaller than"
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,454,        boolean bigTableFound = false;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,455,        long largestBigTableCandidateSize = -1;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,456,        long sumTableSizes = 0;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,457,        for (String alias : aliasToWork.keySet()) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,459,          boolean bigTableCandidate = bigTableCandidates.contains(tablePosition);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,460,          Long size = aliasToSize.get(alias);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,461,          // The size is not available at compile time if the input is a sub-query.
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,462,"          // If the size of atleast n-1 inputs for a n-way join are available at compile time,"
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,463,"          // and the sum of them is less than the specified threshold, then convert the join"
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,464,          // into a map-join without the conditional task.
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,465,          if ((size == null) || (size > mapJoinSize)) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,466,            sumTableSizes += largestBigTableCandidateSize;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,467,            if (bigTableFound || (sumTableSizes > mapJoinSize) || !bigTableCandidate) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,468,              convertJoinMapJoin = false;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,469,              break;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,470,            }
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,471,            bigTableFound = true;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,473,            largestBigTableCandidateSize = mapJoinSize + 1;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,474,          } else {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,475,            if (bigTableCandidate && size > largestBigTableCandidateSize) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,476,              bigTablePosition = tablePosition;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,477,              sumTableSizes += largestBigTableCandidateSize;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,478,              largestBigTableCandidateSize = size;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,479,            } else {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,480,              sumTableSizes += size;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,481,            }
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,482,            if (sumTableSizes > mapJoinSize) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,483,              convertJoinMapJoin = false;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,484,              break;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,485,            }
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,490,      String bigTableAlias = null;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,494,      if (convertJoinMapJoin) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java,524,        bigTableAlias = newTaskAlias.getSecond();
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,22,import java.util.Collections;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,74,"      return aliasToKnownSize == null ? new HashMap<String, Long>() : aliasToKnownSize;"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,111,    ConditionalResolverCommonJoinCtx ctx = (ConditionalResolverCommonJoinCtx) objCtx;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,115,"    HashMap<String, ArrayList<String>> pathToAliases = ctx.getPathToAliases();"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,116,"    HashMap<String, Long> aliasToKnownSize = ctx.getAliasToKnownSize();"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,117,"    String bigTableAlias = this.resolveMapJoinTask(pathToAliases, ctx"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,118,"        .getAliasToTask(), aliasToKnownSize, ctx.getHdfsTmpDir(), ctx"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,119,"        .getLocalTmpDir(), conf);"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,138,  static class AliasFileSizePair implements Comparable<AliasFileSizePair> {
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,139,    String alias;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,140,    long size;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,141,"    AliasFileSizePair(String alias, long size) {"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,142,      super();
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,143,      this.alias = alias;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,144,      this.size = size;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,145,    }
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,146,    @Override
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,147,    public int compareTo(AliasFileSizePair o) {
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,148,      if (o == null) {
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,149,        return 1;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,150,      }
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,151,      return (size < o.size) ? -1 : ((size > o.size) ? 1 : 0);
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,155,  private String resolveMapJoinTask(
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,156,"      HashMap<String, ArrayList<String>> pathToAliases,"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,157,"      HashMap<String, Task<? extends Serializable>> aliasToTask,"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,158,"      HashMap<String, Long> aliasToKnownSize, Path hdfsTmpDir,"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,159,"      Path localTmpDir, HiveConf conf) {"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,162,    long smallTablesFileSizeSum = 0;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,164,"    Map<String, AliasFileSizePair> aliasToFileSizeMap = new HashMap<String, AliasFileSizePair>();"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,165,"    for (Map.Entry<String, Long> entry : aliasToKnownSize.entrySet()) {"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,166,      String alias = entry.getKey();
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,167,"      AliasFileSizePair pair = new AliasFileSizePair(alias, entry.getValue());"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,168,"      aliasToFileSizeMap.put(alias, pair);"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,171,    try {
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,172,"      // need to compute the input size at runtime, and select the biggest as"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,173,      // the big table.
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,174,"      for (Map.Entry<String, ArrayList<String>> oneEntry : pathToAliases"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,175,          .entrySet()) {
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,176,        String p = oneEntry.getKey();
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,177,        // this path is intermediate data
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,178,        if (p.startsWith(hdfsTmpDir.toString()) || p.startsWith(localTmpDir.toString())) {
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,179,          ArrayList<String> aliasArray = oneEntry.getValue();
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,180,          if (aliasArray.size() <= 0) {
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,181,            continue;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,182,          }
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,183,          Path path = new Path(p);
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,184,          FileSystem fs = path.getFileSystem(conf);
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,185,          long fileSize = fs.getContentSummary(path).getLength();
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,186,          for (String alias : aliasArray) {
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,187,            AliasFileSizePair pair = aliasToFileSizeMap.get(alias);
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,188,            if (pair == null) {
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,189,"              pair = new AliasFileSizePair(alias, 0);"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,190,"              aliasToFileSizeMap.put(alias, pair);"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,191,            }
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,192,            pair.size += fileSize;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,193,          }
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,196,"      // generate file size to alias mapping; but not set file size as key,"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,197,      // because different file may have the same file size.
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,198,
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,199,      List<AliasFileSizePair> aliasFileSizeList = new ArrayList<AliasFileSizePair>(
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,200,          aliasToFileSizeMap.values());
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,201,
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,202,      Collections.sort(aliasFileSizeList);
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,203,"      // iterating through this list from the end to beginning, trying to find"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,204,      // the big table for mapjoin
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,205,      int idx = aliasFileSizeList.size() - 1;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,206,      boolean bigAliasFound = false;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,207,      while (idx >= 0) {
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,208,        AliasFileSizePair pair = aliasFileSizeList.get(idx);
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,209,        String alias = pair.alias;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,210,        long size = pair.size;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,211,        idx--;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,212,        if (!bigAliasFound && aliasToTask.get(alias) != null) {
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,213,          // got the big table
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,214,          bigAliasFound = true;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,215,          bigTableFileAlias = alias;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,216,          continue;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,218,        smallTablesFileSizeSum += size;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,220,
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,221,      // compare with threshold
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,222,"      long threshold = HiveConf.getLongVar(conf, HiveConf.ConfVars.HIVESMALLTABLESFILESIZE);"
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,223,      if (smallTablesFileSizeSum <= threshold) {
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,224,        return bigTableFileAlias;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,225,      } else {
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,226,        return null;
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,227,      }
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,228,    } catch (Exception e) {
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,229,      e.printStackTrace();
ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java,230,      return null;
ql/src/test/org/apache/hadoop/hive/ql/plan/TestConditionalResolverCommonJoin.java,21,import junit.framework.TestCase;
ql/src/test/org/apache/hadoop/hive/ql/plan/TestConditionalResolverCommonJoin.java,23,import org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin.AliasFileSizePair;
ql/src/test/org/apache/hadoop/hive/ql/plan/TestConditionalResolverCommonJoin.java,25,public class TestConditionalResolverCommonJoin extends TestCase {
ql/src/test/org/apache/hadoop/hive/ql/plan/TestConditionalResolverCommonJoin.java,27,    public void testAliasFileSizePairCompareTo() {
ql/src/test/org/apache/hadoop/hive/ql/plan/TestConditionalResolverCommonJoin.java,28,"        AliasFileSizePair big = new AliasFileSizePair(""big"", 389560034778L);"
ql/src/test/org/apache/hadoop/hive/ql/plan/TestConditionalResolverCommonJoin.java,29,"        AliasFileSizePair small = new AliasFileSizePair(""small"", 1647L);"
ql/src/test/org/apache/hadoop/hive/ql/plan/TestConditionalResolverCommonJoin.java,31,"        assertEquals(0, big.compareTo(big));"
ql/src/test/org/apache/hadoop/hive/ql/plan/TestConditionalResolverCommonJoin.java,32,"        assertEquals(1, big.compareTo(small));"
ql/src/test/org/apache/hadoop/hive/ql/plan/TestConditionalResolverCommonJoin.java,33,"        assertEquals(-1, small.compareTo(big));"
ql/src/test/org/apache/hadoop/hive/ql/plan/TestConditionalResolverCommonJoin.java,34,    }
ql/src/test/org/apache/hadoop/hive/ql/plan/TestConditionalResolverCommonJoin.java,35,}
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,97,"      HiveConf.ConfVars.METASTOREATTEMPTS,"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,98,"      HiveConf.ConfVars.METASTOREINTERVAL,"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,276,    // Number of attempts to retry connecting after there is a JDO datastore err
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,277,"    METASTOREATTEMPTS(""hive.metastore.ds.retry.attempts"", 1),"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,278,    // Number of miliseconds to wait between attepting
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,279,"    METASTOREINTERVAL(""hive.metastore.ds.retry.interval"", 1000),"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,1261,"        ""effect. Make sure to provide a valid value for hive.metastore.uris if you are "" +"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,1262,"        ""connecting to a remote metastore."");"
itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java,2501,"    int interval= HiveConf.getIntVar(hiveConf, HiveConf.ConfVars.METASTOREINTERVAL);"
itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java,2502,"    int attempts = HiveConf.getIntVar(hiveConf, HiveConf.ConfVars.METASTOREATTEMPTS);"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,444,"      return RetryingRawStore.getProxy(hiveConf, conf, rawStoreClassName, threadLocalId.get());"
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,365,    return currentTransaction.isActive();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,380,"      throw new RuntimeException(""commitTransaction was called but openTransactionCalls = """
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,382,"              ""calls to openTransaction/commitTransaction"");"
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,385,      throw new RuntimeException(
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,386,"          ""Commit is called, but transaction is not active. Either there are"""
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,387,"              + "" mismatching open and close calls or rollback was called in the same trasaction"");"
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6154,  /** Add this to code to debug lexer if needed. DebugTokenStream may also be added here. */
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6155,"  private void debugLexer(CommonTokenStream stream, FilterLexer lexer) {"
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6156,    try {
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6157,      stream.fill();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6158,      List<?> tokens = stream.getTokens();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6159,"      String report = ""LEXER: tokens ("" + ((tokens == null) ? ""null"" : tokens.size()) + ""): "";"
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6160,      if (tokens != null) {
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6161,        for (Object o : tokens) {
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6162,          if (o == null || !(o instanceof Token)) {
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6163,"            report += ""[not a token: "" + o + ""], "";"
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6164,          } else {
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6165,            Token t = (Token)o;
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6166,"            report += ""[at "" + t.getCharPositionInLine() + "": """
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6167,"                + t.getType() + "" "" + t.getText() + ""], "";"
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6168,          }
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6169,        }
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6170,      }
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6171,"      report += ""; lexer error: "" + lexer.errorMsg;"
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6172,      LOG.error(report);
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6173,    } catch (Throwable t) {
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6174,"      LOG.error(""LEXER: tokens (error)"", t);"
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,40,public class RetryingRawStore implements InvocationHandler {
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,41,
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,42,  private static final Log LOG = LogFactory.getLog(RetryingRawStore.class);
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,45,  private int retryInterval = 0;
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,46,  private int retryLimit = 0;
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,53,"  protected RetryingRawStore(HiveConf hiveConf, Configuration conf,"
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,71,"    RetryingRawStore handler = new RetryingRawStore(hiveConf, conf, baseClass, id);"
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,74,"    return (RawStore) Proxy.newProxyInstance(RetryingRawStore.class.getClassLoader(),"
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,89,"    retryInterval = HiveConf.getIntVar(hiveConf,"
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,90,        HiveConf.ConfVars.METASTOREINTERVAL);
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,91,"    retryLimit = HiveConf.getIntVar(hiveConf,"
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,92,        HiveConf.ConfVars.METASTOREATTEMPTS);
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,107,    boolean gotNewConnectUrl = false;
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,115,    int retryCount = 0;
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,116,    Exception caughtException = null;
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,117,    while (true) {
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,118,      try {
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,119,        if (reloadConf || gotNewConnectUrl) {
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,120,          initMS();
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,121,        }
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,122,"        ret = method.invoke(base, args);"
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,123,        break;
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,124,      } catch (javax.jdo.JDOException e) {
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,125,        caughtException = e;
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,126,      } catch (UndeclaredThrowableException e) {
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,127,        throw e.getCause();
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,128,      } catch (InvocationTargetException e) {
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,129,        if (e.getCause() instanceof javax.jdo.JDOException) {
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,130,"          // Due to reflection, the jdo exception is wrapped in"
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,131,          // invocationTargetException
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,132,          caughtException = (javax.jdo.JDOException) e.getCause();
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,133,        } else {
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,134,          throw e.getCause();
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,135,        }
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,136,      }
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,137,
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,138,      if (retryCount >= retryLimit ||
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,139,          (method.getAnnotation(RawStore.CanNotRetry.class) != null)) {
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,140,        throw  caughtException;
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,141,      }
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,142,
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,143,      assert (retryInterval >= 0);
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,144,      retryCount++;
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,145,      LOG.error(
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,146,          String.format(
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,147,"              ""JDO datastore error. Retrying metastore command "" +"
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,148,"                  ""after %d ms (attempt %d of %d)"", retryInterval, retryCount, retryLimit));"
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,149,      Thread.sleep(retryInterval);
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,150,"      // If we have a connection error, the JDO connection URL hook might"
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,151,      // provide us with a new URL to access the datastore.
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,152,      String lastUrl = MetaStoreInit.getConnectionURL(getConf());
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,153,"      gotNewConnectUrl = MetaStoreInit.updateConnectionURL(hiveConf, getConf(),"
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,154,"        lastUrl, metaStoreInitData);"
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,159,  private String addPrefix(String s) {
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,160,"    return id + "": "" + s;"
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,161,  }
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingRawStore.java,162,
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingHMSHandler.java,48,"  protected RetryingHMSHandler(HiveConf hiveConf, String name) throws MetaException {"
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingHMSHandler.java,54,"    this.base = (IHMSHandler) new HiveMetaStore.HMSHandler(name, hiveConf);"
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingHMSHandler.java,79,"  public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {"
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingHMSHandler.java,147,        LOG.error(ExceptionUtils.getStackTrace(caughtException));
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingHMSHandler.java,156,          String.format(
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingHMSHandler.java,157,"              ""JDO datastore error. Retrying HMSHandler "" +"
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingHMSHandler.java,158,"                  ""after %d ms (attempt %d of %d)"", retryInterval, retryCount, retryLimit));"
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,94,"  private static String getTopicName(Partition partition,"
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,95,                     ListenerEvent partitionEvent) throws MetaException {
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,96,    try {
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,97,      return partitionEvent.getHandler()
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,98,"        .get_table(partition.getDbName(), partition.getTableName())"
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,99,        .getParameters().get(HCatConstants.HCAT_MSGBUS_TOPIC_NAME);
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,100,    } catch (NoSuchObjectException e) {
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,101,      throw new MetaException(e.toString());
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,102,    }
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,112,
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,113,      Partition partition = partitionEvent.getPartition();
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,114,"      String topicName = getTopicName(partition, partitionEvent);"
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,115,"      if (topicName != null && !topicName.equals("""")) {"
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,116,"        send(messageFactory.buildAddPartitionMessage(partitionEvent.getTable(), partition), topicName);"
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,117,      } else {
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,118,"        LOG.info(""Topic name not found in metastore. Suppressing HCatalog notification for """
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,119,          + partition.getDbName()
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,120,"          + ""."""
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,121,          + partition.getTableName()
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,122,"          + "" To enable notifications for this table, please do alter table set properties ("""
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,123,          + HCatConstants.HCAT_MSGBUS_TOPIC_NAME
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,124,"          + ""=<dbname>.<tablename>) or whatever you want topic name to be."");"
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,126,    }
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,127,
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/listener/NotificationListener.java,151,"      String topicName = getTopicName(partition, partitionEvent);"
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/messaging/MessageFactory.java,126,   * @param table The Table to which the partition is added.
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/messaging/MessageFactory.java,127,   * @param partition The Partition being added.
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/messaging/MessageFactory.java,130,"  public abstract AddPartitionMessage buildAddPartitionMessage(Table table, Partition partition);"
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/messaging/json/JSONMessageFactory.java,87,"  public AddPartitionMessage buildAddPartitionMessage(Table table, Partition partition) {"
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/messaging/json/JSONMessageFactory.java,88,"    return new JSONAddPartitionMessage(HCAT_SERVER_URL, HCAT_SERVICE_PRINCIPAL, partition.getDbName(),"
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/messaging/json/JSONMessageFactory.java,89,"        partition.getTableName(), Arrays.asList(getPartitionKeyValues(table, partition)),"
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/messaging/json/JSONMessageFactory.java,90,        System.currentTimeMillis()/1000);
hcatalog/server-extensions/src/main/java/org/apache/hcatalog/messaging/json/JSONMessageFactory.java,107,}
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,117,"  private static String getTopicName(Partition partition,"
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,118,                     ListenerEvent partitionEvent) throws MetaException {
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,119,    try {
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,120,      return partitionEvent.getHandler()
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,121,"        .get_table(partition.getDbName(), partition.getTableName())"
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,122,        .getParameters().get(HCatConstants.HCAT_MSGBUS_TOPIC_NAME);
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,123,    } catch (NoSuchObjectException e) {
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,124,      throw new MetaException(e.toString());
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,125,    }
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,135,
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,136,      Partition partition = partitionEvent.getPartition();
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,137,"      String topicName = getTopicName(partition, partitionEvent);"
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,139,"        send(messageFactory.buildAddPartitionMessage(partitionEvent.getTable(), partition), topicName);"
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,142,          + partition.getDbName()
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,143,"          + ""."""
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,144,          + partition.getTableName()
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,145,"          + "" To enable notifications for this table, please do alter table set properties ("""
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,146,          + HCatConstants.HCAT_MSGBUS_TOPIC_NAME
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,147,"          + ""=<dbname>.<tablename>) or whatever you want topic name to be."");"
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,150,
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java,174,"      String topicName = getTopicName(partition, partitionEvent);"
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/messaging/MessageFactory.java,123,  /**
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/messaging/MessageFactory.java,124,   * Factory method for AddPartitionMessage.
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/messaging/MessageFactory.java,125,   * @param table The Table to which the partition is added.
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/messaging/MessageFactory.java,126,   * @param partition The Partition being added.
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/messaging/MessageFactory.java,127,   * @return AddPartitionMessage instance.
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/messaging/MessageFactory.java,128,   */
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/messaging/MessageFactory.java,129,"  public abstract AddPartitionMessage buildAddPartitionMessage(Table table, Partition partition);"
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/messaging/json/JSONMessageFactory.java,34,import java.util.Arrays;
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/messaging/json/JSONMessageFactory.java,35,import java.util.LinkedHashMap;
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/messaging/json/JSONMessageFactory.java,36,import java.util.Map;
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/messaging/json/JSONMessageFactory.java,86,"  public AddPartitionMessage buildAddPartitionMessage(Table table, Partition partition) {"
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/messaging/json/JSONMessageFactory.java,87,"    return new JSONAddPartitionMessage(HCAT_SERVER_URL, HCAT_SERVICE_PRINCIPAL, partition.getDbName(),"
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/messaging/json/JSONMessageFactory.java,88,"        partition.getTableName(), Arrays.asList(getPartitionKeyValues(table, partition)),"
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/messaging/json/JSONMessageFactory.java,89,        System.currentTimeMillis()/1000);
itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreEventListener.java,226,"    validateAddPartition(part, partEvent.getPartition());"
itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreEventListener.java,230,"    validateAddPartition(part, prePartEvent.getPartition());"
itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreEventListener.java,263,"    validateAddPartition(newPart, appendPartEvent.getPartition());"
itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreEventListener.java,267,"    validateAddPartition(newPart, preAppendPartEvent.getPartition());"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1637,"        PreAddPartitionEvent event = new PreAddPartitionEvent(part, this);"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1638,        firePreEvent(event);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1639,
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1798,"        Table tbl = ms.getTable(dbName, tblName);"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1842,          for (Partition part : parts) {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1843,"            fireMetaStoreAddPartitionEvent(ms, part, null, success);"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1844,          }
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1846,          for (Partition part : result) {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1847,"            fireMetaStoreAddPartitionEvent(ms, part, null, success);"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1848,          }
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1851,            for (Partition part : existingParts) {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1852,"              fireMetaStoreAddPartitionEvent(ms, part, null, false);"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1853,            }
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1916,"      firePreEvent(new PreAddPartitionEvent(part, this));"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,2010,      Partition retPtn = null;
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,2013,"        Table tbl = ms.getTable(part.getDbName(), part.getTableName());"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,2019,        assert shouldAdd; // start would thrrow if it already existed here
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,2036,"        fireMetaStoreAddPartitionEvent(ms, part, envContext, success);"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,2041,"    private void fireMetaStoreAddPartitionEvent(final RawStore ms,"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,2042,"        final Partition part, final EnvironmentContext envContext, boolean success)"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,2044,"      final Table tbl = ms.getTable(part.getDbName(), part.getTableName());"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,2045,      for (MetaStoreEventListener listener : listeners) {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,2047,"            new AddPartitionEvent(tbl, part, success, this);"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,2049,        listener.onAddPartition(addPartitionEvent);
metastore/src/java/org/apache/hadoop/hive/metastore/events/AddPartitionEvent.java,28,  private final Partition partition;
metastore/src/java/org/apache/hadoop/hive/metastore/events/AddPartitionEvent.java,30,"  public AddPartitionEvent (Table table, Partition partition, boolean status, HMSHandler handler) {"
metastore/src/java/org/apache/hadoop/hive/metastore/events/AddPartitionEvent.java,31,"    super (status, handler);"
metastore/src/java/org/apache/hadoop/hive/metastore/events/AddPartitionEvent.java,33,    this.partition = partition;
metastore/src/java/org/apache/hadoop/hive/metastore/events/AddPartitionEvent.java,36,  /**
metastore/src/java/org/apache/hadoop/hive/metastore/events/AddPartitionEvent.java,37,   * @return the partition
metastore/src/java/org/apache/hadoop/hive/metastore/events/AddPartitionEvent.java,38,   */
metastore/src/java/org/apache/hadoop/hive/metastore/events/AddPartitionEvent.java,39,  public Partition getPartition() {
metastore/src/java/org/apache/hadoop/hive/metastore/events/AddPartitionEvent.java,40,    return partition;
metastore/src/java/org/apache/hadoop/hive/metastore/events/AddPartitionEvent.java,44,   * @return the table
metastore/src/java/org/apache/hadoop/hive/metastore/events/PreAddPartitionEvent.java,26,  private final Partition partition;
metastore/src/java/org/apache/hadoop/hive/metastore/events/PreAddPartitionEvent.java,28,"  public PreAddPartitionEvent (Partition partition, HMSHandler handler) {"
metastore/src/java/org/apache/hadoop/hive/metastore/events/PreAddPartitionEvent.java,30,    this.partition = partition;
metastore/src/java/org/apache/hadoop/hive/metastore/events/PreAddPartitionEvent.java,34,   * @return the partition
metastore/src/java/org/apache/hadoop/hive/metastore/events/PreAddPartitionEvent.java,36,  public Partition getPartition() {
metastore/src/java/org/apache/hadoop/hive/metastore/events/PreAddPartitionEvent.java,37,    return partition;
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationPreEventListener.java,227,      org.apache.hadoop.hive.metastore.api.Partition mapiPart = context.getPartition();
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationPreEventListener.java,228,"      tAuthorizer.get().authorize(getPartitionFromApiPartition(mapiPart, context),"
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationPreEventListener.java,229,"          HiveOperation.ALTERTABLE_ADDPARTS.getInputRequiredPrivileges(),"
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationPreEventListener.java,230,          HiveOperation.ALTERTABLE_ADDPARTS.getOutputRequiredPrivileges());
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/objectinspector/LazyBinaryMapObjectInspector.java,59,    if (data == null) {
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/objectinspector/LazyBinaryMapObjectInspector.java,60,      return -1;
ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java,65,    FileStatus[] srcs = fs.globStatus(path);
ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java,68,        srcs = fs.listStatus(srcs[0].getPath());
serde/src/test/org/apache/hadoop/hive/serde2/lazy/TestLazyArrayMapStruct.java,182,"        assertEquals(""{'2':'d\\tf','2':'d','-1':null,'0':'0','8':'abc'}"""
serde/src/test/org/apache/hadoop/hive/serde2/lazy/TestLazyArrayMapStruct.java,246,"        assertEquals(""{'a':null,'b':['',''],'c':{'':null,'':null},'d':':'}"""
contrib/src/java/org/apache/hadoop/hive/contrib/serde2/s3/S3LogDeserializer.java,185,"      serDe.initialize(conf, tbl);"
contrib/src/test/org/apache/hadoop/hive/contrib/serde2/TestRegexSerDe.java,47,"    serde.initialize(new Configuration(), schema);"
hbase-handler/src/test/org/apache/hadoop/hive/hbase/TestHBaseSerDe.java,118,"    serDe.initialize(conf, tbl);"
hbase-handler/src/test/org/apache/hadoop/hive/hbase/TestHBaseSerDe.java,125,"    serDe.initialize(conf, tbl);"
hbase-handler/src/test/org/apache/hadoop/hive/hbase/TestHBaseSerDe.java,132,"    serDe.initialize(conf, tbl);"
hbase-handler/src/test/org/apache/hadoop/hive/hbase/TestHBaseSerDe.java,139,"    serDe.initialize(conf, tbl);"
hbase-handler/src/test/org/apache/hadoop/hive/hbase/TestHBaseSerDe.java,152,"    serDe.initialize(conf, tbl);"
hbase-handler/src/test/org/apache/hadoop/hive/hbase/TestHBaseSerDe.java,359,"    serDe.initialize(conf, tbl);"
hbase-handler/src/test/org/apache/hadoop/hive/hbase/TestHBaseSerDe.java,366,"    serDe.initialize(conf, tbl);"
hbase-handler/src/test/org/apache/hadoop/hive/hbase/TestHBaseSerDe.java,373,"    serDe.initialize(conf, tbl);"
hbase-handler/src/test/org/apache/hadoop/hive/hbase/TestHBaseSerDe.java,489,"    hbaseSerDe.initialize(conf, tbl);"
hbase-handler/src/test/org/apache/hadoop/hive/hbase/TestHBaseSerDe.java,497,"    hbaseSerDe.initialize(conf, tbl);"
hbase-handler/src/test/org/apache/hadoop/hive/hbase/TestHBaseSerDe.java,618,"    hbaseSerDe.initialize(conf, tbl);"
hbase-handler/src/test/org/apache/hadoop/hive/hbase/TestHBaseSerDe.java,626,"    hbaseSerDe.initialize(conf, tbl);"
hbase-handler/src/test/org/apache/hadoop/hive/hbase/TestHBaseSerDe.java,751,"    serDe.initialize(conf, tbl);"
hbase-handler/src/test/org/apache/hadoop/hive/hbase/TestHBaseSerDe.java,845,"    serDe.initialize(conf, tbl);"
hbase-handler/src/test/org/apache/hadoop/hive/hbase/TestHBaseSerDe.java,895,"    serDe.initialize(conf, tbl);"
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/InternalUtil.java,145,"    serDe.initialize(conf, getSerdeProperties(jobInfo.getTableInfo(), jobInfo.getOutputSchema()));"
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/InternalUtil.java,152,"    deserializer.initialize(conf, props);"
hcatalog/core/src/test/java/org/apache/hive/hcatalog/data/TestHCatRecordSerDe.java,126,"      hrsd.initialize(conf, tblProps);"
hcatalog/core/src/test/java/org/apache/hive/hcatalog/data/TestHCatRecordSerDe.java,147,"      testSD.initialize(conf, tblProps);"
hcatalog/core/src/test/java/org/apache/hive/hcatalog/data/TestJsonSerDe.java,146,"      hrsd.initialize(conf, tblProps);"
hcatalog/core/src/test/java/org/apache/hive/hcatalog/data/TestJsonSerDe.java,149,"      jsde.initialize(conf, tblProps);"
hcatalog/core/src/test/java/org/apache/hive/hcatalog/data/TestJsonSerDe.java,198,"      wjsd.initialize(conf, internalTblProps);"
hcatalog/core/src/test/java/org/apache/hive/hcatalog/data/TestJsonSerDe.java,201,"      rjsd.initialize(conf, tblProps);"
hcatalog/core/src/test/java/org/apache/hive/hcatalog/data/TestJsonSerDe.java,269,"    rjsd.initialize(conf, props);"
hcatalog/core/src/test/java/org/apache/hive/hcatalog/rcfile/TestRCFileMapReduceInputFormat.java,78,"      serDe.initialize(conf, tbl);"
hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/DelimitedInputWriter.java,251,"      serde.initialize(conf, tableProps);"
hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/StrictJsonWriter.java,98,"      serde.initialize(conf, tableProps);"
jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveQueryResultSet.java,111,"      serde.initialize(new Configuration(), props);"
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java,339,"      deserializer.initialize(conf, MetaStoreUtils.getTableMetadata(table));"
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java,375,"      deserializer.initialize(conf, MetaStoreUtils.getPartitionMetadata(part, table));"
ql/src/java/org/apache/hadoop/hive/ql/exec/DefaultFetchFormatter.java,64,"    serde.initialize(conf, serdeProps);"
ql/src/java/org/apache/hadoop/hive/ql/exec/DemuxOperator.java,134,"        inputKeyDeserializer.initialize(null, keyTableDesc.getProperties());"
ql/src/java/org/apache/hadoop/hive/ql/exec/DemuxOperator.java,139,"        inputValueDeserializer.initialize(null, valueTableDesc.getProperties());"
ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java,243,"    serde.initialize(job, table.getProperties());"
ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java,264,"    serde.initialize(job, table.getProperties());"
ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java,430,"      serde.initialize(job, partDesc.getOverlayedProperties());"
ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java,437,"        tblSerde.initialize(job, currPart.getTableDesc().getProperties());"
ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java,451,"        LOG.debug(""deserializer properties: "" + partDesc.getOverlayedProperties());"
ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java,707,"      tblSerde.initialize(job, partition.getTableDesc().getProperties());"
ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java,716,"        partSerde.initialize(job, listPart.getOverlayedProperties());"
ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableDummyOperator.java,37,"      serde.initialize(hconf, tbl.getProperties());"
ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java,180,"      keySerde.initialize(null, keyTableDesc.getProperties());"
ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java,189,"        valueSerDe.initialize(null, valueTableDesc.getProperties());"
ql/src/java/org/apache/hadoop/hive/ql/exec/JoinUtil.java,264,"      sd.initialize(null, desc.getProperties());"
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,120,"    keySerializer.initialize(null, keyTableDesc.getProperties());"
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,134,"      valueSerDe.initialize(null, valueTableDesc.getProperties());"
ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java,187,    // taking precedence
ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java,188,    Properties partProps = isPartitioned(pd) ?
ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java,189,        pd.getOverlayedProperties() : pd.getTableDesc().getProperties();
ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java,193,"    opCtx.tableName = String.valueOf(partProps.getProperty(""name""));"
ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java,198,"    opCtx.deserializer.initialize(hconf, partProps);"
ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java,211,    String pcols = partProps.getProperty(hive_metastoreConstants.META_TABLE_PARTITION_COLUMNS);
ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java,215,      String pcolTypes = partProps.getProperty(hive_metastoreConstants.META_TABLE_PARTITION_COLUMN_TYPES);
ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java,303,"        // If the partition does not exist, use table properties"
ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java,304,        Properties partProps = isPartitioned(pd) ? pd.getOverlayedProperties() : tblProps;
ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java,307,"        partDeserializer.initialize(hconf, partProps);"
ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java,316,"          tblDeserializer.initialize(hconf, tblProps);"
ql/src/java/org/apache/hadoop/hive/ql/exec/ScriptOperator.java,245,"      scriptOutputDeserializer.initialize(hconf, conf.getScriptOutputInfo()"
ql/src/java/org/apache/hadoop/hive/ql/exec/ScriptOperator.java,246,          .getProperties());
ql/src/java/org/apache/hadoop/hive/ql/exec/SkewJoinHandler.java,141,"        serializer.initialize(null, tblDesc.get(alias).getProperties());"
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,2233,                    partDesc.getOverlayedProperties().getProperty(
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,2234,                    hive_metastoreConstants.META_TABLE_STORAGE));
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecReducer.java,137,"      inputKeyDeserializer.initialize(null, keyTableDesc.getProperties());"
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecReducer.java,145,"        inputValueDeserializer[tag].initialize(null, valueTableDesc[tag]"
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecReducer.java,146,            .getProperties());
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ReduceRecordProcessor.java,125,"      inputKeyDeserializer.initialize(null, keyTableDesc.getProperties());"
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ReduceRecordProcessor.java,134,"        inputValueDeserializer[tag].initialize(null, valueTableDesc[tag]"
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ReduceRecordProcessor.java,135,            .getProperties());
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,179,"    partDeserializer.initialize(hiveConf, partProps);"
ql/src/java/org/apache/hadoop/hive/ql/parse/PTFTranslator.java,817,"    serDe.initialize(cfg, p);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5953,"      deserializer.initialize(conf, table_desc.getProperties());"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,6232,"      deserializer.initialize(conf, table_desc.getProperties());"
ql/src/java/org/apache/hadoop/hive/ql/plan/PTFDeserializer.java,263,"      serDe.initialize(hConf, serDeProps);"
ql/src/java/org/apache/hadoop/hive/ql/plan/PartitionDesc.java,132,"    deserializer.initialize(conf, schema);"
ql/src/java/org/apache/hadoop/hive/ql/plan/PartitionDesc.java,171,  public Properties getOverlayedProperties(){
ql/src/java/org/apache/hadoop/hive/ql/plan/PartitionDesc.java,172,    if (tableDesc != null) {
ql/src/java/org/apache/hadoop/hive/ql/plan/PartitionDesc.java,173,      Properties overlayedProps = new Properties(tableDesc.getProperties());
ql/src/java/org/apache/hadoop/hive/ql/plan/PartitionDesc.java,174,      overlayedProps.putAll(getProperties());
ql/src/java/org/apache/hadoop/hive/ql/plan/PartitionDesc.java,175,      return overlayedProps;
ql/src/java/org/apache/hadoop/hive/ql/plan/PartitionDesc.java,176,    } else {
ql/src/java/org/apache/hadoop/hive/ql/plan/PartitionDesc.java,177,      return getProperties();
ql/src/java/org/apache/hadoop/hive/ql/plan/PartitionDesc.java,178,    }
ql/src/java/org/apache/hadoop/hive/ql/plan/PartitionDesc.java,179,  }
ql/src/java/org/apache/hadoop/hive/ql/plan/PartitionDesc.java,180,
ql/src/java/org/apache/hadoop/hive/ql/plan/TableDesc.java,82,"    de.initialize(null, properties);"
ql/src/test/org/apache/hadoop/hive/ql/exec/persistence/TestMapJoinTableContainer.java,57,"    keySerde.initialize(null, keyProps);"
ql/src/test/org/apache/hadoop/hive/ql/exec/persistence/TestMapJoinTableContainer.java,62,"    valueSerde.initialize(null, keyProps);"
ql/src/test/org/apache/hadoop/hive/ql/exec/persistence/TestPTFRowContainer.java,58,"    serDe.initialize(cfg, p);"
ql/src/test/org/apache/hadoop/hive/ql/exec/persistence/Utilities.java,54,"    serde.initialize(null, props);"
ql/src/test/org/apache/hadoop/hive/ql/exec/persistence/Utilities.java,87,"    serde.initialize(null, props);"
ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorizedRowBatchCtx.java,99,"      serDe.initialize(conf, tbl);"
ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorizedRowBatchCtx.java,337,"    vcs.initialize(this.conf, tbl);"
ql/src/test/org/apache/hadoop/hive/ql/io/TestRCFile.java,116,"    serDe.initialize(conf, tbl);"
ql/src/test/org/apache/hadoop/hive/ql/io/TestRCFile.java,430,"    serDe.initialize(conf, tbl);"
ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java,940,"    serde.initialize(conf, properties);"
ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java,1057,"    serde.initialize(conf, properties);"
ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java,1101,"    serde.initialize(conf, properties);"
ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java,1151,"    serde.initialize(conf, properties);"
ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestParquetSerDe.java,45,"      serDe.initialize(conf, tbl);"
serde/src/test/org/apache/hadoop/hive/serde2/TestStatsSerde.java,64,"      serDe.initialize(conf, tbl);"
serde/src/test/org/apache/hadoop/hive/serde2/TestStatsSerde.java,140,"      serDe.initialize(new Configuration(), schema);"
serde/src/test/org/apache/hadoop/hive/serde2/TestStatsSerde.java,186,"      serDe.initialize(conf, tbl);"
serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroSerde.java,79,"    asd.initialize(conf, props);"
serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroSerde.java,136,"    asd.initialize(new Configuration(), props);"
serde/src/test/org/apache/hadoop/hive/serde2/binarysortable/TestBinarySortableSerDe.java,75,"    serde.initialize(new Configuration(), schema);"
serde/src/test/org/apache/hadoop/hive/serde2/columnar/TestLazyBinaryColumnarSerDe.java,77,"    serde.initialize(new Configuration(), props);"
serde/src/test/org/apache/hadoop/hive/serde2/columnar/TestLazyBinaryColumnarSerDe.java,118,"    serde.initialize(new Configuration(), props);"
serde/src/test/org/apache/hadoop/hive/serde2/columnar/TestLazyBinaryColumnarSerDe.java,153,"    serde.initialize(new Configuration(), props);"
serde/src/test/org/apache/hadoop/hive/serde2/columnar/TestLazyBinaryColumnarSerDe.java,186,"    serde.initialize(new Configuration(), props);"
serde/src/test/org/apache/hadoop/hive/serde2/columnar/TestLazyBinaryColumnarSerDe.java,210,"    serde.initialize(new Configuration(), props);"
serde/src/test/org/apache/hadoop/hive/serde2/columnar/TestLazyBinaryColumnarSerDe.java,276,"    serde.initialize(new Configuration(), props);"
serde/src/test/org/apache/hadoop/hive/serde2/columnar/TestLazyBinaryColumnarSerDe.java,294,"    serde.initialize(new Configuration(), props);"
serde/src/test/org/apache/hadoop/hive/serde2/columnar/TestLazyBinaryColumnarSerDe.java,306,"    serde.initialize(new Configuration(), props);"
serde/src/test/org/apache/hadoop/hive/serde2/lazy/TestLazyArrayMapStruct.java,588,"    serDe.initialize(conf, tableProp);"
serde/src/test/org/apache/hadoop/hive/serde2/lazy/TestLazySimpleSerDe.java,60,"      serDe.initialize(conf, tbl);"
serde/src/test/org/apache/hadoop/hive/serde2/lazy/TestLazySimpleSerDe.java,127,"      serDe.initialize(conf, tbl);"
serde/src/test/org/apache/hadoop/hive/serde2/lazy/TestLazySimpleSerDe.java,155,"      serDe.initialize(conf, tbl);"
serde/src/test/org/apache/hadoop/hive/serde2/lazy/TestLazySimpleSerDe.java,183,"      serDe.initialize(conf, tbl);"
serde/src/test/org/apache/hadoop/hive/serde2/lazybinary/TestLazyBinarySerDe.java,100,"    serde.initialize(new Configuration(), schema);"
serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/TestCrossMapEqualComparer.java,103,"    serde.initialize(conf, tbl);"
serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/TestCrossMapEqualComparer.java,157,"    serde.initialize(conf, tbl);"
serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/TestSimpleMapEqualComparer.java,103,"    serde.initialize(conf, tbl);"
serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/TestSimpleMapEqualComparer.java,157,"    serde.initialize(conf, tbl);"
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,391,"      serde.initialize(new HiveConf(), props);"
beeline/src/java/org/apache/hive/beeline/BeeLine.java,943,"    return line.startsWith(""#"") || line.startsWith(""--"");"
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2214,"      ST createTab_stmt = new ST(""CREATE <"" + EXTERNAL + ""> TABLE `"" +"
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2215,"          tableName + ""`(\n"" +"
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2216,"          ""<"" + LIST_COLUMNS + "">)\n"" +"
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2217,"          ""<"" + TBL_COMMENT + "">\n"" +"
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2218,"          ""<"" + LIST_PARTITIONS + "">\n"" +"
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2219,"          ""<"" + SORT_BUCKET + "">\n"" +"
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2220,"          ""<"" + ROW_FORMAT + "">\n"" +"
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2221,"          ""LOCATION\n"" +"
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2222,"          ""<"" + TBL_LOCATION + "">\n"" +"
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2223,"          ""TBLPROPERTIES (\n"" +"
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2224,"          ""<"" + TBL_PROPERTIES + "">)\n"");"
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2392,"      createTab_stmt.add(TBL_LOCATION, tbl_location);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4396,"        OperatorFactory.getAndMakeChild(PlanUtils.getReduceSinkDesc(reduceKeys,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4397,"            keyLength, reduceValues, distinctColIndices,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4398,"            outputKeyColumnNames, outputValueColumnNames, true, -1, keyLength,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4399,"            -1), new RowSchema(reduceSinkOutputRowResolver"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,159,"        if (! equalsFileSystem(srcFs, destFs)) {"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,254,  /**
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,255,   * @param fs1
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,256,   * @param fs2
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,257,   * @return return true if both file system arguments point to same file system
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,258,   */
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,259,"  private boolean equalsFileSystem(FileSystem fs1, FileSystem fs2) {"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,260,"    //When file system cache is disabled, you get different FileSystem objects"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,261,"    // for same file system, so '==' can't be used in such cases"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,262,"    //FileSystem api doesn't have a .equals() function implemented, so using"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,263,    //the uri for comparison. FileSystem already uses uri+Configuration for
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,264,    //equality in its CACHE .
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,265,"    //Once equality has been added in HDFS-4321, we should make use of it"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,266,    return fs1.getUri().equals(fs2.getUri());
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,267,  }
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,268,
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,1245,          if (oldPartPathFS.equals(loadPathFS)) {
itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/authorization/plugin/TestHiveAuthorizerCheckInvocation.java,32,import java.util.Set;
itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/authorization/plugin/TestHiveAuthorizerCheckInvocation.java,113,  private List<String> getSortedList(Set<String> columns) {
itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/CheckColumnAccessHook.java,23,import java.util.Set;
itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/CheckColumnAccessHook.java,37, * ColumnAccessAnalyer. All the hook does is print out the columns
itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/CheckColumnAccessHook.java,61,"    Map<String, Set<String>> tableToColumnAccessMap ="
itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/CheckColumnAccessHook.java,68,"    for (Map.Entry<String, Set<String>> tableAccess : tableToColumnAccessMap.entrySet()) {"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,84,    String oldTblLoc = null;
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,85,    String newTblLoc = null;
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,135,"      // table is not an external table, that means useris asking metastore to"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,145,"        newTblLoc = wh.getTablePath(msdb.getDatabase(newt.getDbName()),"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,146,            newt.getTableName()).toString();
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,147,"        Path newTblPath = constructRenamedPath(new Path(newTblLoc),"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,148,            new Path(newt.getSd().getLocation()));
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,149,        newTblLoc = newTblPath.toString();
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,150,        newt.getSd().setLocation(newTblLoc);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,151,        oldTblLoc = oldt.getSd().getLocation();
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,155,        srcPath = new Path(oldTblLoc);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,156,        srcFs = wh.getFs(srcPath);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,157,        destPath = new Path(newTblLoc);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,158,        destFs = wh.getFs(destPath);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,184,          Path oldPartLocPath = new Path(oldPartLoc);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,185,          String oldTblLocPath = new Path(oldTblLoc).toUri().getPath();
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,186,          String newTblLocPath = new Path(newTblLoc).toUri().getPath();
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,188,            Path newPartLocPath = null;
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,189,            URI oldUri = oldPartLocPath.toUri();
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,190,"            String newPath = oldUri.getPath().replace(oldTblLocPath,"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,191,                                                      newTblLocPath);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,192,
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,193,"            newPartLocPath = new Path(oldUri.getScheme(),"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,194,"                                      oldUri.getAuthority(),"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,195,                                      newPath);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,240,"            LOG.error(""Reverting metadata opeation failed During HDFS operation failed"", e1);"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,3294,"            indexTbl = ms.getTable(index.getDbName(), index.getIndexTableName());"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,3315,"              this.drop_table(index.getDbName(), index.getIndexTableName(), false);"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,3369,          Table tbl = null;
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,3370,"          tbl = this.get_table(dbName, idxTblName);"
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2850,"    MTable indexTable = getMTable(index.getDbName(), index.getIndexTableName());"
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,510,"      Map<String, Set<String>> tab2Cols = colAccessInfo != null ? colAccessInfo"
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,703,"      HashSet<WriteEntity> outputs, String command, Map<String, Set<String>> tab2cols) throws HiveException {"
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,714,    List<HivePrivilegeObject> inputsHObjs = getHivePrivObjects(inputs);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,715,"    updateInputColumnInfo(inputsHObjs, tab2cols);"
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,717,    List<HivePrivilegeObject> outputHObjs = getHivePrivObjects(outputs);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,719,    return;
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,722,  /**
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,723,   * Add column information for input table objects
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,724,   * @param inputsHObjs input HivePrivilegeObject
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,725,   * @param map table to used input columns mapping
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,726,   */
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,727,"  private static void updateInputColumnInfo(List<HivePrivilegeObject> inputsHObjs,"
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,728,"      Map<String, Set<String>> tableName2Cols) {"
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,729,    if(tableName2Cols == null) {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,730,      return;
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,731,    }
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,732,    for(HivePrivilegeObject inputObj : inputsHObjs){
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,733,      if(inputObj.getType() != HivePrivilegeObjectType.TABLE_OR_VIEW){
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,734,        // input columns are relevant only for tables or views
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,735,        continue;
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,736,      }
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,737,"      Set<String> cols = tableName2Cols.get(Table.getCompleteName(inputObj.getDbname(),"
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,738,          inputObj.getObjectName()));
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,739,      inputObj.setColumns(cols);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,740,    }
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,741,  }
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,742,
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,743,  private static List<HivePrivilegeObject> getHivePrivObjects(HashSet<? extends Entity> privObjects) {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,769,        dbname = privObject.getDatabase() == null ? null : privObject.getDatabase().getName();
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,772,        dbname = privObject.getTable() == null ? null : privObject.getTable().getDbName();
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,773,        objName = privObject.getTable() == null ? null : privObject.getTable().getTableName();
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,791,          actionType);
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,38,import java.util.HashSet;
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,603,      Set<String> colSet = showGrantDesc.getColumns() != null ? new HashSet<String>(
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,604,          showGrantDesc.getColumns()) : null;
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,607,"          AuthorizationUtils.getHivePrivilegeObject(showGrantDesc.getHiveObj(),"
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,608,              colSet
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,609,              ));
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,628,"    HivePrivilegeObject hivePrivObject = AuthorizationUtils.getHivePrivilegeObject(privSubjectDesc, null);"
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,757,"    db.dropIndex(SessionState.get().getCurrentDatabase(), dropIdx.getTableName(),"
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,758,"        dropIdx.getIndexName(), true);"
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,768,    String indexTableName =
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,769,      crtIndex.getIndexTableName() != null ? crtIndex.getIndexTableName() :
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,770,"        MetaStoreUtils.getIndexTableName(SessionState.get().getCurrentDatabase(),"
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,771,"             crtIndex.getTableName(), crtIndex.getIndexName());"
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,772,
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,795,    String dbName = alterIndex.getDbName();
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,798,"    Index idx = db.getIndex(dbName, baseTableName, indexName);"
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,809,"        Table baseTbl = db.getTable(SessionState.get().getCurrentDatabase(),"
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,810,            baseTableName);
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,857,"      db.alterIndex(dbName, baseTableName, indexName, idx);"
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,899,"    Table tbl = db.getTable(renamePartitionDesc.getDbName(), renamePartitionDesc.getTableName());"
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,926,"    Table tbl = db.getTable(alterPartitionDesc.getDbName(), alterPartitionDesc.getTableName());"
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1018,    String dbName = touchDesc.getDbName();
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1019,    String tblName = touchDesc.getTableName();
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1020,
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1021,"    Table tbl = db.getTable(dbName, tblName);"
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1025,"        db.alterTable(tblName, tbl);"
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1037,"        db.alterPartition(tblName, part);"
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1176,    String dbName = simpleDesc.getDbName();
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1177,    String tblName = simpleDesc.getTableName();
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1179,"    Table tbl = db.getTable(dbName, tblName);"
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1381,"        db.alterPartition(tblName, p);"
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1402,    String dbName = simpleDesc.getDbName();
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1403,    String tblName = simpleDesc.getTableName();
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1405,"    Table tbl = db.getTable(dbName, tblName);"
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1590,"        db.alterPartition(tblName, p);"
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1639,    String dbName = desc.getDbName();
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1640,    String tblName = desc.getTableName();
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1641,
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1642,"    Table tbl = db.getTable(dbName, tblName);"
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2234,    String dbName = showCols.getDbName();
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2235,    String tableName = showCols.getTableName();
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2236,    Table table = null;
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2237,    if (dbName == null) {
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2238,      table = db.getTable(tableName);
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2239,    }
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2240,    else {
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2241,"      table = db.getTable(dbName, tableName);"
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2242,    }
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,3281,      tbl.setTableName(alterTbl.getNewName());
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,4310,      if (db.getDatabaseCurrent().getName().equalsIgnoreCase(MetaStoreUtils.DEFAULT_DATABASE_NAME)) {
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,4313,"        path = new Path(HiveConf.getVar(conf, HiveConf.ConfVars.METASTOREWAREHOUSE), name.toLowerCase());"
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,1385,    Class<?> codecClass = null;
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,1387,"      codecClass = FileOutputFormat.getOutputCompressorClass(jc, DefaultCodec.class);"
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,2051,  public static String[] getDbTableName(String dbtable) throws HiveException{
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,2052,    if(dbtable == null){
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,2057,    case 2:
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,2058,      return names;
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,2059,    case 1:
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,2060,"      return new String [] {SessionState.get().getCurrentDatabase(), dbtable};"
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,2061,    default:
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,2062,"      throw new HiveException(ErrorMsg.INVALID_TABLE_NAME, dbtable);"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,670,      String dbName = SessionState.get().getCurrentDatabase();
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,673,"        old_index = getIndex(dbName, tableName, indexName);"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,677,"        throw new HiveException(""Index "" + indexName + "" already exists on table "" + tableName + "", db="" + dbName);"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,680,"      org.apache.hadoop.hive.metastore.api.Table baseTbl = getMSC().getTable(dbName, tableName);"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,689,      if (indexTblName == null) {
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,690,"        indexTblName = MetaStoreUtils.getIndexTableName(dbName, tableName, indexName);"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,691,      } else {
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,692,        org.apache.hadoop.hive.metastore.api.Table temp = null;
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,693,        try {
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,694,"          temp = getMSC().getTable(dbName, indexTblName);"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,695,        } catch (Exception e) {
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,696,        }
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,697,        if (temp != null) {
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,698,"          throw new HiveException(""Table name "" + indexTblName + "" already exists. Choose another name."");"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,699,        }
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,777,"        tt = new org.apache.hadoop.hive.ql.metadata.Table(dbName, indexTblName).getTTable();"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,801,"      Index indexDesc = new Index(indexName, indexHandlerClass, dbName, tableName, time, time, indexTblName,"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,821,  public Index getIndex(String qualifiedIndexName) throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,822,    String[] names = getQualifiedNames(qualifiedIndexName);
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,823,    switch (names.length) {
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,824,    case 3:
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,825,"      return getIndex(names[0], names[1], names[2]);"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,826,    case 2:
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,827,"      return getIndex(SessionState.get().getCurrentDatabase(),"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,828,"          names[0], names[1]);"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,829,    default:
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,830,"      throw new HiveException(""Invalid index name:"" + qualifiedIndexName);"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,831,    }
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,832,  }
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,833,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/IndexUtils.java,102,      Table indexTable = hive.getTable(index.getIndexTableName());
ql/src/java/org/apache/hadoop/hive/ql/optimizer/IndexUtils.java,183,    List<Index> indexesOnTable = null;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/index/RewriteGBUsingIndex.java,339,"        Table idxTbl = hiveInstance.getTable(index.getDbName(),"
ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java,820,"  protected HashMap<String, String> extractPartitionSpecs(Tree partspec)"
ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java,822,"    HashMap<String, String> partSpec = new LinkedHashMap<String, String>();"
ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java,1184,    String currentDb = SessionState.get().getCurrentDatabase();
ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java,1185,"    return getTable(currentDb, tblName, throwException);"
ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java,1186,  }
ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java,1187,
ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java,1188,  // qnName : possibly contains database name (dot separated)
ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java,1189,"  protected Table getTableWithQN(String qnName, boolean throwException) throws SemanticException {"
ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java,1190,    int dot = qnName.indexOf('.');
ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java,1191,    if (dot < 0) {
ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java,1192,      String currentDb = SessionState.get().getCurrentDatabase();
ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java,1193,"      return getTable(currentDb, qnName, throwException);"
ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java,1194,    }
ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java,1195,"    return getTable(qnName.substring(0, dot), qnName.substring(dot + 1), throwException);"
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnAccessInfo.java,45,"  public Map<String, Set<String>> getTableToColumnAccessMap() {"
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnAccessInfo.java,46,    return tableToColumnAccessMap;
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,220,      tableName = unescapeIdentifier(tblPart.getChild(0).getText());
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,1018,    String tableName = getUnescapedName((ASTNode) ast.getChild(2));
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,1085,"    CreateIndexDesc crtIndexDesc = new CreateIndexDesc(tableName, indexName,"
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,1119,    String baseTableName = unescapeIdentifier(ast.getChild(0).getText());
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,1126,"    List<Task<?>> indexBuilder = getIndexBuilderMapRed(baseTableName, indexName, partSpec);"
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,1132,    alterIdxDesc.setBaseTableName(baseTableName);
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,1133,    alterIdxDesc.setDbName(SessionState.get().getCurrentDatabase());
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,1145,    String baseTableName = getUnescapedName((ASTNode) ast.getChild(0));
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,1150,    AlterIndexDesc alterIdxDesc =
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,1151,        new AlterIndexDesc(AlterIndexTypes.ADDPROPS);
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,1154,    alterIdxDesc.setBaseTableName(baseTableName);
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,1155,    alterIdxDesc.setDbName(SessionState.get().getCurrentDatabase());
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,1160,"  private List<Task<?>> getIndexBuilderMapRed(String baseTableName, String indexName,"
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,1163,      String dbName = SessionState.get().getCurrentDatabase();
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,1164,"      Index index = db.getIndex(dbName, baseTableName, indexName);"
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,1165,      Table indexTbl = getTable(index.getIndexTableName());
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,1477,"        List<Index> indexes = db.getIndexes(tblObj.getDbName(), tableName,"
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,1636,"    AlterTableSimpleDesc desc = new AlterTableSimpleDesc(SessionState.get().getCurrentDatabase(),"
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2101,    ShowColumnsDesc showColumnsDesc;
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2102,    String dbName = null;
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2103,    String tableName = null;
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2104,    switch (ast.getChildCount()) {
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2105,    case 1:
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2106,      tableName = getUnescapedName((ASTNode) ast.getChild(0));
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2107,      break;
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2108,    case 2:
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2109,      dbName = getUnescapedName((ASTNode) ast.getChild(0));
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2110,      tableName = getUnescapedName((ASTNode) ast.getChild(1));
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2111,      break;
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2112,    default:
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2113,      break;
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2115,
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2116,"    Table tab = getTable(dbName, tableName, true);"
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2119,"    showColumnsDesc = new ShowColumnsDesc(ctx.getResFile(), dbName, tableName);"
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2160,    String tableNames = getUnescapedName((ASTNode) ast.getChild(0));
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2161,    String dbName = SessionState.get().getCurrentDatabase();
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2440,    String tblName = getUnescapedName((ASTNode) ast.getChild(0));
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2441,"    AlterTableDesc alterTblDesc = new AlterTableDesc(tblName,"
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2442,"        getUnescapedName((ASTNode) ast.getChild(1)), expectView);"
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2444,"    addInputsOutputsAlterTable(tblName, null, alterTblDesc);"
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2450,    String tblName = getUnescapedName((ASTNode) ast.getChild(0));
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2480,    Table tab = getTable(tblName);
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2514,"    addTablePartsOutputs(tblName, partSpecs, WriteEntity.WriteType.DDL_EXCLUSIVE);"
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2515,    RenamePartitionDesc renamePartitionDesc = new RenamePartitionDesc(
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2516,"        SessionState.get().getCurrentDatabase(), tblName, oldPartSpec, newPartSpec);"
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2539,    String tblName = getUnescapedName((ASTNode) ast.getChild(0));
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2562,    String tblName = getUnescapedName((ASTNode) ast.getChild(0));
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2563,"    Table tab = getTable(tblName, true);"
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2577,"        new DropTableDesc(tblName, partSpecs, expectView, ignoreProtection);"
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2584,    String tblName = getUnescapedName((ASTNode)ast.getChild(0));
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2586,    Table tab = null;
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2589,    try {
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2590,"      tab = getTable(tblName, true);"
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2591,      inputs.add(new ReadEntity(tab));
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2592,    } catch (HiveException e) {
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2593,      throw new SemanticException(ErrorMsg.INVALID_TABLE.getMsg(tblName));
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2594,    }
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2628,"            new AlterTableAlterPartDesc(SessionState.get().getCurrentDatabase(), tblName, newCol);"
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2651,    String tblName = getUnescapedName((ASTNode)ast.getChild(0));
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2654,"    Table tab = getTable(tblName, true);"
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2665,"    AddPartitionDesc addPartitionDesc = new AddPartitionDesc(tab.getDbName(), tblName, ifNotExists);"
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2686,          // do best effor to determine if this is a local file
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2717,      cmd.append(HiveUtils.unparseIdentifier(tblName));
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2779,    String tblName = getUnescapedName((ASTNode)ast.getChild(0));
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2780,"    Table tab = getTable(tblName, true);"
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2789,"          SessionState.get().getCurrentDatabase(), tblName, null,"
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2795,"      addTablePartsOutputs(tblName, partSpecs, WriteEntity.WriteType.DDL_NO_LOCK);"
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2798,"            SessionState.get().getCurrentDatabase(), tblName, partSpec,"
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2813,    String tblName = getUnescapedName((ASTNode) ast.getChild(0));
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2817,"    Table tab = getTable(tblName, true);"
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2818,"    addTablePartsOutputs(tblName, partSpecs, true, WriteEntity.WriteType.DDL_NO_LOCK);"
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,2838,"        SessionState.get().getCurrentDatabase(), tblName, partSpec,"
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,3006,"  private void addTablePartsOutputs(String tblName, List<Map<String, String>> partSpecs,"
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,3009,"    addTablePartsOutputs(tblName, partSpecs, false, false, null, writeType);"
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,3016,"  private void addTablePartsOutputs(String tblName, List<Map<String, String>> partSpecs,"
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,3019,"    addTablePartsOutputs(tblName, partSpecs, false, allowMany, null, writeType);"
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,3027,"  private void addTablePartsOutputs(String tblName, List<Map<String, String>> partSpecs,"
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,3030,    Table tab = getTable(tblName);
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,3039,"          parts = db.getPartitions(tab, partSpec);"
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,3048,"          Partition p = db.getPartition(tab, partSpec, false);"
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,3128,    String tableName = getUnescapedName((ASTNode) ast.getChild(0));
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,3129,"    Table tab = getTable(tableName, true);"
ql/src/java/org/apache/hadoop/hive/ql/parse/IndexUpdater.java,137,
ql/src/java/org/apache/hadoop/hive/ql/parse/IndexUpdater.java,139,    throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/parse/IndexUpdater.java,140,    Table indexTable = hive.getTable(index.getIndexTableName());
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,10152,"        Table table = getTableWithQN(tableName, false);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,10217,"        Table likeTable = getTableWithQN(likeTableName, false);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,10337,"      Table oldView = getTableWithQN(createVwDesc.getViewName(), false);"
ql/src/java/org/apache/hadoop/hive/ql/parse/authorization/HiveAuthorizationTaskFactoryImpl.java,164,    List<String> cols = null;
ql/src/java/org/apache/hadoop/hive/ql/parse/authorization/HiveAuthorizationTaskFactoryImpl.java,179,        privHiveObj = new PrivilegeObjectDesc();
ql/src/java/org/apache/hadoop/hive/ql/parse/authorization/HiveAuthorizationTaskFactoryImpl.java,180,        //set object name
ql/src/java/org/apache/hadoop/hive/ql/parse/authorization/HiveAuthorizationTaskFactoryImpl.java,181,        String text = param.getChild(0).getText();
ql/src/java/org/apache/hadoop/hive/ql/parse/authorization/HiveAuthorizationTaskFactoryImpl.java,182,        privHiveObj.setObject(BaseSemanticAnalyzer.unescapeIdentifier(text));
ql/src/java/org/apache/hadoop/hive/ql/parse/authorization/HiveAuthorizationTaskFactoryImpl.java,183,        //set object type
ql/src/java/org/apache/hadoop/hive/ql/parse/authorization/HiveAuthorizationTaskFactoryImpl.java,184,        ASTNode objTypeNode = (ASTNode) param.getChild(1);
ql/src/java/org/apache/hadoop/hive/ql/parse/authorization/HiveAuthorizationTaskFactoryImpl.java,185,        privHiveObj.setTable(objTypeNode.getToken().getType() == HiveParser.TOK_TABLE_TYPE);
ql/src/java/org/apache/hadoop/hive/ql/parse/authorization/HiveAuthorizationTaskFactoryImpl.java,186,
ql/src/java/org/apache/hadoop/hive/ql/parse/authorization/HiveAuthorizationTaskFactoryImpl.java,187,        //set col and partition spec if specified
ql/src/java/org/apache/hadoop/hive/ql/parse/authorization/HiveAuthorizationTaskFactoryImpl.java,188,        for (int i = 2; i < param.getChildCount(); i++) {
ql/src/java/org/apache/hadoop/hive/ql/parse/authorization/HiveAuthorizationTaskFactoryImpl.java,189,          ASTNode partOrCol = (ASTNode) param.getChild(i);
ql/src/java/org/apache/hadoop/hive/ql/parse/authorization/HiveAuthorizationTaskFactoryImpl.java,190,          if (partOrCol.getType() == HiveParser.TOK_PARTSPEC) {
ql/src/java/org/apache/hadoop/hive/ql/parse/authorization/HiveAuthorizationTaskFactoryImpl.java,191,            privHiveObj.setPartSpec(DDLSemanticAnalyzer.getPartSpec(partOrCol));
ql/src/java/org/apache/hadoop/hive/ql/parse/authorization/HiveAuthorizationTaskFactoryImpl.java,192,          } else if (partOrCol.getType() == HiveParser.TOK_TABCOLNAME) {
ql/src/java/org/apache/hadoop/hive/ql/parse/authorization/HiveAuthorizationTaskFactoryImpl.java,193,            cols = BaseSemanticAnalyzer.getColumnNames(partOrCol);
ql/src/java/org/apache/hadoop/hive/ql/parse/authorization/HiveAuthorizationTaskFactoryImpl.java,194,          } else {
ql/src/java/org/apache/hadoop/hive/ql/parse/authorization/HiveAuthorizationTaskFactoryImpl.java,195,"            throw new SemanticException(""Invalid token type "" + partOrCol.getType());"
ql/src/java/org/apache/hadoop/hive/ql/parse/authorization/HiveAuthorizationTaskFactoryImpl.java,196,          }
ql/src/java/org/apache/hadoop/hive/ql/parse/authorization/HiveAuthorizationTaskFactoryImpl.java,197,        }
ql/src/java/org/apache/hadoop/hive/ql/parse/authorization/HiveAuthorizationTaskFactoryImpl.java,202,"        principalDesc, privHiveObj, cols);"
ql/src/java/org/apache/hadoop/hive/ql/parse/authorization/HiveAuthorizationTaskFactoryImpl.java,222,      rolesStartPos = 2; //start reading role names from next postion
ql/src/java/org/apache/hadoop/hive/ql/parse/authorization/HiveAuthorizationTaskFactoryImpl.java,245,    PrivilegeObjectDesc subject = new PrivilegeObjectDesc();
ql/src/java/org/apache/hadoop/hive/ql/parse/authorization/HiveAuthorizationTaskFactoryImpl.java,246,    //set object identifier
ql/src/java/org/apache/hadoop/hive/ql/parse/authorization/HiveAuthorizationTaskFactoryImpl.java,247,    subject.setObject(BaseSemanticAnalyzer.unescapeIdentifier(ast.getChild(0).getText()));
ql/src/java/org/apache/hadoop/hive/ql/parse/authorization/HiveAuthorizationTaskFactoryImpl.java,248,    //set object type
ql/src/java/org/apache/hadoop/hive/ql/parse/authorization/HiveAuthorizationTaskFactoryImpl.java,249,    ASTNode objTypeNode =  (ASTNode) ast.getChild(1);
ql/src/java/org/apache/hadoop/hive/ql/parse/authorization/HiveAuthorizationTaskFactoryImpl.java,250,    subject.setTable(objTypeNode.getToken().getType() == HiveParser.TOK_TABLE_TYPE);
ql/src/java/org/apache/hadoop/hive/ql/parse/authorization/HiveAuthorizationTaskFactoryImpl.java,251,    if (ast.getChildCount() == 3) {
ql/src/java/org/apache/hadoop/hive/ql/parse/authorization/HiveAuthorizationTaskFactoryImpl.java,252,"      //if partition spec node is present, set partition spec"
ql/src/java/org/apache/hadoop/hive/ql/parse/authorization/HiveAuthorizationTaskFactoryImpl.java,253,      ASTNode partSpecNode = (ASTNode) ast.getChild(2);
ql/src/java/org/apache/hadoop/hive/ql/parse/authorization/HiveAuthorizationTaskFactoryImpl.java,254,      subject.setPartSpec(DDLSemanticAnalyzer.getPartSpec(partSpecNode));
ql/src/java/org/apache/hadoop/hive/ql/parse/authorization/HiveAuthorizationTaskFactoryImpl.java,255,    }
ql/src/java/org/apache/hadoop/hive/ql/parse/authorization/HiveAuthorizationTaskFactoryImpl.java,258,"      Table tbl = getTable(SessionState.get().getCurrentDatabase(), subject.getObject());"
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterIndexDesc.java,22,import java.util.ArrayList;
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterIndexDesc.java,24,import java.util.List;
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterIndexDesc.java,25,
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterIndexDesc.java,26,import org.apache.hadoop.hive.metastore.api.FieldSchema;
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterIndexDesc.java,27,import org.apache.hadoop.hive.metastore.api.Order;
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterIndexDesc.java,28,import org.apache.hadoop.hive.ql.exec.Utilities;
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterIndexDesc.java,39,  private String dbName;
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterIndexDesc.java,107,  /**
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterIndexDesc.java,108,   * @return the name of the database that the base table is in
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterIndexDesc.java,109,   */
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterIndexDesc.java,110,  public String getDbName() {
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterIndexDesc.java,111,    return dbName;
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterIndexDesc.java,112,  }
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterIndexDesc.java,113,
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterIndexDesc.java,114,  /**
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterIndexDesc.java,115,   * @param dbName
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterIndexDesc.java,116,   *          the dbName to set
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterIndexDesc.java,117,   */
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterIndexDesc.java,118,  public void setDbName(String dbName) {
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterIndexDesc.java,119,    this.dbName = dbName;
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterIndexDesc.java,120,  }
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterIndexDesc.java,121,
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterTableAlterPartDesc.java,23,import java.util.List;
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterTableAlterPartDesc.java,24,
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterTableAlterPartDesc.java,27,  private String dbName;
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterTableAlterPartDesc.java,34,   * @param dbName
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterTableAlterPartDesc.java,35,   *          database that contains the table / partition
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterTableAlterPartDesc.java,39,   *          key column specification.
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterTableAlterPartDesc.java,41,"  public AlterTableAlterPartDesc(String dbName, String tableName, FieldSchema partKeySpec) {"
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterTableAlterPartDesc.java,43,    this.dbName = dbName;
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterTableAlterPartDesc.java,56,  public String getDbName() {
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterTableAlterPartDesc.java,57,    return dbName;
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterTableAlterPartDesc.java,58,  }
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterTableAlterPartDesc.java,59,
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterTableAlterPartDesc.java,60,  public void setDbName(String dbName) {
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterTableAlterPartDesc.java,61,    this.dbName = dbName;
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterTableAlterPartDesc.java,62,  }
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterTableAlterPartDesc.java,63,
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterTableSimpleDesc.java,32,  private String dbName;
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterTableSimpleDesc.java,42,   * @param dbName
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterTableSimpleDesc.java,43,   *          database that contains the table / partition
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterTableSimpleDesc.java,47,   *          partition specification. Null if touching a table.
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterTableSimpleDesc.java,49,"  public AlterTableSimpleDesc(String dbName, String tableName,"
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterTableSimpleDesc.java,50,"      Map<String, String> partSpec, AlterTableDesc.AlterTableTypes type) {"
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterTableSimpleDesc.java,51,    super();
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterTableSimpleDesc.java,52,    this.dbName = dbName;
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterTableSimpleDesc.java,64,   * @param dbname name of the database containing the table
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterTableSimpleDesc.java,69,"  public AlterTableSimpleDesc(String dbname, String tableName,"
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterTableSimpleDesc.java,70,"                              LinkedHashMap<String,  String> partSpec,  String compactionType) {"
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterTableSimpleDesc.java,73,    this.dbName = dbname;
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterTableSimpleDesc.java,86,  public String getDbName() {
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterTableSimpleDesc.java,87,    return dbName;
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterTableSimpleDesc.java,88,  }
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterTableSimpleDesc.java,89,
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterTableSimpleDesc.java,90,  public void setDbName(String dbName) {
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterTableSimpleDesc.java,91,    this.dbName = dbName;
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterTableSimpleDesc.java,92,  }
ql/src/java/org/apache/hadoop/hive/ql/plan/AlterTableSimpleDesc.java,93,
ql/src/java/org/apache/hadoop/hive/ql/plan/RenamePartitionDesc.java,32,  String dbName;
ql/src/java/org/apache/hadoop/hive/ql/plan/RenamePartitionDesc.java,53,"  public RenamePartitionDesc(String dbName, String tableName,"
ql/src/java/org/apache/hadoop/hive/ql/plan/RenamePartitionDesc.java,55,    super();
ql/src/java/org/apache/hadoop/hive/ql/plan/RenamePartitionDesc.java,56,    this.dbName = dbName;
ql/src/java/org/apache/hadoop/hive/ql/plan/RenamePartitionDesc.java,62,  /**
ql/src/java/org/apache/hadoop/hive/ql/plan/RenamePartitionDesc.java,63,   * @return database name
ql/src/java/org/apache/hadoop/hive/ql/plan/RenamePartitionDesc.java,64,   */
ql/src/java/org/apache/hadoop/hive/ql/plan/RenamePartitionDesc.java,65,  public String getDbName() {
ql/src/java/org/apache/hadoop/hive/ql/plan/RenamePartitionDesc.java,66,    return dbName;
ql/src/java/org/apache/hadoop/hive/ql/plan/RenamePartitionDesc.java,67,  }
ql/src/java/org/apache/hadoop/hive/ql/plan/RenamePartitionDesc.java,68,
ql/src/java/org/apache/hadoop/hive/ql/plan/RenamePartitionDesc.java,69,  /**
ql/src/java/org/apache/hadoop/hive/ql/plan/RenamePartitionDesc.java,70,   * @param dbName
ql/src/java/org/apache/hadoop/hive/ql/plan/RenamePartitionDesc.java,71,   *          database name
ql/src/java/org/apache/hadoop/hive/ql/plan/RenamePartitionDesc.java,72,   */
ql/src/java/org/apache/hadoop/hive/ql/plan/RenamePartitionDesc.java,73,  public void setDbName(String dbName) {
ql/src/java/org/apache/hadoop/hive/ql/plan/RenamePartitionDesc.java,74,    this.dbName = dbName;
ql/src/java/org/apache/hadoop/hive/ql/plan/RenamePartitionDesc.java,75,  }
ql/src/java/org/apache/hadoop/hive/ql/plan/RenamePartitionDesc.java,76,
ql/src/java/org/apache/hadoop/hive/ql/plan/ShowColumnsDesc.java,26,  String dbName;
ql/src/java/org/apache/hadoop/hive/ql/plan/ShowColumnsDesc.java,65,  /**
ql/src/java/org/apache/hadoop/hive/ql/plan/ShowColumnsDesc.java,66,   * @param dbName    name of the database
ql/src/java/org/apache/hadoop/hive/ql/plan/ShowColumnsDesc.java,67,   * @param tableName name of table to show columns of
ql/src/java/org/apache/hadoop/hive/ql/plan/ShowColumnsDesc.java,68,   */
ql/src/java/org/apache/hadoop/hive/ql/plan/ShowColumnsDesc.java,69,"  public ShowColumnsDesc(Path resFile, String dbName, String tableName) {"
ql/src/java/org/apache/hadoop/hive/ql/plan/ShowColumnsDesc.java,70,    this.resFile = resFile.toString();
ql/src/java/org/apache/hadoop/hive/ql/plan/ShowColumnsDesc.java,71,    this.dbName = dbName;
ql/src/java/org/apache/hadoop/hive/ql/plan/ShowColumnsDesc.java,72,    this.tableName = tableName;
ql/src/java/org/apache/hadoop/hive/ql/plan/ShowColumnsDesc.java,73,  }
ql/src/java/org/apache/hadoop/hive/ql/plan/ShowColumnsDesc.java,74,
ql/src/java/org/apache/hadoop/hive/ql/plan/ShowColumnsDesc.java,106,
ql/src/java/org/apache/hadoop/hive/ql/plan/ShowColumnsDesc.java,107,  public String getDbName() {
ql/src/java/org/apache/hadoop/hive/ql/plan/ShowColumnsDesc.java,108,    return dbName;
ql/src/java/org/apache/hadoop/hive/ql/plan/ShowColumnsDesc.java,109,  }
ql/src/java/org/apache/hadoop/hive/ql/plan/ShowColumnsDesc.java,110,
ql/src/java/org/apache/hadoop/hive/ql/plan/ShowColumnsDesc.java,111,  public void setDbName(String dbName) {
ql/src/java/org/apache/hadoop/hive/ql/plan/ShowColumnsDesc.java,112,    this.dbName = dbName;
ql/src/java/org/apache/hadoop/hive/ql/plan/ShowColumnsDesc.java,113,  }
ql/src/java/org/apache/hadoop/hive/ql/plan/ShowGrantDesc.java,20,import java.util.List;
ql/src/java/org/apache/hadoop/hive/ql/plan/ShowGrantDesc.java,21,
ql/src/java/org/apache/hadoop/hive/ql/plan/ShowGrantDesc.java,29,  private List<String> columns;
ql/src/java/org/apache/hadoop/hive/ql/plan/ShowGrantDesc.java,30,
ql/src/java/org/apache/hadoop/hive/ql/plan/ShowGrantDesc.java,45,"      PrivilegeObjectDesc subjectObj, List<String> columns) {"
ql/src/java/org/apache/hadoop/hive/ql/plan/ShowGrantDesc.java,49,    this.columns = columns;
ql/src/java/org/apache/hadoop/hive/ql/plan/ShowGrantDesc.java,81,
ql/src/java/org/apache/hadoop/hive/ql/plan/ShowGrantDesc.java,82,  public List<String> getColumns() {
ql/src/java/org/apache/hadoop/hive/ql/plan/ShowGrantDesc.java,83,    return columns;
ql/src/java/org/apache/hadoop/hive/ql/plan/ShowGrantDesc.java,84,  }
ql/src/java/org/apache/hadoop/hive/ql/plan/ShowGrantDesc.java,85,
ql/src/java/org/apache/hadoop/hive/ql/plan/ShowGrantDesc.java,86,  public void setColumns(List<String> columns) {
ql/src/java/org/apache/hadoop/hive/ql/plan/ShowGrantDesc.java,87,    this.columns = columns;
ql/src/java/org/apache/hadoop/hive/ql/plan/ShowGrantDesc.java,88,  }
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationUtils.java,22,import java.util.Set;
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationUtils.java,176,  public static HivePrivilegeObject getHivePrivilegeObject(
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationUtils.java,177,"      PrivilegeObjectDesc privSubjectDesc, Set<String> columns) throws HiveException {"
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HivePrivilegeObject.java,22,import java.util.HashSet;
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HivePrivilegeObject.java,25,import java.util.Set;
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HivePrivilegeObject.java,94,  private Set<String> columns;
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HivePrivilegeObject.java,109,"        column == null ? null : new HashSet<String>(Arrays.asList(column)),"
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HivePrivilegeObject.java,124,"    List<String> partKeys, Set<String> columns, List<String> commandParams) {"
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HivePrivilegeObject.java,129,"      List<String> partKeys, Set<String> columns, HivePrivObjectActionType actionType,"
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HivePrivilegeObject.java,173,  public Set<String> getColumns() {
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HivePrivilegeObject.java,221,
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HivePrivilegeObject.java,222,  public void setColumns(Set<String> columnms) {
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HivePrivilegeObject.java,223,    this.columns = columnms;
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HivePrivilegeObject.java,224,  }
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HivePrivilegeObject.java,225,
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HiveV1Authorizer.java,322,          Set<String> columns = privObj.getColumns();
ql/src/test/org/apache/hadoop/hive/ql/metadata/TestHive.java,449,"      hm.createIndex(tableName, indexName, indexHandlerClass, indexedCols, indexTableName,"
ql/src/test/org/apache/hadoop/hive/ql/parse/TestQBCompact.java,82,"    Assert.assertEquals(""foo"", desc.getTableName());"
ql/src/test/org/apache/hadoop/hive/ql/parse/TestQBCompact.java,83,"    Assert.assertEquals(""default"", desc.getDbName());"
ql/src/test/org/apache/hadoop/hive/ql/parse/TestQBCompact.java,103,"    Assert.assertEquals(""foo"", desc.getTableName());"
ql/src/test/org/apache/hadoop/hive/ql/parse/TestQBCompact.java,104,"    Assert.assertEquals(""default"", desc.getDbName());"
ql/src/test/org/apache/hadoop/hive/ql/parse/TestQBCompact.java,115,"    Assert.assertEquals(""foo"", desc.getTableName());"
ql/src/test/org/apache/hadoop/hive/ql/parse/TestQBCompact.java,116,"    Assert.assertEquals(""default"", desc.getDbName());"
ql/src/test/org/apache/hadoop/hive/ql/parse/authorization/PrivilegesTestBase.java,53,"    Assert.assertEquals(TABLE, grantDesc.getPrivilegeSubjectDesc().getObject());"
ql/src/test/org/apache/hadoop/hive/ql/parse/authorization/TestHiveAuthorizationTaskFactory.java,124,"    Assert.assertEquals(TABLE, grantDesc.getPrivilegeSubjectDesc().getObject());"
ql/src/test/org/apache/hadoop/hive/ql/parse/authorization/TestHiveAuthorizationTaskFactory.java,142,"    Assert.assertEquals(TABLE, grantDesc.getPrivilegeSubjectDesc().getObject());"
ql/src/test/org/apache/hadoop/hive/ql/parse/authorization/TestHiveAuthorizationTaskFactory.java,160,"    Assert.assertEquals(TABLE, grantDesc.getPrivilegeSubjectDesc().getObject());"
ql/src/test/org/apache/hadoop/hive/ql/parse/authorization/TestHiveAuthorizationTaskFactory.java,178,"    Assert.assertEquals(TABLE, grantDesc.getPrivilegeSubjectDesc().getObject());"
ql/src/test/org/apache/hadoop/hive/ql/parse/authorization/TestHiveAuthorizationTaskFactory.java,196,"    Assert.assertEquals(TABLE, grantDesc.getPrivilegeSubjectDesc().getObject());"
ql/src/test/org/apache/hadoop/hive/ql/parse/authorization/TestHiveAuthorizationTaskFactory.java,214,"    Assert.assertEquals(TABLE, grantDesc.getPrivilegeSubjectDesc().getObject());"
ql/src/test/org/apache/hadoop/hive/ql/parse/authorization/TestHiveAuthorizationTaskFactory.java,383,"    Assert.assertEquals(TABLE, grantDesc.getHiveObj().getObject());"
ql/src/test/org/apache/hadoop/hive/ql/parse/authorization/TestHiveAuthorizationTaskFactory.java,397,"    Assert.assertEquals(TABLE, grantDesc.getHiveObj().getObject());"
ql/src/test/org/apache/hadoop/hive/ql/parse/authorization/TestHiveAuthorizationTaskFactory.java,411,"    Assert.assertEquals(TABLE, grantDesc.getHiveObj().getObject());"
beeline/src/java/org/apache/hive/beeline/Commands.java,728,"          line += "" "" + extra;"
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java,698,"           dropTable(name, table, deleteData, false);"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,660,      if (tbl.getCols().size() == 0) {
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,689,"    HIVE_OPTIMIZE_MULTI_GROUPBY_COMMON_DISTINCTS(""hive.optimize.multigroupby.common.distincts"", true,"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,690,"        ""Whether to optimize a multi-groupby query with the same distinct.\n"" +"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,691,"        ""Consider a query like:\n"" +"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,692,"        ""\n"" +"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,693,"        ""  from src\n"" +"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,694,"        ""    insert overwrite table dest1 select col1, count(distinct colx) group by col1\n"" +"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,695,"        ""    insert overwrite table dest2 select col2, count(distinct colx) group by col2;\n"" +"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,696,"        ""\n"" +"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,697,"        ""With this parameter set to true, first we spray by the distinct value (colx), and then\n"" +"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,698,"        ""perform the 2 groups bys. This makes sense if map-side aggregation is turned off. However,\n"" +"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,699,"        ""with maps-side aggregation, it might be useful in some cases to treat the 2 inserts independently, \n"" +"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,700,"        ""thereby performing the query above in 2MR jobs instead of 3 (due to spraying by distinct key first).\n"" +"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,701,"        ""If this parameter is turned off, we don't consider the fact that the distinct key is the same across\n"" +"
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,702,"        ""different MR jobs.""),"
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,83,
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,110,  // Used by hash distinct aggregations when hashGrpKeyNotRedKey is true
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,111,  private transient HashSet<KeyWrapper> keysCurrentGroup;
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,112,
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,115,"  // The reduction is happening on the reducer, and the grouping key and"
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,116,  // reduction keys are different.
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,117,"  // For example: select a, count(distinct b) from T group by a"
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,118,  // The data is sprayed by 'b' and the reducer is grouping it by 'a'
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,119,  private transient boolean groupKeyIsNotReduceKey;
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,120,  private transient boolean firstRowInGroup;
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,136,  private transient List<Integer> groupingSets;       // declared grouping set values
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,137,  private transient FastBitSet[] groupingSetsBitSet;  // bitsets acquired from grouping set values
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,365,      groupKeyIsNotReduceKey = conf.getGroupKeyNotReductionKey();
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,366,      if (groupKeyIsNotReduceKey) {
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,367,        keysCurrentGroup = new HashSet<KeyWrapper>();
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,368,      }
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,378,    ObjectInspector[] objectInspectors =
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,699,  @Override
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,700,  public void startGroup() throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,701,    firstRowInGroup = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,702,    super.startGroup();
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,703,  }
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,704,
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,705,  @Override
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,706,  public void endGroup() throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,707,    if (groupKeyIsNotReduceKey) {
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,708,      keysCurrentGroup.clear();
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,709,    }
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,710,  }
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,711,
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,721,    firstRowInGroup = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,722,
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,735,    if (hashAggr && !groupKeyIsNotReduceKey) {
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,811,"    // If the grouping key and the reduction key are different, a set of"
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,812,    // grouping keys for the current reduction key are maintained in
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,813,    // keysCurrentGroup
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,814,    // Peek into the set to find out if a new grouping key is seen for the given
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,815,    // reduction key
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,816,    if (groupKeyIsNotReduceKey) {
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,817,      newEntryForHashAggr = keysCurrentGroup.add(newKeys.copyKey());
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,818,    }
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,819,
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,829,"    // If the grouping key is not the same as reduction key, flushing can only"
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,830,    // happen at boundaries
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,831,    if ((!groupKeyIsNotReduceKey || firstRowInGroup)
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,832,        && shouldBeFlushed(newKeys)) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1089,      if (desc.getGroupKeyNotReductionKey()) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1090,"        LOG.info(""Reduce vector mode not supported when group key is not reduction key"");"
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1091,        return false;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1092,      }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,233,"  private LinkedHashMap<Operator<? extends OperatorDesc>, OpParseContext> opParseCtx;"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4173,   * @param distPartAggr
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4174,   *          partial aggregation for distincts
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4187,"      boolean distPartAgg,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4286,      boolean partialAggDone = !(distPartAgg || isDistinct);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4287,      if (!partialAggDone) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4306,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4319,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4321,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4337,      boolean isAllColumns = value.getType() == HiveParser.TOK_FUNCTIONSTAR;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4340,"      // For distincts, partial aggregations have not been done"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4341,      if (distPartAgg) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4342,"        genericUDAFEvaluator = getGenericUDAFEvaluator(aggName, aggParameters,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4343,"            value, isDistinct, isAllColumns);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4344,        assert (genericUDAFEvaluator != null);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4345,"        genericUDAFEvaluators.put(entry.getKey(), genericUDAFEvaluator);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4346,      } else {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4347,        genericUDAFEvaluator = genericUDAFEvaluators.get(entry.getKey());
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4348,        assert (genericUDAFEvaluator != null);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4349,      }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4373,"            distPartAgg, groupByMemoryUsage, memoryThreshold,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5261,  /**
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5262,   * Generate a Multi Group-By plan using a 2 map-reduce jobs.
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5263,   *
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5264,   * @param dest
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5265,   * @param qb
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5266,   * @param input
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5267,   * @return
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5268,   * @throws SemanticException
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5269,   *
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5270,   *           Generate a Group-By plan using a 2 map-reduce jobs. Spray by the
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5271,"   *           distinct key in hope of getting a uniform distribution, and"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5272,   *           compute partial aggregates by the grouping key. Evaluate partial
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5273,"   *           aggregates first, and spray by the grouping key to compute actual"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5274,   *           aggregates in the second phase. The aggregation evaluation
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5275,   *           functions are as follows: Partitioning Key: distinct key
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5276,   *
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5277,   *           Sorting Key: distinct key
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5278,   *
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5279,   *           Reducer: iterate/terminatePartial (mode = PARTIAL1)
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5280,   *
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5281,   *           STAGE 2
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5282,   *
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5283,   *           Partitioning Key: grouping key
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5284,   *
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5285,   *           Sorting Key: grouping key
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5286,   *
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5287,   *           Reducer: merge/terminate (mode = FINAL)
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5288,   */
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5289,"  @SuppressWarnings(""nls"")"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5290,"  private Operator genGroupByPlan2MRMultiGroupBy(String dest, QB qb,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5291,      Operator input) throws SemanticException {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5292,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5293,    // ////// Generate GroupbyOperator for a map-side partial aggregation
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5294,"    Map<String, GenericUDAFEvaluator> genericUDAFEvaluators ="
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5295,"        new LinkedHashMap<String, GenericUDAFEvaluator>();"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5296,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5297,    QBParseInfo parseInfo = qb.getParseInfo();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5298,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5299,    // ////// 2. Generate GroupbyOperator
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5300,"    Operator groupByOperatorInfo = genGroupByPlanGroupByOperator1(parseInfo,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5301,"        dest, input, GroupByDesc.Mode.HASH, genericUDAFEvaluators, true,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5302,"        null, false, false);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5303,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5304,    int numReducers = -1;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5305,"    List<ASTNode> grpByExprs = getGroupByForClause(parseInfo, dest);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5306,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5307,    // ////// 3. Generate ReduceSinkOperator2
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5308,    Operator reduceSinkOperatorInfo2 = genGroupByPlanReduceSinkOperator2MR(
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5309,"        parseInfo, dest, groupByOperatorInfo, grpByExprs.size(), numReducers, false);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5310,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5311,    // ////// 4. Generate GroupbyOperator2
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5312,"    Operator groupByOperatorInfo2 = genGroupByPlanGroupByOperator2MR(parseInfo,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5313,"        dest, reduceSinkOperatorInfo2, GroupByDesc.Mode.FINAL,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5314,"        genericUDAFEvaluators, false);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5315,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5316,    return groupByOperatorInfo2;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5317,  }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5318,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5643,"          genericUDAFEvaluators, false,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5656,"              genericUDAFEvaluators, false,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5787,"          genericUDAFEvaluators, false,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8422,  // Return the common distinct expression
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8423,"  // There should be more than 1 destination, with group bys in all of them."
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8424,"  private List<ASTNode> getCommonDistinctExprs(QB qb, Operator input) {"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8425,    QBParseInfo qbp = qb.getParseInfo();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8426,"    // If a grouping set aggregation is present, common processing is not possible"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8427,    if (!qbp.getDestCubes().isEmpty() || !qbp.getDestRollups().isEmpty()
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8428,        || !qbp.getDestToLateralView().isEmpty()) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8429,      return null;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8430,    }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8431,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8432,    RowResolver inputRR = opParseCtx.get(input).getRowResolver();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8433,    TreeSet<String> ks = new TreeSet<String>();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8434,    ks.addAll(qbp.getClauseNames());
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8435,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8436,    // Go over all the destination tables
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8437,    if (ks.size() <= 1) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8438,      return null;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8439,    }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8440,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8441,    List<ExprNodeDesc> oldList = null;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8442,    List<ASTNode> oldASTList = null;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8443,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8444,    for (String dest : ks) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8445,"      // If a filter is present, common processing is not possible"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8446,      if (qbp.getWhrForClause(dest) != null) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8447,        return null;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8448,      }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8449,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8450,      if (qbp.getAggregationExprsForClause(dest).size() == 0
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8451,"          && getGroupByForClause(qbp, dest).size() == 0) {"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8452,        return null;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8453,      }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8454,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8455,      // All distinct expressions must be the same
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8456,      List<ASTNode> list = qbp.getDistinctFuncExprsForClause(dest);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8457,      if (list.isEmpty()) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8458,        return null;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8459,      }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8460,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8461,      List<ExprNodeDesc> currDestList;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8462,      try {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8463,"        currDestList = getDistinctExprs(qbp, dest, inputRR);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8464,      } catch (SemanticException e) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8465,        return null;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8466,      }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8467,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8468,      List<ASTNode> currASTList = new ArrayList<ASTNode>();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8469,      for (ASTNode value : list) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8470,        // 0 is function name
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8471,        for (int i = 1; i < value.getChildCount(); i++) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8472,          ASTNode parameter = (ASTNode) value.getChild(i);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8473,          currASTList.add(parameter);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8474,        }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8475,        if (oldList == null) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8476,          oldList = currDestList;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8477,          oldASTList = currASTList;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8478,        } else {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8479,"          if (!matchExprLists(oldList, currDestList)) {"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8480,            return null;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8481,          }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8482,        }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8483,      }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8484,    }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8485,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8486,    return oldASTList;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8487,  }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8488,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8489,"  private Operator createCommonReduceSink(QB qb, Operator input)"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8490,      throws SemanticException {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8491,    // Go over all the tables and extract the common distinct key
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8492,"    List<ASTNode> distExprs = getCommonDistinctExprs(qb, input);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8493,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8494,    QBParseInfo qbp = qb.getParseInfo();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8495,    TreeSet<String> ks = new TreeSet<String>();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8496,    ks.addAll(qbp.getClauseNames());
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8497,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8498,    // Pass the entire row
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8499,    RowResolver inputRR = opParseCtx.get(input).getRowResolver();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8500,    RowResolver reduceSinkOutputRowResolver = new RowResolver();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8501,    reduceSinkOutputRowResolver.setIsExprResolver(true);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8502,    ArrayList<ExprNodeDesc> reduceKeys = new ArrayList<ExprNodeDesc>();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8503,    ArrayList<ExprNodeDesc> reduceValues = new ArrayList<ExprNodeDesc>();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8504,"    Map<String, ExprNodeDesc> colExprMap = new HashMap<String, ExprNodeDesc>();"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8505,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8506,    // Pre-compute distinct group-by keys and store in reduceKeys
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8507,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8508,    List<String> outputColumnNames = new ArrayList<String>();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8509,    for (ASTNode distn : distExprs) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8510,"      ExprNodeDesc distExpr = genExprNodeDesc(distn, inputRR);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8511,      if (reduceSinkOutputRowResolver.getExpression(distn) == null) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8512,        reduceKeys.add(distExpr);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8513,        outputColumnNames.add(getColumnInternalName(reduceKeys.size() - 1));
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8514,"        String field = Utilities.ReduceField.KEY.toString() + ""."""
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8515,            + getColumnInternalName(reduceKeys.size() - 1);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8516,"        ColumnInfo colInfo = new ColumnInfo(field, reduceKeys.get("
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8517,"            reduceKeys.size() - 1).getTypeInfo(), """", false);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8518,"        reduceSinkOutputRowResolver.putExpression(distn, colInfo);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8519,"        colExprMap.put(colInfo.getInternalName(), distExpr);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8520,      }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8521,    }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8522,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8523,    // Go over all the grouping keys and aggregations
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8524,    for (String dest : ks) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8525,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8526,"      List<ASTNode> grpByExprs = getGroupByForClause(qbp, dest);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8527,      for (int i = 0; i < grpByExprs.size(); ++i) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8528,        ASTNode grpbyExpr = grpByExprs.get(i);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8529,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8530,        if (reduceSinkOutputRowResolver.getExpression(grpbyExpr) == null) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8531,"          ExprNodeDesc grpByExprNode = genExprNodeDesc(grpbyExpr, inputRR);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8532,          reduceValues.add(grpByExprNode);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8533,"          String field = Utilities.ReduceField.VALUE.toString() + ""."""
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8534,              + getColumnInternalName(reduceValues.size() - 1);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8535,"          ColumnInfo colInfo = new ColumnInfo(field, reduceValues.get("
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8536,"              reduceValues.size() - 1).getTypeInfo(), """", false);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8537,"          reduceSinkOutputRowResolver.putExpression(grpbyExpr, colInfo);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8538,          outputColumnNames.add(getColumnInternalName(reduceValues.size() - 1));
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8539,"          colExprMap.put(field, grpByExprNode);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8540,        }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8541,      }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8542,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8543,      // For each aggregation
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8544,"      HashMap<String, ASTNode> aggregationTrees = qbp"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8545,          .getAggregationExprsForClause(dest);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8546,      assert (aggregationTrees != null);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8547,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8548,"      for (Map.Entry<String, ASTNode> entry : aggregationTrees.entrySet()) {"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8549,        ASTNode value = entry.getValue();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8550,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8551,        // 0 is the function name
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8552,        for (int i = 1; i < value.getChildCount(); i++) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8553,          ASTNode paraExpr = (ASTNode) value.getChild(i);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8554,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8555,          if (reduceSinkOutputRowResolver.getExpression(paraExpr) == null) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8556,"            ExprNodeDesc paraExprNode = genExprNodeDesc(paraExpr, inputRR);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8557,            reduceValues.add(paraExprNode);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8558,"            String field = Utilities.ReduceField.VALUE.toString() + ""."""
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8559,                + getColumnInternalName(reduceValues.size() - 1);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8560,"            ColumnInfo colInfo = new ColumnInfo(field, reduceValues.get("
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8561,"                reduceValues.size() - 1).getTypeInfo(), """", false);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8562,"            reduceSinkOutputRowResolver.putExpression(paraExpr, colInfo);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8563,            outputColumnNames
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8564,                .add(getColumnInternalName(reduceValues.size() - 1));
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8565,"            colExprMap.put(field, paraExprNode);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8566,          }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8567,        }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8568,      }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8569,    }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8570,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8571,    ReduceSinkOperator rsOp = (ReduceSinkOperator) putOpInsertMap(
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8572,"        OperatorFactory.getAndMakeChild(PlanUtils.getReduceSinkDesc(reduceKeys,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8573,"            reduceValues, outputColumnNames, true, -1, reduceKeys.size(), -1,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8574,"                AcidUtils.Operation.NOT_ACID),"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8575,"            new RowSchema(reduceSinkOutputRowResolver.getColumnInfos()), input),"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8576,        reduceSinkOutputRowResolver);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8577,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8578,    rsOp.setColumnExprMap(colExprMap);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8579,    return rsOp;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8580,  }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8581,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8759,"    // For multi-group by with the same distinct, we ignore all user hints"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8760,    // currently. It doesnt matter whether he has asked to do
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8761,    // map-side aggregation or not. Map side aggregation is turned off
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8762,"    List<ASTNode> commonDistinctExprs = getCommonDistinctExprs(qb, input);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8763,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8764,    // Consider a query like:
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8765,    //
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8766,    //  from src
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8767,"    //    insert overwrite table dest1 select col1, count(distinct colx) group by col1"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8768,"    //    insert overwrite table dest2 select col2, count(distinct colx) group by col2;"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8769,    //
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8770,"    // With HIVE_OPTIMIZE_MULTI_GROUPBY_COMMON_DISTINCTS set to true, first we spray by the distinct"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8771,"    // value (colx), and then perform the 2 groups bys. This makes sense if map-side aggregation is"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8772,"    // turned off. However, with maps-side aggregation, it might be useful in some cases to treat"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8773,"    // the 2 inserts independently, thereby performing the query above in 2MR jobs instead of 3"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8774,    // (due to spraying by distinct key first).
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8775,    boolean optimizeMultiGroupBy = commonDistinctExprs != null &&
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8776,        conf.getBoolVar(HiveConf.ConfVars.HIVE_OPTIMIZE_MULTI_GROUPBY_COMMON_DISTINCTS);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8780,"    // if there is a single distinct, optimize that. Spray initially by the"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8781,"    // distinct key,"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8782,    // no computation at the mapper. Have multiple group by operators at the
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8783,    // reducer - and then
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8784,    // proceed
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8785,    if (optimizeMultiGroupBy) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8786,"      curr = createCommonReduceSink(qb, input);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8787,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8788,      RowResolver currRR = opParseCtx.get(curr).getRowResolver();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8789,      // create a forward operator
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8790,"      input = putOpInsertMap(OperatorFactory.getAndMakeChild(new ForwardDesc(),"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8791,"          new RowSchema(currRR.getColumnInfos()), curr), currRR);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8792,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8793,      for (String dest : ks) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8794,        curr = input;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8795,"        curr = genGroupByPlan2MRMultiGroupBy(dest, qb, curr);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8796,"        curr = genSelectPlan(dest, qb, curr, null); // TODO: we may need to pass ""input"" here instead of null"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8797,        Integer limit = qbp.getDestLimit(dest);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8798,        if (limit != null) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8799,"          curr = genLimitMapRedPlan(dest, qb, curr, limit.intValue(), true);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8800,          qb.getParseInfo().setOuterQueryLimit(limit.intValue());
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8801,        }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8802,"        curr = genFileSinkPlan(dest, qb, curr);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8803,      }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8804,    } else {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8805,      List<List<String>> commonGroupByDestGroups = null;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8807,"      // If we can put multiple group bys in a single reducer, determine suitable groups of"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8808,"      // expressions, otherwise treat all the expressions as a single group"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8809,      if (conf.getBoolVar(HiveConf.ConfVars.HIVEMULTIGROUPBYSINGLEREDUCER)) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8810,        try {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8811,"          commonGroupByDestGroups = getCommonGroupByDestGroups(qb, inputs);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8812,        } catch (SemanticException e) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8813,"          LOG.error(""Failed to group clauses by common spray keys."", e);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8814,        }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8817,      if (commonGroupByDestGroups == null) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8818,        commonGroupByDestGroups = new ArrayList<List<String>>();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8819,        commonGroupByDestGroups.add(new ArrayList<String>(ks));
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8820,      }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8822,      if (!commonGroupByDestGroups.isEmpty()) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8824,        // Iterate over each group of subqueries with the same group by/distinct keys
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8825,        for (List<String> commonGroupByDestGroup : commonGroupByDestGroups) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8826,          if (commonGroupByDestGroup.isEmpty()) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8827,            continue;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8828,          }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8830,          String firstDest = commonGroupByDestGroup.get(0);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8831,          input = inputs.get(firstDest);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8832,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8833,          // Constructs a standard group by plan if:
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8834,          // There is no other subquery with the same group by/distinct keys or
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8835,          // (There are no aggregations in a representative query for the group and
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8836,          // There is no group by in that representative query) or
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8837,          // The data is skewed or
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8838,          // The conf variable used to control combining group bys into a single reducer is false
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8839,          if (commonGroupByDestGroup.size() == 1 ||
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8840,              (qbp.getAggregationExprsForClause(firstDest).size() == 0 &&
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8841,"              getGroupByForClause(qbp, firstDest).size() == 0) ||"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8842,              conf.getBoolVar(HiveConf.ConfVars.HIVEGROUPBYSKEW) ||
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8843,              !conf.getBoolVar(HiveConf.ConfVars.HIVEMULTIGROUPBYSINGLEREDUCER)) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8844,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8845,            // Go over all the destination tables
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8846,            for (String dest : commonGroupByDestGroup) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8847,              curr = inputs.get(dest);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8848,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8849,              if (qbp.getWhrForClause(dest) != null) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8850,                ASTNode whereExpr = qb.getParseInfo().getWhrForClause(dest);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8851,"                curr = genFilterPlan((ASTNode) whereExpr.getChild(0), qb, curr, aliasToOpInfo, false);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8853,              // Preserve operator before the GBY - we'll use it to resolve '*'
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8854,              Operator<?> gbySource = curr;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8855,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8856,              if (qbp.getAggregationExprsForClause(dest).size() != 0
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8857,"                  || getGroupByForClause(qbp, dest).size() > 0) {"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8858,                // multiple distincts is not supported with skew in data
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8859,                if (conf.getBoolVar(HiveConf.ConfVars.HIVEGROUPBYSKEW) &&
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8860,                    qbp.getDistinctFuncExprsForClause(dest).size() > 1) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8861,                  throw new SemanticException(ErrorMsg.UNSUPPORTED_MULTIPLE_DISTINCTS.
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8862,                      getMsg());
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8864,                // insert a select operator here used by the ColumnPruner to reduce
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8865,                // the data to shuffle
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8866,                curr = insertSelectAllPlanForGroupBy(curr);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8867,                // Check and transform group by *. This will only happen for select distinct *.
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8868,"                // Here the ""genSelectPlan"" is being leveraged."
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8869,                // The main benefits are (1) remove virtual columns that should
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8870,                // not be included in the group by; (2) add the fully qualified column names to unParseTranslator
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8871,                // so that view is supported. The drawback is that an additional SEL op is added. If it is
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8872,"                // not necessary, it will be removed by NonBlockingOpDeDupProc Optimizer because it will match"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8873,                // SEL%SEL% rule.
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8874,                ASTNode selExprList = qbp.getSelForClause(dest);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8875,                if (selExprList.getToken().getType() == HiveParser.TOK_SELECTDI
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8876,                    && selExprList.getChildCount() == 1 && selExprList.getChild(0).getChildCount() == 1) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8877,                  ASTNode node = (ASTNode) selExprList.getChild(0).getChild(0);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8878,                  if (node.getToken().getType() == HiveParser.TOK_ALLCOLREF) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8879,"                    curr = genSelectPlan(dest, qb, curr, curr);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8880,                    RowResolver rr = opParseCtx.get(curr).getRowResolver();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8881,"                    qbp.setSelExprForClause(dest, SemanticAnalyzer.genSelectDIAST(rr));"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8882,                  }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8883,                }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8884,                if (conf.getBoolVar(HiveConf.ConfVars.HIVEMAPSIDEAGGREGATE)) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8885,                  if (!conf.getBoolVar(HiveConf.ConfVars.HIVEGROUPBYSKEW)) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8886,"                    curr = genGroupByPlanMapAggrNoSkew(dest, qb, curr);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8887,                  } else {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8888,"                    curr = genGroupByPlanMapAggr2MR(dest, qb, curr);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8889,                  }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8890,                } else if (conf.getBoolVar(HiveConf.ConfVars.HIVEGROUPBYSKEW)) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8891,"                  curr = genGroupByPlan2MR(dest, qb, curr);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8893,"                  curr = genGroupByPlan1MR(dest, qb, curr);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8896,              if (LOG.isDebugEnabled()) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8897,"                LOG.debug(""RR before GB "" + opParseCtx.get(gbySource).getRowResolver()"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8898,"                    + "" after GB "" + opParseCtx.get(curr).getRowResolver());"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8899,              }
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8900,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8901,"              curr = genPostGroupByBodyPlan(curr, dest, qb, aliasToOpInfo, gbySource);"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8903,          } else {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8904,"            curr = genGroupByPlan1ReduceMultiGBY(commonGroupByDestGroup, qb, input, aliasToOpInfo);"
ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java,57,  private boolean groupKeyNotReductionKey;
ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java,85,"      final boolean groupKeyNotReductionKey,"
ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java,92,"    this(mode, outputColumnNames, keys, aggregators, groupKeyNotReductionKey,"
ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java,102,"      final boolean groupKeyNotReductionKey,"
ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java,115,    this.groupKeyNotReductionKey = groupKeyNotReductionKey;
ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java,183,    return groupingSetPosition >= 0 &&
ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java,232,
ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java,233,  public boolean getGroupKeyNotReductionKey() {
ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java,234,    return groupKeyNotReductionKey;
ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java,235,  }
ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java,236,
ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java,237,  public void setGroupKeyNotReductionKey(final boolean groupKeyNotReductionKey) {
ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java,238,    this.groupKeyNotReductionKey = groupKeyNotReductionKey;
ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java,239,  }
ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java,240,
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/MathExpr.java,78,  /* Convert an integer value in miliseconds since the epoch to a timestamp value
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/MathExpr.java,82,    return v * 1000000;
ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorTypeCasts.java,49,  // Number of microseconds in one second
ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorTypeCasts.java,50,  private static final long MICROS_PER_SECOND = 1000000;
ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorTypeCasts.java,51,
ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorTypeCasts.java,114,"    Assert.assertEquals(-2 * MICROS_PER_SECOND, resultV.vector[0]);"
ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorTypeCasts.java,115,"    Assert.assertEquals(2 * MICROS_PER_SECOND, resultV.vector[1]);"
serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java,515,  /**
serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java,516,   * Interprets a float as a unix timestamp and returns a Timestamp object
serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java,517,   * @param f
serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java,518,   * @return the equivalent Timestamp object
serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java,519,   */
serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java,520,  public static Timestamp floatToTimestamp(float f) {
serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java,521,    return doubleToTimestamp((double) f);
serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java,522,  }
serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java,523,
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1066,      result = new Timestamp(((BooleanObjectInspector) oi).get(o) ? 1 : 0);
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1069,      result = new Timestamp(((ByteObjectInspector) oi).get(o));
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1072,      result = new Timestamp(((ShortObjectInspector) oi).get(o));
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1075,      result = new Timestamp(((IntObjectInspector) oi).get(o));
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1078,      result = new Timestamp(((LongObjectInspector) oi).get(o));
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1081,      result = TimestampWritable.floatToTimestamp(((FloatObjectInspector) oi).get(o));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1156,
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/MathExpr.java,21,import java.io.IOException;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/MathExpr.java,22,import java.io.OutputStream;
serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java,530,   * Converts the time in seconds to a timestamp.
serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java,531,   * @param timeInSeconds time in seconds
serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java,534,  public static Timestamp longToTimestamp(long timeInSeconds) {
serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java,535,    return new Timestamp(timeInSeconds * 1000);
serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java,537,
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorConverter.java,286,          inputOI));
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1062,    switch (oi.getPrimitiveCategory()) {
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1067,      longValue = ((BooleanObjectInspector) oi).get(o) ? 1 : 0;
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1068,      result = TimestampWritable.longToTimestamp(longValue);
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1071,      longValue = ((ByteObjectInspector) oi).get(o);
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1072,      result = TimestampWritable.longToTimestamp(longValue);
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1075,      longValue = ((ShortObjectInspector) oi).get(o);
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1076,      result = TimestampWritable.longToTimestamp(longValue);
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1079,      longValue = ((IntObjectInspector) oi).get(o);
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1080,      result = TimestampWritable.longToTimestamp(longValue);
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1083,      longValue = ((LongObjectInspector) oi).get(o);
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1084,      result = TimestampWritable.longToTimestamp(longValue);
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1087,      result = TimestampWritable.doubleToTimestamp(((FloatObjectInspector) oi).get(o));
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1090,      result = TimestampWritable.doubleToTimestamp(((DoubleObjectInspector) oi).get(o));
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1093,      result = TimestampWritable.decimalToTimestamp(((HiveDecimalObjectInspector) oi)
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1097,      StringObjectInspector soi = (StringObjectInspector) oi;
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1103,"      result = getTimestampFromString(getString(o, oi));"
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1107,          ((DateObjectInspector) oi).getPrimitiveWritableObject(o).get().getTime());
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1110,      result = ((TimestampObjectInspector) oi).getPrimitiveWritableObject(o).getTimestamp();
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,1114,          + oi.getTypeName());
serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/primitive/TestPrimitiveObjectInspectorUtils.java,75,
serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/primitive/TestPrimitiveObjectInspectorUtils.java,77,  public void testgetTimestamp() {
serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/primitive/TestPrimitiveObjectInspectorUtils.java,88,"    assertEquals(""1970-01-01 00:00:01.000"", gmtDateFormat.format(PrimitiveObjectInspectorUtils.getTimestamp(true, booleanOI)));"
serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/primitive/TestPrimitiveObjectInspectorUtils.java,93,"    assertEquals(""1970-01-01 00:00:01.000"", gmtDateFormat.format(PrimitiveObjectInspectorUtils.getTimestamp((byte)1, byteOI)));"
serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/primitive/TestPrimitiveObjectInspectorUtils.java,94,"    assertEquals(""1969-12-31 23:59:59.000"", gmtDateFormat.format(PrimitiveObjectInspectorUtils.getTimestamp((byte)-1, byteOI)));"
serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/primitive/TestPrimitiveObjectInspectorUtils.java,98,"    assertEquals(""1970-01-01 00:00:01.000"", gmtDateFormat.format(PrimitiveObjectInspectorUtils.getTimestamp((short)1, shortOI)));"
serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/primitive/TestPrimitiveObjectInspectorUtils.java,99,"    assertEquals(""1969-12-31 23:59:59.000"", gmtDateFormat.format(PrimitiveObjectInspectorUtils.getTimestamp((short)-1, shortOI)));"
serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/primitive/TestPrimitiveObjectInspectorUtils.java,103,"    assertEquals(""2015-02-07 15:01:22.000"", gmtDateFormat.format(PrimitiveObjectInspectorUtils.getTimestamp((int)1423321282, intOI)));"
serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/primitive/TestPrimitiveObjectInspectorUtils.java,104,"    assertEquals(""1969-12-31 23:59:59.000"", gmtDateFormat.format(PrimitiveObjectInspectorUtils.getTimestamp((int)-1, intOI)));"
serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/primitive/TestPrimitiveObjectInspectorUtils.java,108,"    assertEquals(""2015-02-07 15:01:22.000"", gmtDateFormat.format(PrimitiveObjectInspectorUtils.getTimestamp(1423321282L, longOI)));"
serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/primitive/TestPrimitiveObjectInspectorUtils.java,109,"    assertEquals(""1969-12-31 23:59:59.000"", gmtDateFormat.format(PrimitiveObjectInspectorUtils.getTimestamp(-1L, longOI)));"
service/src/java/org/apache/hive/service/auth/LdapAuthenticationProviderImpl.java,26,import org.apache.hadoop.hive.conf.HiveConf;
service/src/java/org/apache/hive/service/auth/LdapAuthenticationProviderImpl.java,27,import org.apache.hive.service.ServiceUtils;
service/src/java/org/apache/hive/service/auth/LdapAuthenticationProviderImpl.java,28,
service/src/java/org/apache/hive/service/auth/LdapAuthenticationProviderImpl.java,37,    ldapURL = conf.getVar(HiveConf.ConfVars.HIVE_SERVER2_PLAIN_LDAP_URL);
service/src/java/org/apache/hive/service/auth/LdapAuthenticationProviderImpl.java,38,    baseDN = conf.getVar(HiveConf.ConfVars.HIVE_SERVER2_PLAIN_LDAP_BASEDN);
service/src/java/org/apache/hive/service/auth/LdapAuthenticationProviderImpl.java,39,    ldapDomain = conf.getVar(HiveConf.ConfVars.HIVE_SERVER2_PLAIN_LDAP_DOMAIN);
service/src/java/org/apache/hive/service/auth/LdapAuthenticationProviderImpl.java,61,    // setup the security principal
service/src/java/org/apache/hive/service/auth/LdapAuthenticationProviderImpl.java,62,    String bindDN;
service/src/java/org/apache/hive/service/auth/LdapAuthenticationProviderImpl.java,63,    if (baseDN == null) {
service/src/java/org/apache/hive/service/auth/LdapAuthenticationProviderImpl.java,64,      bindDN = user;
service/src/java/org/apache/hive/service/auth/LdapAuthenticationProviderImpl.java,65,    } else {
service/src/java/org/apache/hive/service/auth/LdapAuthenticationProviderImpl.java,66,"      bindDN = ""uid="" + user + "","" + baseDN;"
service/src/java/org/apache/hive/service/auth/LdapAuthenticationProviderImpl.java,67,    }
service/src/java/org/apache/hive/service/auth/LdapAuthenticationProviderImpl.java,69,"    env.put(Context.SECURITY_PRINCIPAL, bindDN);"
service/src/java/org/apache/hive/service/auth/LdapAuthenticationProviderImpl.java,74,      Context ctx = new InitialDirContext(env);
service/src/java/org/apache/hive/service/auth/LdapAuthenticationProviderImpl.java,75,      ctx.close();
service/src/java/org/apache/hive/service/auth/LdapAuthenticationProviderImpl.java,77,"      throw new AuthenticationException(""Error validating LDAP user"", e);"
itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/hbase/HBaseIntegrationTests.java,67,    for (String tableName : HBaseReadWrite.tableNames) {
itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/hbase/HBaseIntegrationTests.java,68,      List<byte[]> families = HBaseReadWrite.columnFamilies.get(tableName);
itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/hbase/HBaseIntegrationTests.java,69,      HTableDescriptor desc = new HTableDescriptor(TableName.valueOf(tableName));
itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/hbase/HBaseIntegrationTests.java,70,      for (byte[] family : families) {
itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/hbase/HBaseIntegrationTests.java,71,        HColumnDescriptor columnDesc = new HColumnDescriptor(family);
itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/hbase/HBaseIntegrationTests.java,72,        if (testingTephra) columnDesc.setMaxVersions(Integer.MAX_VALUE);
itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/hbase/HBaseIntegrationTests.java,73,        desc.addFamily(columnDesc);
itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/hbase/HBaseIntegrationTests.java,74,      }
itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/hbase/HBaseIntegrationTests.java,75,      if (testingTephra) desc.addCoprocessor(TransactionProcessor.class.getName());
itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/hbase/HBaseIntegrationTests.java,76,      admin.createTable(desc);
itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/hbase/HBaseIntegrationTests.java,77,    }
itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/hbase/HBaseIntegrationTests.java,78,    admin.close();
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,21,import static org.apache.hadoop.hive.metastore.MetaStoreUtils.DEFAULT_DATABASE_COMMENT;
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,70,import org.apache.hadoop.hbase.HColumnDescriptor;
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,71,import org.apache.hadoop.hbase.HTableDescriptor;
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,72,import org.apache.hadoop.hbase.TableName;
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,84,import org.apache.hadoop.hive.metastore.Warehouse;
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,86,import org.apache.hadoop.hive.metastore.hbase.HBaseReadWrite;
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,360,"    conf = new HiveConf(utility.getConfiguration(), Driver.class);"
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,362,    for (String tableName : HBaseReadWrite.tableNames) {
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,363,      List<byte[]> families = HBaseReadWrite.columnFamilies.get(tableName);
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,364,      HTableDescriptor desc = new HTableDescriptor(TableName.valueOf(tableName));
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,365,      for (byte[] family : families) {
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,366,        HColumnDescriptor columnDesc = new HColumnDescriptor(family);
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,367,        desc.addFamily(columnDesc);
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,368,      }
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,369,      admin.createTable(desc);
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,370,    }
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,371,    admin.close();
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,372,    HBaseReadWrite.getInstance(conf);
itests/hive-unit/src/main/java/org/apache/hive/jdbc/miniHS2/MiniHS2.java,184,"        mr = ShimLoader.getHadoopShims().getMiniTezCluster(hiveConf, 4, uriString);"
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,48,import java.util.Iterator;
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,51,import java.util.Map.Entry;
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,63,import org.apache.hadoop.conf.Configuration;
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,76,import org.apache.hadoop.hive.llap.configuration.LlapConfiguration;
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,77,import org.apache.hadoop.hive.llap.daemon.MiniLlapCluster;
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,78,import org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon;
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,157,  private MiniLlapCluster llapCluster = null;
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,412,"        mr = shims.getMiniTezCluster(conf, 4, uriString);"
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,414,        Configuration daemonConf;
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,416,"          URL llapDaemonConfURL = new URL(""file://"""
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,417,"              + new File(confDir).toURI().getPath() + ""/llap-daemon-site.xml"");"
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,418,"          daemonConf = new LlapConfiguration(conf, llapDaemonConfURL);"
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,419,        } else {
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,420,          daemonConf = new LlapConfiguration(conf);
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,421,        }
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,422,"        final String clusterName = ""llap"";"
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,423,        final long maxMemory = LlapDaemon.getTotalHeapSize();
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,424,        // 15% for io cache
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,425,        final long memoryForCache = (long) (0.15f * maxMemory);
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,426,        // 75% for 4 executors
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,427,        final long totalExecutorMemory = (long) (0.75f * maxMemory);
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,428,"        final int numExecutors = daemonConf.getInt(LlapConfiguration.LLAP_DAEMON_NUM_EXECUTORS,"
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,429,            LlapConfiguration.LLAP_DAEMON_NUM_EXECUTORS_DEFAULT);
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,430,        final boolean asyncIOEnabled = true;
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,431,        // enabling this will cause test failures in Mac OS X
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,432,        final boolean directMemoryEnabled = false;
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,433,        final int numLocalDirs = 1;
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,434,"        LOG.info(""MiniLlap Configs - maxMemory: "" + maxMemory + "" memoryForCache: "" + memoryForCache"
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,435,"            + "" totalExecutorMemory: "" + totalExecutorMemory + "" numExecutors: "" + numExecutors"
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,436,"            + "" asyncIOEnabled: "" + asyncIOEnabled + "" directMemoryEnabled: "" + directMemoryEnabled"
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,437,"            + "" numLocalDirs: "" + numLocalDirs);"
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,438,"        llapCluster = MiniLlapCluster.create(clusterName,"
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,439,"            numExecutors,"
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,440,"            totalExecutorMemory,"
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,441,"            asyncIOEnabled,"
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,442,"            directMemoryEnabled,"
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,443,"            memoryForCache,"
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,444,            numLocalDirs);
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,445,        llapCluster.init(daemonConf);
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,446,        llapCluster.start();
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,447,        Configuration llapConf = llapCluster.getClusterSpecificConfiguration();
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,448,"        Iterator<Entry<String, String>> confIter = llapConf.iterator();"
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,449,        while (confIter.hasNext()) {
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,450,"          Entry<String, String> entry = confIter.next();"
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,451,"          conf.set(entry.getKey(), entry.getValue());"
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,453,"        mr = shims.getMiniTezCluster(conf, 2, uriString);"
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,504,    if (llapCluster != null) {
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,505,      llapCluster.stop();
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,506,      llapCluster = null;
itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java,507,    }
shims/0.20S/src/main/java/org/apache/hadoop/hive/shims/Hadoop20SShims.java,21,import java.lang.Override;
shims/0.20S/src/main/java/org/apache/hadoop/hive/shims/Hadoop20SShims.java,72,import org.apache.hadoop.security.token.Token;
shims/0.20S/src/main/java/org/apache/hadoop/hive/shims/Hadoop20SShims.java,77,import org.apache.hadoop.util.Tool;
shims/0.20S/src/main/java/org/apache/hadoop/hive/shims/Hadoop20SShims.java,236,      String nameNode) throws IOException {
shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java,68,import org.apache.hadoop.hdfs.protocol.BlockStoragePolicy;
shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java,72,import org.apache.hadoop.hdfs.client.HdfsAdmin;
shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java,73,import org.apache.hadoop.hdfs.protocol.EncryptionZone;
shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java,74,import org.apache.hadoop.hive.shims.HadoopShims.TextReaderShim;
shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java,96,import org.apache.hadoop.security.authentication.util.KerberosName;
shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java,375,      String nameNode) throws IOException {
shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java,376,"    return new MiniTezShim(conf, numberOfTaskTrackers, nameNode);"
shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java,387,"    public MiniTezShim(Configuration conf, int numberOfTaskTrackers, String nameNode)"
shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java,388,        throws IOException {
shims/common/src/main/java/org/apache/hadoop/hive/shims/HadoopShims.java,100,      String nameNode) throws IOException;
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,152,"      ""userPassword: user2"""
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,230,"    user = ""user1"";"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,232,"      ldapProvider.Authenticate(user, ""user1"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,233,"      assertTrue(""testUserBindPositive: Authentication succeeded for user1 as expected"", true);"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,235,"      Assert.fail(""testUserBindPositive: Authentication failed for user:"" + user + "" with password user1, expected to succeed"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,238,"    user = ""user2"";"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,240,"      ldapProvider.Authenticate(user, ""user2"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,241,"      assertTrue(""testUserBindPositive: Authentication succeeded for user2 as expected"", true);"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,243,"      Assert.fail(""testUserBindPositive: Authentication failed for user:"" + user + "" with password user2, expected to succeed"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,254,"    user = ""user1"";"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,256,"      ldapProvider.Authenticate(user, ""user1"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,257,"      assertTrue(""testUserBindPositive: Authentication succeeded for user1 as expected"", true);"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,259,"      Assert.fail(""testUserBindPositive: Authentication failed for user:"" + user + "" with password user1, expected to succeed"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,262,"    user = ""user2"";"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,264,"      ldapProvider.Authenticate(user, ""user2"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,265,"      assertTrue(""testUserBindPositive: Authentication succeeded for user2 as expected"", true);"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,267,"      Assert.fail(""testUserBindPositive: Authentication failed for user:"" + user + "" with password user2, expected to succeed"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,279,"      ldapProvider.Authenticate(""user1"", ""user2"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,280,"      Assert.fail(""testUserBindNegative: Authentication succeeded for user1 with password user2, expected to fail"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,282,"      assertTrue(""testUserBindNegative: Authentication failed for user1 as expected"", true);"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,286,"      ldapProvider.Authenticate(""user2"", ""user"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,287,"      Assert.fail(""testUserBindNegative: Authentication failed for user2 with password user, expected to fail"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,289,"      assertTrue(""testUserBindNegative: Authentication failed for user2 as expected"", true);"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,300,"      ldapProvider.Authenticate(""user1"", ""user2"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,301,"      Assert.fail(""testUserBindNegative: Authentication succeeded for user1 with password user2, expected to fail"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,303,"      assertTrue(""testUserBindNegative: Authentication failed for user1 as expected"", true);"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,307,"      ldapProvider.Authenticate(""user2"", ""user"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,308,"      Assert.fail(""testUserBindNegative: Authentication failed for user2 with password user, expected to fail"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,310,"      assertTrue(""testUserBindNegative: Authentication failed for user2 as expected"", true);"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,324,"    user = ""uid=user1,ou=People,dc=example,dc=com"";"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,326,"      ldapProvider.Authenticate(user, ""user1"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,330,"                    "" with password user1, expected to succeed:"" + e.getMessage());"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,333,"    user = ""uid=user2,ou=People,dc=example,dc=com"";"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,335,"      ldapProvider.Authenticate(user, ""user2"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,339,"                    "" with password user2, expected to succeed:"" + e.getMessage());"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,352,"    user = ""uid=user1,ou=People,dc=example,dc=com"";"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,354,"      ldapProvider.Authenticate(user, ""user1"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,358,"                    "" with password user1, expected to succeed"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,361,"    user = ""uid=user2,ou=People,dc=example,dc=com"";"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,363,"      ldapProvider.Authenticate(user, ""user2"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,367,"                    "" with password user2, expected to succeed"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,380,"    user = ""uid=user1,ou=People,dc=example,dc=com"";"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,382,"      ldapProvider.Authenticate(user, ""user1"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,386,"                    "" with password user1, expected to succeed"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,389,"    user = ""uid=user2,ou=People,dc=example,dc=com"";"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,391,"      ldapProvider.Authenticate(user, ""user2"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,395,"                    "" with password user2, expected to succeed"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,409,"    user = ""uid=user1,ou=People,dc=example,dc=com"";"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,411,"      ldapProvider.Authenticate(user, ""user1"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,415,"                    "" with password user1, expected to succeed"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,418,"    user = ""uid=user2,ou=People,dc=example,dc=com"";"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,420,"      ldapProvider.Authenticate(user, ""user2"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,424,"                    "" with password user2, expected to succeed"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,438,"    user = ""uid=user1,ou=People,dc=example,dc=com"";"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,440,"      ldapProvider.Authenticate(user, ""user1"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,444,"                    "" with password user1, expected to succeed"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,447,"    user = ""uid=user2,ou=People,dc=example,dc=com"";"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,449,"      ldapProvider.Authenticate(user, ""user2"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,453,"                    "" with password user2, expected to succeed"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,466,"    user = ""uid=user1,ou=People,dc=example,dc=com"";"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,468,"      ldapProvider.Authenticate(user, ""user1"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,472,"                    "" with password user1, expected to succeed"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,475,"    user = ""uid=user2,ou=People,dc=example,dc=com"";"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,477,"      ldapProvider.Authenticate(user, ""user2"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,481,"                    "" with password user2, expected to succeed"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,494,"    user = ""uid=user1,ou=People,dc=example,dc=com"";"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,496,"      ldapProvider.Authenticate(user, ""user2"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,498,"                   ""user2, expected to fail"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,503,"    user = ""uid=user2,ou=People,dc=example,dc=com"";"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,521,"    user = ""uid=user1,ou=People,dc=example,dc=com"";"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,523,"      ldapProvider.Authenticate(user, ""user2"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,525,"                   ""user2, expected to fail"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,530,"    user = ""uid=user2,ou=People,dc=example,dc=com"";"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,545,"    ldapProperties.put(""hive.server2.authentication.ldap.userFilter"", ""user2"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,548,"    user = ""uid=user2,ou=People,dc=example,dc=com"";"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,550,"      ldapProvider.Authenticate(user, ""user2"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,553,"      user = ""user2"";"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,554,"      ldapProvider.Authenticate(user, ""user2"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,562,"    ldapProperties.put(""hive.server2.authentication.ldap.userFilter"", ""user1"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,566,"      user = ""uid=user1,ou=People,dc=example,dc=com"";"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,567,"      ldapProvider.Authenticate(user, ""user1"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,570,"      user = ""user1"";"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,571,"      ldapProvider.Authenticate(user, ""user1"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,579,"    ldapProperties.put(""hive.server2.authentication.ldap.userFilter"", ""user2,user1"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,583,"      user = ""uid=user1,ou=People,dc=example,dc=com"";"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,584,"      ldapProvider.Authenticate(user, ""user1"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,587,"      user = ""user2"";"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,588,"      ldapProvider.Authenticate(user, ""user2"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,601,"    ldapProperties.put(""hive.server2.authentication.ldap.userFilter"", ""user2"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,604,"    user = ""uid=user1,ou=People,dc=example,dc=com"";"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,606,"      ldapProvider.Authenticate(user, ""user1"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,612,"    user = ""user1"";"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,614,"      ldapProvider.Authenticate(user, ""user1"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,622,"    ldapProperties.put(""hive.server2.authentication.ldap.userFilter"", ""user1"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,625,"    user = ""uid=user2,ou=People,dc=example,dc=com"";"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,627,"      ldapProvider.Authenticate(user, ""user2"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,633,"    user = ""user2"";"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,635,"      ldapProvider.Authenticate(user, ""user2"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,643,"    ldapProperties.put(""hive.server2.authentication.ldap.userFilter"", ""user3"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,646,"    user = ""user1"";"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,648,"      ldapProvider.Authenticate(user, ""user1"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,654,"    user = ""uid=user2,ou=People,dc=example,dc=com"";"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,656,"      ldapProvider.Authenticate(user, ""user2"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,672,"    user = ""uid=user1,ou=People,dc=example,dc=com"";"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,674,"      ldapProvider.Authenticate(user, ""user1"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,677,"      user = ""user1"";"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,678,"      ldapProvider.Authenticate(user, ""user1"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,681,"      user = ""uid=user2,ou=People,dc=example,dc=com"";"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,682,"      ldapProvider.Authenticate(user, ""user2"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,694,"    user = ""uid=user2,ou=People,dc=example,dc=com"";"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,696,"      ldapProvider.Authenticate(user, ""user2"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,712,"    user = ""uid=user2,ou=People,dc=example,dc=com"";"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,714,"      ldapProvider.Authenticate(user, ""user2"");"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,726,"    user = ""uid=user1,ou=People,dc=example,dc=com"";"
service/src/test/org/apache/hive/service/auth/TestLdapAtnProviderWithMiniDS.java,728,"      ldapProvider.Authenticate(user, ""user1"");"
ql/src/java/org/apache/hadoop/hive/ql/processors/DfsProcessor.java,77,"      String[] tokens = command.split(""\\s+"");"
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,535,        result = t.getLength() != 0;
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,538,        result = s.length() != 0;
serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/primitive/TestPrimitiveObjectInspectorUtils.java,30,import org.apache.hadoop.hive.conf.HiveConf;
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,2048,"    HIVEHASHTABLEKEYCOUNTADJUSTMENT(""hive.hashtable.key.count.adjustment"", 2.0f,"
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,487,      requestInterceptor = new HttpKerberosRequestInterceptor(
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,488,"          sessConfMap.get(JdbcConnectionParams.AUTH_PRINCIPAL), host, getServerHttpUrl(useSsl),"
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,489,"          assumeSubject, cookieStore, cookieName, useSsl, additionalHttpHeaders, customCookies);"
jdbc/src/java/org/apache/hive/jdbc/HttpKerberosRequestInterceptor.java,40,  boolean assumeSubject;
jdbc/src/java/org/apache/hive/jdbc/HttpKerberosRequestInterceptor.java,45,"  public HttpKerberosRequestInterceptor(String principal, String host, String serverHttpUrl,"
jdbc/src/java/org/apache/hive/jdbc/HttpKerberosRequestInterceptor.java,46,"      boolean assumeSubject, CookieStore cs, String cn, boolean isSSL,"
jdbc/src/java/org/apache/hive/jdbc/HttpKerberosRequestInterceptor.java,47,"      Map<String, String> additionalHeaders, Map<String, String> customCookies) {"
jdbc/src/java/org/apache/hive/jdbc/HttpKerberosRequestInterceptor.java,52,    this.assumeSubject = assumeSubject;
jdbc/src/java/org/apache/hive/jdbc/HttpKerberosRequestInterceptor.java,56,"  protected void addHttpAuthHeader(HttpRequest httpRequest,"
jdbc/src/java/org/apache/hive/jdbc/HttpKerberosRequestInterceptor.java,57,    HttpContext httpContext) throws Exception {
jdbc/src/java/org/apache/hive/jdbc/HttpKerberosRequestInterceptor.java,58, try {
jdbc/src/java/org/apache/hive/jdbc/HttpKerberosRequestInterceptor.java,62,      String kerberosAuthHeader = HttpAuthUtils.getKerberosServiceTicket(
jdbc/src/java/org/apache/hive/jdbc/HttpKerberosRequestInterceptor.java,63,"        principal, host, serverHttpUrl, assumeSubject);"
jdbc/src/java/org/apache/hive/jdbc/HttpKerberosRequestInterceptor.java,65,"      httpRequest.addHeader(HttpAuthUtils.AUTHORIZATION + "": "" +"
jdbc/src/java/org/apache/hive/jdbc/HttpKerberosRequestInterceptor.java,66,"        HttpAuthUtils.NEGOTIATE + "" "", kerberosAuthHeader);"
service/src/java/org/apache/hive/service/auth/HttpAuthUtils.java,21,import java.security.AccessControlContext;
service/src/java/org/apache/hive/service/auth/HttpAuthUtils.java,22,import java.security.AccessController;
service/src/java/org/apache/hive/service/auth/HttpAuthUtils.java,36,import org.apache.hadoop.hive.shims.ShimLoader;
service/src/java/org/apache/hive/service/auth/HttpAuthUtils.java,67,"  public static String getKerberosServiceTicket(String principal, String host,"
service/src/java/org/apache/hive/service/auth/HttpAuthUtils.java,68,"      String serverHttpUrl, boolean assumeSubject) throws Exception {"
service/src/java/org/apache/hive/service/auth/HttpAuthUtils.java,69,    String serverPrincipal =
service/src/java/org/apache/hive/service/auth/HttpAuthUtils.java,70,"        HadoopThriftAuthBridge.getBridge().getServerPrincipal(principal, host);"
service/src/java/org/apache/hive/service/auth/HttpAuthUtils.java,71,    if (assumeSubject) {
service/src/java/org/apache/hive/service/auth/HttpAuthUtils.java,72,"      // With this option, we're assuming that the external application,"
service/src/java/org/apache/hive/service/auth/HttpAuthUtils.java,73,      // using the JDBC driver has done a JAAS kerberos login already
service/src/java/org/apache/hive/service/auth/HttpAuthUtils.java,74,      AccessControlContext context = AccessController.getContext();
service/src/java/org/apache/hive/service/auth/HttpAuthUtils.java,75,      Subject subject = Subject.getSubject(context);
service/src/java/org/apache/hive/service/auth/HttpAuthUtils.java,76,      if (subject == null) {
service/src/java/org/apache/hive/service/auth/HttpAuthUtils.java,77,"        throw new Exception(""The Subject is not set"");"
service/src/java/org/apache/hive/service/auth/HttpAuthUtils.java,78,      }
service/src/java/org/apache/hive/service/auth/HttpAuthUtils.java,79,"      return Subject.doAs(subject, new HttpKerberosClientAction(serverPrincipal, serverHttpUrl));"
service/src/java/org/apache/hive/service/auth/HttpAuthUtils.java,82,      UserGroupInformation clientUGI =
service/src/java/org/apache/hive/service/auth/HttpAuthUtils.java,83,"          HadoopThriftAuthBridge.getBridge().getCurrentUGIWithConf(""kerberos"");"
ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMaterializedViewsRegistry.java,55,import org.apache.hadoop.hive.ql.Context;
ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMaterializedViewsRegistry.java,56,import org.apache.hadoop.hive.ql.QueryState;
ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMaterializedViewsRegistry.java,64,import org.apache.hadoop.hive.ql.parse.ASTNode;
ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMaterializedViewsRegistry.java,66,import org.apache.hadoop.hive.ql.parse.ColumnStatsList;
ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMaterializedViewsRegistry.java,68,import org.apache.hadoop.hive.ql.parse.PrunedPartitionList;
ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMaterializedViewsRegistry.java,230,"    final RelNode queryRel = parseQuery(conf, viewQuery);"
ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMaterializedViewsRegistry.java,231,    if (queryRel == null) {
ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMaterializedViewsRegistry.java,233,"          "" ignored; error parsing original query"");"
ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMaterializedViewsRegistry.java,406,"  private static RelNode parseQuery(HiveConf conf, String viewQuery) {"
ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMaterializedViewsRegistry.java,407,    try {
ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMaterializedViewsRegistry.java,408,      final ASTNode node = ParseUtils.parse(viewQuery);
ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMaterializedViewsRegistry.java,409,      final QueryState qs =
ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMaterializedViewsRegistry.java,410,          new QueryState.Builder().withHiveConf(conf).build();
ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMaterializedViewsRegistry.java,411,      CalcitePlanner analyzer = new CalcitePlanner(qs);
ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMaterializedViewsRegistry.java,412,      Context ctx = new Context(conf);
ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMaterializedViewsRegistry.java,413,      ctx.setIsLoadingMaterializedView(true);
ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMaterializedViewsRegistry.java,414,      analyzer.initCtx(ctx);
ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMaterializedViewsRegistry.java,415,      analyzer.init(false);
ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMaterializedViewsRegistry.java,416,      return analyzer.genLogicalPlan(node);
ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMaterializedViewsRegistry.java,417,    } catch (Exception e) {
ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMaterializedViewsRegistry.java,418,      // We could not parse the view
ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMaterializedViewsRegistry.java,419,"      LOG.error(""Error parsing original query for materialized view"", e);"
ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMaterializedViewsRegistry.java,420,      return null;
ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMaterializedViewsRegistry.java,421,    }
ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMaterializedViewsRegistry.java,422,  }
ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMaterializedViewsRegistry.java,423,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFGetSplits.java,277,      if(num == 0) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFGetSplits.java,278,        //Schema only
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFGetSplits.java,279,"        return new PlanFragment(null, schema, null);"
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFGetSplits.java,280,      }
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFGetSplits.java,668,  private Schema convertSchema(Object obj) throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFGetSplits.java,669,    org.apache.hadoop.hive.metastore.api.Schema schema = (org.apache.hadoop.hive.metastore.api.Schema) obj;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFGetSplits.java,671,    for (FieldSchema fs : schema.getFieldSchemas()) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFGetSplits.java,676,    Schema Schema = new Schema(colDescs);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFGetSplits.java,677,    return Schema;
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,41,"  private static final org.apache.thrift.protocol.TField TABLE_NAME_FIELD_DESC = new org.apache.thrift.protocol.TField(""tableName"", org.apache.thrift.protocol.TType.STRING, (short)1);"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,42,"  private static final org.apache.thrift.protocol.TField DB_NAME_FIELD_DESC = new org.apache.thrift.protocol.TField(""dbName"", org.apache.thrift.protocol.TType.STRING, (short)2);"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,43,"  private static final org.apache.thrift.protocol.TField OWNER_FIELD_DESC = new org.apache.thrift.protocol.TField(""owner"", org.apache.thrift.protocol.TType.STRING, (short)3);"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,44,"  private static final org.apache.thrift.protocol.TField CREATE_TIME_FIELD_DESC = new org.apache.thrift.protocol.TField(""createTime"", org.apache.thrift.protocol.TType.I32, (short)4);"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,45,"  private static final org.apache.thrift.protocol.TField LAST_ACCESS_TIME_FIELD_DESC = new org.apache.thrift.protocol.TField(""lastAccessTime"", org.apache.thrift.protocol.TType.I32, (short)5);"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,46,"  private static final org.apache.thrift.protocol.TField RETENTION_FIELD_DESC = new org.apache.thrift.protocol.TField(""retention"", org.apache.thrift.protocol.TType.I32, (short)6);"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,47,"  private static final org.apache.thrift.protocol.TField SD_FIELD_DESC = new org.apache.thrift.protocol.TField(""sd"", org.apache.thrift.protocol.TType.STRUCT, (short)7);"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,48,"  private static final org.apache.thrift.protocol.TField PARTITION_KEYS_FIELD_DESC = new org.apache.thrift.protocol.TField(""partitionKeys"", org.apache.thrift.protocol.TType.LIST, (short)8);"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,49,"  private static final org.apache.thrift.protocol.TField PARAMETERS_FIELD_DESC = new org.apache.thrift.protocol.TField(""parameters"", org.apache.thrift.protocol.TType.MAP, (short)9);"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,50,"  private static final org.apache.thrift.protocol.TField VIEW_ORIGINAL_TEXT_FIELD_DESC = new org.apache.thrift.protocol.TField(""viewOriginalText"", org.apache.thrift.protocol.TType.STRING, (short)10);"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,51,"  private static final org.apache.thrift.protocol.TField VIEW_EXPANDED_TEXT_FIELD_DESC = new org.apache.thrift.protocol.TField(""viewExpandedText"", org.apache.thrift.protocol.TType.STRING, (short)11);"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,52,"  private static final org.apache.thrift.protocol.TField TABLE_TYPE_FIELD_DESC = new org.apache.thrift.protocol.TField(""tableType"", org.apache.thrift.protocol.TType.STRING, (short)12);"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,53,"  private static final org.apache.thrift.protocol.TField PRIVILEGES_FIELD_DESC = new org.apache.thrift.protocol.TField(""privileges"", org.apache.thrift.protocol.TType.STRUCT, (short)13);"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,54,"  private static final org.apache.thrift.protocol.TField TEMPORARY_FIELD_DESC = new org.apache.thrift.protocol.TField(""temporary"", org.apache.thrift.protocol.TType.BOOL, (short)14);"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,55,"  private static final org.apache.thrift.protocol.TField REWRITE_ENABLED_FIELD_DESC = new org.apache.thrift.protocol.TField(""rewriteEnabled"", org.apache.thrift.protocol.TType.BOOL, (short)15);"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,56,"  private static final org.apache.thrift.protocol.TField CREATION_METADATA_FIELD_DESC = new org.apache.thrift.protocol.TField(""creationMetadata"", org.apache.thrift.protocol.TType.STRUCT, (short)16);"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,57,"  private static final org.apache.thrift.protocol.TField CAT_NAME_FIELD_DESC = new org.apache.thrift.protocol.TField(""catName"", org.apache.thrift.protocol.TType.STRING, (short)17);"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,58,"  private static final org.apache.thrift.protocol.TField OWNER_TYPE_FIELD_DESC = new org.apache.thrift.protocol.TField(""ownerType"", org.apache.thrift.protocol.TType.I32, (short)18);"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,59,"  private static final org.apache.thrift.protocol.TField WRITE_ID_FIELD_DESC = new org.apache.thrift.protocol.TField(""writeId"", org.apache.thrift.protocol.TType.I64, (short)19);"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,60,"  private static final org.apache.thrift.protocol.TField IS_STATS_COMPLIANT_FIELD_DESC = new org.apache.thrift.protocol.TField(""isStatsCompliant"", org.apache.thrift.protocol.TType.BOOL, (short)20);"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,91,"    TABLE_NAME((short)1, ""tableName""),"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,92,"    DB_NAME((short)2, ""dbName""),"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,93,"    OWNER((short)3, ""owner""),"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,94,"    CREATE_TIME((short)4, ""createTime""),"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,95,"    LAST_ACCESS_TIME((short)5, ""lastAccessTime""),"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,96,"    RETENTION((short)6, ""retention""),"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,97,"    SD((short)7, ""sd""),"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,98,"    PARTITION_KEYS((short)8, ""partitionKeys""),"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,99,"    PARAMETERS((short)9, ""parameters""),"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,100,"    VIEW_ORIGINAL_TEXT((short)10, ""viewOriginalText""),"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,101,"    VIEW_EXPANDED_TEXT((short)11, ""viewExpandedText""),"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,102,"    TABLE_TYPE((short)12, ""tableType""),"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,103,"    PRIVILEGES((short)13, ""privileges""),"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,104,"    TEMPORARY((short)14, ""temporary""),"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,105,"    REWRITE_ENABLED((short)15, ""rewriteEnabled""),"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,106,"    CREATION_METADATA((short)16, ""creationMetadata""),"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,107,"    CAT_NAME((short)17, ""catName""),"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,112,"    OWNER_TYPE((short)18, ""ownerType""),"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,113,"    WRITE_ID((short)19, ""writeId""),"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,114,"    IS_STATS_COMPLIANT((short)20, ""isStatsCompliant"");"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,129,        case 1: // TABLE_NAME
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,131,        case 2: // DB_NAME
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,133,        case 3: // OWNER
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,135,        case 4: // CREATE_TIME
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,137,        case 5: // LAST_ACCESS_TIME
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,139,        case 6: // RETENTION
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,141,        case 7: // SD
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,143,        case 8: // PARTITION_KEYS
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,145,        case 9: // PARAMETERS
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,147,        case 10: // VIEW_ORIGINAL_TEXT
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,149,        case 11: // VIEW_EXPANDED_TEXT
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,151,        case 12: // TABLE_TYPE
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,153,        case 13: // PRIVILEGES
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,155,        case 14: // TEMPORARY
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,157,        case 15: // REWRITE_ENABLED
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,159,        case 16: // CREATION_METADATA
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,161,        case 17: // CAT_NAME
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,163,        case 18: // OWNER_TYPE
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,165,        case 19: // WRITE_ID
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,167,        case 20: // IS_STATS_COMPLIANT
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,209,  private static final int __CREATETIME_ISSET_ID = 0;
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,210,  private static final int __LASTACCESSTIME_ISSET_ID = 1;
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,211,  private static final int __RETENTION_ISSET_ID = 2;
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,212,  private static final int __TEMPORARY_ISSET_ID = 3;
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,213,  private static final int __REWRITEENABLED_ISSET_ID = 4;
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,214,  private static final int __WRITEID_ISSET_ID = 5;
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,215,  private static final int __ISSTATSCOMPLIANT_ISSET_ID = 6;
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,217,"  private static final _Fields optionals[] = {_Fields.PRIVILEGES,_Fields.TEMPORARY,_Fields.REWRITE_ENABLED,_Fields.CREATION_METADATA,_Fields.CAT_NAME,_Fields.OWNER_TYPE,_Fields.WRITE_ID,_Fields.IS_STATS_COMPLIANT};"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,1901,          case 1: // TABLE_NAME
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,1909,          case 2: // DB_NAME
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,1917,          case 3: // OWNER
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,1925,          case 4: // CREATE_TIME
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,1933,          case 5: // LAST_ACCESS_TIME
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,1941,          case 6: // RETENTION
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,1949,          case 7: // SD
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,1958,          case 8: // PARTITION_KEYS
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,1977,          case 9: // PARAMETERS
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,1997,          case 10: // VIEW_ORIGINAL_TEXT
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2005,          case 11: // VIEW_EXPANDED_TEXT
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2013,          case 12: // TABLE_TYPE
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2021,          case 13: // PRIVILEGES
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2030,          case 14: // TEMPORARY
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2038,          case 15: // REWRITE_ENABLED
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2046,          case 16: // CREATION_METADATA
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2055,          case 17: // CAT_NAME
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2063,          case 18: // OWNER_TYPE
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2071,          case 19: // WRITE_ID
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2079,          case 20: // IS_STATS_COMPLIANT
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2235,      if (struct.isSetTableName()) {
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2238,      if (struct.isSetDbName()) {
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2241,      if (struct.isSetOwner()) {
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2244,      if (struct.isSetCreateTime()) {
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2247,      if (struct.isSetLastAccessTime()) {
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2250,      if (struct.isSetRetention()) {
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2253,      if (struct.isSetSd()) {
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2256,      if (struct.isSetPartitionKeys()) {
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2259,      if (struct.isSetParameters()) {
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2262,      if (struct.isSetViewOriginalText()) {
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2265,      if (struct.isSetViewExpandedText()) {
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2268,      if (struct.isSetTableType()) {
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2271,      if (struct.isSetPrivileges()) {
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2274,      if (struct.isSetTemporary()) {
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2277,      if (struct.isSetRewriteEnabled()) {
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2280,      if (struct.isSetCreationMetadata()) {
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2283,      if (struct.isSetCatName()) {
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2286,      if (struct.isSetOwnerType()) {
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2289,      if (struct.isSetWriteId()) {
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2292,      if (struct.isSetIsStatsCompliant()) {
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2295,"      oprot.writeBitSet(optionals, 20);"
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2374,      BitSet incoming = iprot.readBitSet(20);
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2379,      if (incoming.get(1)) {
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2383,      if (incoming.get(2)) {
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2387,      if (incoming.get(3)) {
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2391,      if (incoming.get(4)) {
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2395,      if (incoming.get(5)) {
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2399,      if (incoming.get(6)) {
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2404,      if (incoming.get(7)) {
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2418,      if (incoming.get(8)) {
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2433,      if (incoming.get(9)) {
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2437,      if (incoming.get(10)) {
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2441,      if (incoming.get(11)) {
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2445,      if (incoming.get(12)) {
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2450,      if (incoming.get(13)) {
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2454,      if (incoming.get(14)) {
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2458,      if (incoming.get(15)) {
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2463,      if (incoming.get(16)) {
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2467,      if (incoming.get(17)) {
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2471,      if (incoming.get(18)) {
standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java,2475,      if (incoming.get(19)) {
standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/model/MTable.java,26,
jdbc-handler/src/main/java/org/apache/hive/storage/jdbc/dao/GenericJdbcDatabaseAccessor.java,240,      String queryAfterFrom = m.group(2);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,44,import java.util.function.Supplier;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4127,    if (groupByExprs.size() > Long.SIZE) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4128,      throw new SemanticException(ErrorMsg.HIVE_GROUPING_SETS_SIZE_LIMIT.getMsg());
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4129,    }
hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hive/hcatalog/pig/HCatLoader.java,270,        if (location.equals(inputJobInfo.getTableName())) {
hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hive/hcatalog/pig/HCatLoader.java,277,"        throw new IOException(""Could not calculate input size for location (table) "" + location);"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatLoaderTest.java,102,"  private void createTable(String tablename, String schema, String partitionedBy) throws Exception {"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatLoaderTest.java,103,"    createTable(tablename, schema, partitionedBy, driver, storageFormat);"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatLoaderTest.java,106,"  static void createTable(String tablename, String schema, String partitionedBy, IDriver driver, String storageFormat)"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatLoaderTest.java,119,"  private void createTable(String tablename, String schema) throws Exception {"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatLoaderTest.java,120,"    createTable(tablename, schema, null);"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatLoaderTest.java,143,"    createTable(BASIC_TABLE, ""a int, b string"");"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatLoaderTest.java,144,"    createTable(COMPLEX_TABLE,"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatLoaderTest.java,151,"    createTable(PARTITIONED_TABLE, ""a int, b string"", ""bkt string"");"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatLoaderTest.java,152,"    createTable(SPECIFIC_SIZE_TABLE, ""a int, b string"");"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatLoaderTest.java,153,"    createTable(SPECIFIC_SIZE_TABLE_2, ""a int, b string"");"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatLoaderTest.java,154,"    createTable(PARTITIONED_DATE_TABLE, ""b string"", ""dt date"");"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatLoaderTest.java,216,        dropTable(SPECIFIC_SIZE_TABLE_2);
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatLoaderTest.java,581,"    file = new File(TEST_WAREHOUSE_DIR + ""/"" + SPECIFIC_SIZE_TABLE_2 + ""/part-m-00000"");"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatLoaderTest.java,593,"    hCatLoader.setUDFContextSignature(""testGetInputBytesMultipleTables"" + SPECIFIC_SIZE_TABLE_2);"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatLoaderTest.java,594,"    hCatLoader.setLocation(SPECIFIC_SIZE_TABLE_2, job);"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatLoaderTest.java,596,"    hCatLoader.setUDFContextSignature(""testGetInputBytesMultipleTables"" + PARTITIONED_TABLE);"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatLoaderTest.java,597,"    hCatLoader.setLocation(PARTITIONED_TABLE, job);"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatLoaderTest.java,607,"    statistics = hCatLoader.getStatistics(SPECIFIC_SIZE_TABLE_2, job);"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatLoaderTest.java,611,"    statistics = hCatLoader.getStatistics(PARTITIONED_TABLE, job);"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatStorerTest.java,268,"    AbstractHCatLoaderTest.createTable(tblName, field + "" "" + hiveType, null, driver, storageFormat);"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatStorerTest.java,347,"    AbstractHCatLoaderTest.createTable(tblName,"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatStorerTest.java,413,"    AbstractHCatLoaderTest.createTable(""junit_unparted"",""a int"", ""b string"", driver, storageFormat);"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatStorerTest.java,447,"    AbstractHCatLoaderTest.createTable(""employee"","
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatStorerTest.java,498,"    AbstractHCatLoaderTest.createTable(""junit_unparted"",""a int"", ""b string"","
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatStorerTest.java,536,"    AbstractHCatLoaderTest.createTable(""junit_parted"",""a int, b string"", ""ds string"", driver, storageFormat);"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatStorerTest.java,580,"    AbstractHCatLoaderTest.createTable(""junit_unparted"",""a int, b string"", null,"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatStorerTest.java,584,"    AbstractHCatLoaderTest.createTable(""junit_unparted2"",""a int, b string"", null,"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatStorerTest.java,631,"    AbstractHCatLoaderTest.createTable(""junit_unparted"",""a int, b string"", null,"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatStorerTest.java,667,"    AbstractHCatLoaderTest.createTable(""junit_unparted"",""a int, b string"", null,"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatStorerTest.java,704,"    AbstractHCatLoaderTest.createTable(""junit_unparted"",""a int, b string"", null, driver, storageFormat);"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatStorerTest.java,736,"    AbstractHCatLoaderTest.createTable(""junit_unparted"","
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatStorerTest.java,778,"    AbstractHCatLoaderTest.createTable(""junit_unparted"","
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatStorerTest.java,837,"    AbstractHCatLoaderTest.createTable(""junit_unparted"",""a int, b string"", null,"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatStorerTest.java,875,"    AbstractHCatLoaderTest.createTable(""employee"","
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatStorerTest.java,908,"    AbstractHCatLoaderTest.createTable(""employee"","
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatStorerTest.java,940,"    AbstractHCatLoaderTest.createTable(""employee"","
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatStorerTest.java,966,"    AbstractHCatLoaderTest.createTable(""ptn_fail"",""a int, c string"", ""b string"","
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestE2EScenarios.java,116,"   AbstractHCatLoaderTest.createTable(tablename, schema, partitionedBy, driver, storageFormat);"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatLoaderComplexSchema.java,99,"    AbstractHCatLoaderTest.createTable(tablename, schema, partitionedBy, driver, storageFormat);"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatLoaderStorer.java,68,"    AbstractHCatLoaderTest.createTable(tblName2, ""my_small_int smallint, my_tiny_int tinyint"", null, driver,"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatLoaderStorer.java,69,"      ""textfile"");"
hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatStorerMulti.java,85,"    AbstractHCatLoaderTest.createTable(tablename, schema, partitionedBy, driver, storageFormat);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/DeleteDelegator.java,56,      List<JobState> children = state.getChildren();
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/DeleteDelegator.java,57,      if (children != null) {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/DeleteDelegator.java,58,        for (JobState child : children) {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/DeleteDelegator.java,59,          try {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/DeleteDelegator.java,60,            tracker.killJob(StatusDelegator.StringToJobID(child.getId()));
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/DeleteDelegator.java,61,          } catch (IOException e) {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/DeleteDelegator.java,62,"            LOG.warn(""templeton: fail to kill job "" + child.getId());"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/HiveDelegator.java,55,"    List<String> args = makeArgs(execute, srcFile, defines, hiveArgs, otherFiles, statusdir,"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/HiveDelegator.java,61,"  private List<String> makeArgs(String execute, String srcFile,"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/HiveDelegator.java,76,"      args.add(""--service"");"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/HiveDelegator.java,77,"      args.add(""cli"");"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/HiveDelegator.java,78,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/HiveDelegator.java,79,      //the token file location as initial hiveconf arg
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/HiveDelegator.java,80,"      args.add(""--hiveconf"");"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/HiveDelegator.java,81,      args.add(TempletonControllerJob.TOKEN_FILE_ARG_PLACEHOLDER);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/HiveDelegator.java,82,
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/HiveDelegator.java,83,      //this is needed specifcally for Hive on Tez (in addition to
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/HiveDelegator.java,84,      //JobSubmissionConstants.TOKEN_FILE_ARG_PLACEHOLDER)
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/HiveDelegator.java,85,"      args.add(""--hiveconf"");"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/HiveDelegator.java,86,      args.add(JobSubmissionConstants.TOKEN_FILE_ARG_PLACEHOLDER_TEZ);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/HiveDelegator.java,90,      args.add(TempletonControllerJob.MAPREDUCE_JOB_TAGS_ARG_PLACEHOLDER);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/HiveDelegator.java,92,      for (String prop : appConf.hiveProps()) {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/HiveDelegator.java,93,"        args.add(""--hiveconf"");"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/HiveDelegator.java,94,        args.add(prop);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/HiveDelegator.java,95,      }
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/LaunchMapper.java,156,"  protected Process startJob(Configuration conf, String jobId, String user, String overrideClasspath)"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/LaunchMapper.java,175,"    handleMapReduceJobTag(jarArgsList, JobSubmissionConstants.MAPREDUCE_JOB_TAGS_ARG_PLACEHOLDER,"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/LaunchMapper.java,176,"        JobSubmissionConstants.MAPREDUCE_JOB_TAGS, jobId);"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/LaunchMapper.java,248,   * Replace the placeholder mapreduce tags with our MR jobid so that all child jobs
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/LaunchMapper.java,252,"  private static void handleMapReduceJobTag(List<String> jarArgsList, String placeholder,"
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/LaunchMapper.java,404,            conf.get(OVERRIDE_CLASSPATH));
